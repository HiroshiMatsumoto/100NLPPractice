	word
word	sense
sense	
	disambiguation
disambiguation	
	yael
yael	karov
karov	
	weizmann
weizmann	institute
institute	
	shimon
shimon	edelman
edelman	t
t	
	mit
mit	
	we
we	describe
describe	a
a	method
method	for
for	automatic
automatic	word
word	sense
sense	disambiguation
disambiguation	using
using	a
a	text
text	corpus
corpus	and
and	a
a	machinereadable
machinereadable	
	dictionary
dictionary	mrd
mrd	the
the	method
method	is
is	based
based	on
on	word
word	similarity
similarity	and
and	context
context	similarity
similarity	
	measures
measures	words
words	are
are	considered
considered	similar
similar	if
if	they
they	appear
appear	in
in	similar
similar	contexts
contexts	contexts
contexts	are
are	similar
similar	if
if	
	they
they	contain
contain	similar
similar	words
words	the
the	circularity
circularity	of
of	this
this	definition
definition	is
is	resolved
resolved	by
by	an
an	iterative
iterative	converging
converging	
	process
process	in
in	which
which	the
the	system
system	learns
learns	from
from	the
the	corpus
corpus	a
a	set
set	of
of	typical
typical	usages
usages	for
for	each
each	of
of	the
the	senses
senses	
	of
of	the
the	polysemous
polysemous	word
word	listed
listed	in
in	the
the	mrd
mrd	a
a	new
new	instance
instance	of
of	a
a	polysemous
polysemous	word
word	is
is	assigned
assigned	the
the	
	sense
sense	associated
associated	with
with	the
the	typical
typical	usage
usage	most
most	similar
similar	to
to	its
its	context
context	experiments
experiments	show
show	that
that	this
this	
	method
method	can
can	learn
learn	even
even	from
from	very
very	sparse
sparse	training
training	data
data	achieving
achieving	over
over	92
92	correct
correct	disambiguation
disambiguation	
	performance
performance	
	1
1	introduction
introduction	
	word
word	sense
sense	disambiguation
disambiguation	wsd
wsd	is
is	the
the	problem
problem	of
of	assigning
assigning	a
a	sense
sense	to
to	an
an	ambiguous
ambiguous	
	word
word	using
using	its
its	context
context	we
we	assume
assume	that
that	different
different	senses
senses	of
of	a
a	word
word	correspond
correspond	to
to	
	different
different	entries
entries	in
in	its
its	dictionary
dictionary	definition
definition	for
for	example
example	suit
suit	has
has	two
two	senses
senses	listed
listed	in
in	
	a
a	dictionary
dictionary	action
action	in
in	court
court	and
and	of
of	clothes
clothes	given
given	the
the	sentence
sentence	the
the	
	lawyers
lawyers	are
are	reviewing
reviewing	the
the	suit
suit	we
we	would
would	like
like	the
the	system
system	to
to	decide
decide	automatically
automatically	that
that	
	suit
suit	is
is	used
used	there
there	in
in	its
its	sense
sense	we
we	assume
assume	that
that	the
the	part
part	of
of	speech
speech	of
of	the
the	
	polysemous
polysemous	word
word	is
is	known
known	
	in
in	recent
recent	years
years	text
text	corpora
corpora	have
have	been
been	the
the	main
main	source
source	of
of	information
information	for
for	learning
learning	
	automatic
automatic	wsd
wsd	see
see	for
for	example
example	gale
gale	church
church	and
and	yarowsky
yarowsky	a
a	typical
typical	
	algorithm
algorithm	constructs
constructs	a
a	training
training	set
set	from
from	all
all	contexts
contexts	of
of	a
a	polysemous
polysemous	
	word
word	w
w	in
in	the
the	corpus
corpus	and
and	uses
uses	it
it	to
to	learn
learn	a
a	classifier
classifier	that
that	maps
maps	instances
instances	of
of	w
w	each
each	
	supplied
supplied	with
with	its
its	context
context	into
into	the
the	senses
senses	because
because	learning
learning	requires
requires	that
that	the
the	examples
examples	
	in
in	the
the	training
training	set
set	be
be	partitioned
partitioned	into
into	the
the	different
different	senses
senses	and
and	because
because	sense
sense	information
information	
	is
is	not
not	available
available	in
in	the
the	corpus
corpus	explicitly
explicitly	this
this	approach
approach	depends
depends	critically
critically	on
on	manual
manual	
	sense
sense	laborious
laborious	and
and	process
process	that
that	has
has	to
to	be
be	repeated
repeated	for
for	
	every
every	word
word	in
in	every
every	language
language	and
and	more
more	likely
likely	than
than	not
not	for
for	every
every	topic
topic	of
of	discourse
discourse	
	or
or	source
source	of
of	information
information	
	the
the	need
need	for
for	tagged
tagged	examples
examples	creates
creates	a
a	problem
problem	referred
referred	to
to	in
in	previous
previous	works
works	as
as	
	the
the	knowledge
knowledge	acquisition
acquisition	bottleneck
bottleneck	training
training	a
a	disambiguator
disambiguator	for
for	w
w	requires
requires	that
that	
	the
the	examples
examples	in
in	the
the	corpus
corpus	be
be	partitioned
partitioned	into
into	senses
senses	which
which	in
in	turn
turn	requires
requires	a
a	fully
fully	
	operational
operational	disambiguator
disambiguator	the
the	method
method	we
we	propose
propose	circumvents
circumvents	this
this	problem
problem	by
by	automatically
automatically	
	tagging
tagging	the
the	training
training	set
set	examples
examples	for
for	w
w	using
using	other
other	examples
examples	that
that	do
do	not
not	
	contain
contain	w
w	but
but	do
do	contain
contain	related
related	words
words	extracted
extracted	from
from	its
its	dictionary
dictionary	definition
definition	for
for	
	instance
instance	in
in	the
the	training
training	set
set	for
for	suit
suit	we
we	would
would	use
use	in
in	addition
addition	to
to	the
the	contexts
contexts	of
of	suit
suit	
	dept
dept	of
of	applied
applied	mathematics
mathematics	and
and	computer
computer	science
science	rehovot
rehovot	76100
76100	israel
israel	
	t
t	center
center	for
for	biological
biological	computational
computational	learning
learning	mit
mit	cambridge
cambridge	ma
ma	02142
02142	present
present	address
address	
	school
school	of
of	cognitive
cognitive	and
and	computing
computing	sciences
sciences	university
university	of
of	sussex
sussex	falmer
falmer	bn1
bn1	9qh
9qh	uk
uk	
	1998
1998	association
association	for
for	computational
computational	linguistics
linguistics	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	all
all	the
the	contexts
contexts	of
of	court
court	and
and	of
of	clothes
clothes	in
in	the
the	corpus
corpus	because
because	court
court	and
and	clothes
clothes	appear
appear	in
in	
	the
the	dictionary
dictionary	mrd
mrd	entry
entry	of
of	suit
suit	that
that	defines
defines	its
its	two
two	senses
senses	note
note	
	that
that	unlike
unlike	the
the	contexts
contexts	of
of	suit
suit	which
which	may
may	discuss
discuss	either
either	court
court	action
action	or
or	clothing
clothing	the
the	
	contexts
contexts	of
of	court
court	are
are	not
not	likely
likely	to
to	be
be	especially
especially	related
related	to
to	clothing
clothing	and
and	similarly
similarly	those
those	
	of
of	clothes
clothes	will
will	normally
normally	have
have	little
little	to
to	do
do	with
with	lawsuits
lawsuits	we
we	will
will	use
use	this
this	observation
observation	to
to	
	tag
tag	the
the	original
original	contexts
contexts	of
of	suit
suit	
	another
another	problem
problem	that
that	affects
affects	the
the	wsd
wsd	methods
methods	is
is	the
the	sparseness
sparseness	of
of	
	data
data	these
these	methods
methods	typically
typically	rely
rely	on
on	the
the	statistics
statistics	of
of	of
of	words
words	while
while	
	many
many	of
of	the
the	possible
possible	are
are	not
not	observed
observed	even
even	in
in	a
a	very
very	large
large	corpus
corpus	
	church
church	and
and	mercer
mercer	1993
1993	we
we	address
address	this
this	problem
problem	in
in	several
several	ways
ways	first
first	instead
instead	of
of	
	tallying
tallying	word
word	statistics
statistics	from
from	the
the	examples
examples	for
for	each
each	sense
sense	which
which	may
may	be
be	unreliable
unreliable	
	when
when	the
the	examples
examples	are
are	few
few	we
we	collect
collect	statistics
statistics	representing
representing	each
each	
	sentence
sentence	by
by	the
the	set
set	of
of	features
features	it
it	contains
contains	for
for	more
more	on
on	features
features	see
see	section
section	4
4	2
2	second
second	
	we
we	define
define	a
a	similarity
similarity	measure
measure	on
on	the
the	feature
feature	space
space	which
which	allows
allows	us
us	to
to	pool
pool	the
the	
	statistics
statistics	of
of	similar
similar	features
features	third
third	in
in	addition
addition	to
to	the
the	examples
examples	of
of	the
the	polysemous
polysemous	word
word	
	142
142	in
in	the
the	corpus
corpus	we
we	learn
learn	also
also	from
from	the
the	examples
examples	of
of	all
all	the
the	words
words	in
in	the
the	dictionary
dictionary	
	definition
definition	of
of	w
w	in
in	our
our	experiments
experiments	this
this	resulted
resulted	in
in	a
a	training
training	set
set	that
that	could
could	be
be	up
up	to
to	
	20
20	times
times	larger
larger	than
than	the
the	set
set	of
of	original
original	examples
examples	
	the
the	rest
rest	of
of	this
this	paper
paper	is
is	organized
organized	as
as	follows
follows	section
section	2
2	describes
describes	the
the	approach
approach	
	we
we	have
have	developed
developed	in
in	section
section	3
3	we
we	report
report	the
the	results
results	of
of	tests
tests	we
we	have
have	conducted
conducted	on
on	
	the
the	corpus
corpus	section
section	4
4	concludes
concludes	with
with	a
a	discussion
discussion	of
of	related
related	methods
methods	and
and	
	a
a	summary
summary	proofs
proofs	and
and	other
other	details
details	of
of	our
our	scheme
scheme	can
can	be
be	found
found	in
in	the
the	appendix
appendix	
	2
2	disambiguation
disambiguation	
	our
our	aim
aim	is
is	to
to	have
have	the
the	system
system	learn
learn	to
to	disambiguate
disambiguate	the
the	appearances
appearances	of
of	a
a	polysemous
polysemous	
	word
word	w
w	noun
noun	verb
verb	or
or	adjective
adjective	with
with	senses
senses	sl
sl	using
using	as
as	examples
examples	the
the	appearances
appearances	
	of
of	w
w	in
in	an
an	untagged
untagged	corpus
corpus	to
to	avoid
avoid	the
the	need
need	to
to	tag
tag	the
the	training
training	examples
examples	
	manually
manually	we
we	augment
augment	the
the	training
training	set
set	by
by	additional
additional	examples
examples	which
which	
	we
we	call
call	a
a	feedback
feedback	set
set	the
the	feedback
feedback	set
set	for
for	sense
sense	si
si	of
of	word
word	w
w	is
is	the
the	union
union	of
of	all
all	
	contexts
contexts	that
that	contain
contain	some
some	noun
noun	found
found	in
in	the
the	entry
entry	of
of	si
si	w
w	in
in	an
an	mrd
mrd	1
1	words
words	in
in	
	the
the	intersection
intersection	of
of	any
any	two
two	sense
sense	entries
entries	as
as	well
well	as
as	examples
examples	in
in	the
the	intersection
intersection	of
of	
	two
two	feedback
feedback	sets
sets	are
are	discarded
discarded	during
during	initialization
initialization	we
we	also
also	use
use	a
a	stop
stop	list
list	to
to	discard
discard	
	from
from	the
the	mrd
mrd	definition
definition	words
words	such
such	as
as	that
that	which
which	do
do	not
not	contribute
contribute	
	to
to	the
the	disambiguation
disambiguation	process
process	the
the	feedback
feedback	sets
sets	can
can	be
be	augmented
augmented	in
in	turn
turn	by
by	original
original	
	sentences
sentences	that
that	are
are	closely
closely	related
related	in
in	a
a	sense
sense	defined
defined	below
below	to
to	one
one	of
of	
	the
the	sentences
sentences	these
these	additional
additional	examples
examples	can
can	then
then	attract
attract	other
other	original
original	
	examples
examples	
	the
the	feedback
feedback	sets
sets	constitute
constitute	a
a	rich
rich	source
source	of
of	data
data	that
that	are
are	known
known	to
to	be
be	sorted
sorted	by
by	
	sense
sense	specifically
specifically	the
the	feedback
feedback	set
set	of
of	si
si	is
is	known
known	to
to	be
be	more
more	closely
closely	related
related	to
to	si
si	than
than	
	to
to	the
the	other
other	senses
senses	of
of	the
the	same
same	word
word	we
we	rely
rely	on
on	this
this	observation
observation	to
to	tag
tag	automatically
automatically	
	the
the	examples
examples	of
of	w
w	as
as	follows
follows	each
each	original
original	sentence
sentence	containing
containing	w
w	is
is	assigned
assigned	the
the	
	sense
sense	of
of	its
its	most
most	similar
similar	sentence
sentence	in
in	the
the	feedback
feedback	sets
sets	two
two	sentences
sentences	are
are	considered
considered	to
to	
	be
be	similar
similar	insofar
insofar	as
as	they
they	contain
contain	similar
similar	words
words	they
they	do
do	not
not	have
have	to
to	share
share	any
any	word
word	
	words
words	are
are	considered
considered	to
to	be
be	similar
similar	if
if	they
they	appear
appear	in
in	similar
similar	sentences
sentences	the
the	circularity
circularity	
	of
of	this
this	definition
definition	is
is	resolved
resolved	by
by	an
an	iterative
iterative	converging
converging	process
process	described
described	below
below	
	1
1	by
by	mrd
mrd	we
we	mean
mean	a
a	dictionary
dictionary	or
or	a
a	thesaurus
thesaurus	or
or	any
any	combination
combination	of
of	such
such	
	knowledge
knowledge	sources
sources	
	42
42	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	2
2	1
1	terminology
terminology	
	a
a	context
context	or
or	example
example	of
of	the
the	target
target	word
word	w
w	is
is	any
any	sentence
sentence	that
that	contains
contains	w
w	and
and	
	optionally
optionally	the
the	two
two	adjacent
adjacent	sentences
sentences	in
in	the
the	corpus
corpus	the
the	features
features	of
of	a
a	sentence
sentence	8
8	are
are	
	its
its	nouns
nouns	verbs
verbs	and
and	the
the	adjectives
adjectives	of
of	w
w	and
and	of
of	any
any	noun
noun	that
that	appears
appears	both
both	in
in	and
and	
	in
in	mrd
mrd	definition
definition	s
s	all
all	used
used	after
after	stemming
stemming	it
it	is
is	also
also	possible
possible	to
to	use
use	other
other	types
types	
	of
of	features
features	such
such	as
as	word
word	or
or	syntactic
syntactic	information
information	see
see	section
section	4
4	2
2	as
as	the
the	
	number
number	of
of	features
features	in
in	the
the	training
training	data
data	can
can	be
be	very
very	large
large	we
we	automatically
automatically	assign
assign	each
each	
	relevant
relevant	feature
feature	a
a	weight
weight	indicating
indicating	the
the	extent
extent	to
to	which
which	it
it	is
is	indicative
indicative	of
of	the
the	sense
sense	
	see
see	section
section	a
a	3
3	in
in	the
the	appendix
appendix	features
features	that
that	appear
appear	less
less	than
than	two
two	times
times	in
in	the
the	
	training
training	set
set	and
and	features
features	whose
whose	weight
weight	falls
falls	under
under	a
a	certain
certain	threshold
threshold	are
are	excluded
excluded	
	a
a	sentence
sentence	is
is	represented
represented	by
by	the
the	set
set	of
of	the
the	remaining
remaining	relevant
relevant	features
features	it
it	contains
contains	
	2
2	2
2	computation
computation	of
of	similarity
similarity	
	our
our	method
method	hinges
hinges	on
on	the
the	possibility
possibility	of
of	computing
computing	similarity
similarity	between
between	the
the	original
original	
	contexts
contexts	of
of	w
w	and
and	the
the	sentences
sentences	in
in	the
the	feedback
feedback	sets
sets	we
we	concentrate
concentrate	on
on	similarities
similarities	
	in
in	the
the	way
way	sentences
sentences	use
use	w
w	not
not	on
on	similarities
similarities	in
in	the
the	meaning
meaning	of
of	the
the	sentences
sentences	thus
thus	
	similar
similar	words
words	tend
tend	to
to	appear
appear	in
in	similar
similar	contexts
contexts	and
and	their
their	textual
textual	proximity
proximity	to
to	the
the	
	ambiguous
ambiguous	word
word	w
w	is
is	indicative
indicative	of
of	the
the	sense
sense	of
of	w
w	note
note	that
that	contextually
contextually	similar
similar	
	words
words	do
do	not
not	have
have	to
to	be
be	synonyms
synonyms	or
or	to
to	belong
belong	to
to	the
the	same
same	lexical
lexical	category
category	for
for	example
example	
	we
we	consider
consider	the
the	words
words	doctor
doctor	and
and	health
health	to
to	be
be	similar
similar	because
because	they
they	frequently
frequently	
	share
share	contexts
contexts	although
although	they
they	are
are	far
far	removed
removed	from
from	each
each	other
other	in
in	a
a	typical
typical	semantic
semantic	
	hierarchy
hierarchy	such
such	as
as	the
the	wordnet
wordnet	miller
miller	et
et	al
al	1993
1993	note
note	further
further	that
that	because
because	we
we	
	learn
learn	similarity
similarity	from
from	the
the	training
training	set
set	of
of	w
w	and
and	not
not	from
from	the
the	entire
entire	corpus
corpus	it
it	tends
tends	
	to
to	capture
capture	regularities
regularities	with
with	respect
respect	to
to	the
the	usage
usage	of
of	14
14	rather
rather	than
than	abstract
abstract	or
or	general
general	
	regularities
regularities	for
for	example
example	the
the	otherwise
otherwise	unrelated
unrelated	words
words	war
war	and
and	trafficking
trafficking	are
are	similar
similar	
	in
in	the
the	contexts
contexts	of
of	the
the	polysemous
polysemous	word
word	drug
drug	because
because	the
the	
	expressions
expressions	drug
drug	trafficking
trafficking	and
and	the
the	war
war	on
on	drugs
drugs	appear
appear	in
in	related
related	contexts
contexts	of
of	drug
drug	as
as	a
a	
	result
result	both
both	war
war	and
and	trafficking
trafficking	are
are	similar
similar	in
in	being
being	strongly
strongly	indicative
indicative	of
of	the
the	
	sense
sense	of
of	drug
drug	
	words
words	and
and	sentences
sentences	play
play	complementary
complementary	roles
roles	in
in	our
our	approach
approach	a
a	sentence
sentence	is
is	
	represented
represented	by
by	the
the	set
set	of
of	words
words	it
it	contains
contains	and
and	a
a	word
word	by
by	the
the	set
set	of
of	sentences
sentences	in
in	
	which
which	it
it	appears
appears	sentences
sentences	are
are	similar
similar	to
to	the
the	extent
extent	they
they	contain
contain	similar
similar	words
words	2
2	words
words	
	are
are	similar
similar	to
to	the
the	extent
extent	they
they	appear
appear	in
in	similar
similar	sentences
sentences	although
although	this
this	definition
definition	is
is	
	circular
circular	it
it	turns
turns	out
out	to
to	be
be	of
of	great
great	use
use	if
if	applied
applied	iteratively
iteratively	as
as	described
described	below
below	
	in
in	each
each	iteration
iteration	n
n	we
we	update
update	a
a	word
word	similarity
similarity	matrix
matrix	wsmn
wsmn	one
one	matrix
matrix	for
for	each
each	
	polysemous
polysemous	word
word	whose
whose	rows
rows	and
and	columns
columns	are
are	labeled
labeled	by
by	all
all	the
the	words
words	encountered
encountered	
	in
in	the
the	training
training	set
set	of
of	w
w	in
in	that
that	matrix
matrix	the
the	cell
cell	i
i	j
j	holds
holds	a
a	value
value	between
between	0
0	and
and	1
1	
	indicating
indicating	the
the	extent
extent	to
to	which
which	word
word	wi
wi	is
is	contextually
contextually	similar
similar	to
to	word
word	wj
wj	in
in	addition
addition	
	we
we	keep
keep	and
and	update
update	a
a	separate
separate	sentence
sentence	similarity
similarity	matrix
matrix	ssm
ssm	for
for	each
each	sense
sense	sk
sk	of
of	w
w	
	including
including	a
a	matrix
matrix	ssmok
ssmok	that
that	contains
contains	the
the	similarities
similarities	of
of	the
the	original
original	examples
examples	to
to	
	themselves
themselves	the
the	rows
rows	in
in	a
a	sentence
sentence	matrix
matrix	ssm
ssm	correspond
correspond	to
to	the
the	original
original	examples
examples	
	of
of	w
w	and
and	the
the	columns
columns	to
to	the
the	original
original	examples
examples	of
of	w
w	for
for	n
n	0
0	and
and	to
to	the
the	
	examples
examples	for
for	sense
sense	sk
sk	for
for	n
n	0
0	
	to
to	compute
compute	the
the	similarities
similarities	we
we	initialize
initialize	the
the	word
word	similarity
similarity	matrix
matrix	to
to	the
the	identity
identity	
	matrix
matrix	each
each	word
word	is
is	fully
fully	similar
similar	to
to	itself
itself	and
and	completely
completely	dissimilar
dissimilar	to
to	other
other	words
words	
	and
and	iterate
iterate	see
see	figure
figure	1
1	
	1
1	update
update	the
the	sentence
sentence	similarity
similarity	matrices
matrices	ssm
ssm	using
using	the
the	word
word	similarity
similarity	
	matrix
matrix	wsmn
wsmn	
	2
2	ignoring
ignoring	word
word	order
order	this
this	information
information	can
can	be
be	put
put	to
to	use
use	by
by	including
including	in
in	the
the	feature
feature	set
set	see
see	
	section
section	4
4	2
2	
	43
43	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	word
word	
	similarity
similarity	
	matrix
matrix	
	sentence
sentence	
	similarity
similarity	
	matrix
matrix	
	figure
figure	1
1	
	iterative
iterative	computation
computation	of
of	word
word	and
and	sentence
sentence	similarities
similarities	
	update
update	the
the	word
word	similarity
similarity	matrix
matrix	wsmn
wsmn	using
using	the
the	sentence
sentence	similarity
similarity	
	matrices
matrices	ssm
ssm	
	until
until	the
the	changes
changes	in
in	the
the	similarity
similarity	values
values	are
are	small
small	enough
enough	see
see	section
section	a
a	1
1	of
of	the
the	
	appendix
appendix	for
for	a
a	detailed
detailed	description
description	of
of	the
the	stopping
stopping	conditions
conditions	a
a	proof
proof	of
of	convergence
convergence	
	appears
appears	in
in	the
the	appendix
appendix	
	2
2	2
2	1
1	the
the	affinity
affinity	formulae
formulae	the
the	algorithm
algorithm	for
for	updating
updating	the
the	similarity
similarity	matrices
matrices	involves
involves	
	an
an	auxiliary
auxiliary	relation
relation	between
between	words
words	and
and	sentences
sentences	which
which	we
we	call
call	affinity
affinity	
	introduced
introduced	to
to	simplify
simplify	the
the	symmetric
symmetric	iterative
iterative	treatment
treatment	of
of	similarity
similarity	between
between	words
words	
	and
and	sentences
sentences	a
a	word
word	w
w	is
is	assumed
assumed	to
to	have
have	a
a	certain
certain	affinity
affinity	to
to	every
every	sentence
sentence	affinity
affinity	
	a
a	real
real	number
number	between
between	0
0	and
and	1
1	reflects
reflects	the
the	contextual
contextual	relationships
relationships	between
between	w
w	
	and
and	the
the	words
words	of
of	the
the	sentence
sentence	if
if	w
w	belongs
belongs	to
to	a
a	sentence
sentence	s
s	its
its	affinity
affinity	to
to	s
s	is
is	1
1	if
if	
	w
w	is
is	totally
totally	unrelated
unrelated	to
to	s
s	the
the	affinity
affinity	is
is	close
close	to
to	0
0	this
this	is
is	the
the	most
most	common
common	case
case	
	if
if	w
w	is
is	contextually
contextually	similar
similar	to
to	the
the	words
words	of
of	s
s	its
its	affinity
affinity	to
to	s
s	is
is	between
between	0
0	and
and	1
1	
	in
in	a
a	symmetric
symmetric	manner
manner	a
a	sentence
sentence	s
s	has
has	some
some	affinity
affinity	to
to	every
every	word
word	reflecting
reflecting	the
the	
	similarity
similarity	of
of	s
s	to
to	sentences
sentences	involving
involving	that
that	word
word	
	we
we	say
say	that
that	a
a	word
word	belongs
belongs	to
to	a
a	sentence
sentence	denoted
denoted	as
as	w
w	e
e	s
s	if
if	it
it	is
is	textually
textually	
	contained
contained	there
there	in
in	this
this	case
case	sentence
sentence	is
is	said
said	to
to	include
include	the
the	word
word	s
s	9
9	w
w	affinity
affinity	is
is	
	then
then	defined
defined	as
as	follows
follows	
	affn
affn	w
w	s
s	max
max	simn
simn	w
w	wi
wi	1
1	
	wics
wics	
	aff
aff	s
s	w
w	max
max	simn
simn	s
s	sj
sj	2
2	
	8j
8j	
	where
where	n
n	denotes
denotes	the
the	iteration
iteration	number
number	and
and	the
the	similarity
similarity	values
values	are
are	defined
defined	by
by	the
the	
	word
word	and
and	sentence
sentence	similarity
similarity	matrices
matrices	wsmn
wsmn	and
and	ssmn
ssmn	3
3	the
the	initial
initial	representation
representation	
	of
of	a
a	sentence
sentence	as
as	the
the	set
set	of
of	words
words	that
that	it
it	directly
directly	contains
contains	is
is	now
now	augmented
augmented	by
by	a
a	
	3
3	at
at	first
first	glance
glance	it
it	may
may	seem
seem	that
that	the
the	mean
mean	rather
rather	than
than	the
the	maximal
maximal	similarity
similarity	of
of	w
w	to
to	the
the	words
words	of
of	a
a	
	sentence
sentence	should
should	determine
determine	the
the	affinity
affinity	between
between	the
the	two
two	however
however	any
any	definition
definition	of
of	affinity
affinity	that
that	takes
takes	
	into
into	account
account	more
more	words
words	than
than	just
just	the
the	one
one	with
with	the
the	maximal
maximal	similarity
similarity	to
to	w
w	may
may	result
result	in
in	a
a	word
word	
	being
being	directly
directly	contained
contained	in
in	the
the	sentence
sentence	but
but	having
having	an
an	affinity
affinity	to
to	it
it	that
that	is
is	smaller
smaller	than
than	1
1	
	44
44	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	representation
representation	the
the	sentence
sentence	contains
contains	more
more	information
information	or
or	features
features	
	than
than	the
the	words
words	directly
directly	contained
contained	in
in	it
it	every
every	word
word	has
has	some
some	affinity
affinity	to
to	the
the	sentence
sentence	
	and
and	the
the	sentence
sentence	can
can	be
be	represented
represented	by
by	a
a	vector
vector	indicating
indicating	the
the	affinity
affinity	of
of	each
each	word
word	
	to
to	it
it	similarly
similarly	every
every	word
word	can
can	be
be	represented
represented	by
by	the
the	affinity
affinity	of
of	every
every	sentence
sentence	to
to	it
it	
	note
note	that
that	affinity
affinity	is
is	asymmetric
asymmetric	aft
aft	s
s	14
14	aft
aft	w
w	s
s	because
because	14
14	may
may	be
be	similar
similar	to
to	
	one
one	of
of	the
the	words
words	in
in	s
s	which
which	however
however	is
is	not
not	one
one	of
of	the
the	topic
topic	words
words	of
of	s
s	it
it	is
is	not
not	an
an	
	important
important	word
word	in
in	s
s	in
in	this
this	case
case	aft
aft	w
w	s
s	is
is	high
high	because
because	14
14	is
is	similar
similar	to
to	a
a	word
word	
	in
in	s
s	but
but	aft
aft	s
s	14
14	is
is	low
low	because
because	s
s	is
is	not
not	a
a	representative
representative	example
example	of
of	the
the	usage
usage	of
of	
	the
the	word
word	14
14	
	2
2	2
2	2
2	the
the	similarity
similarity	formulae
formulae	we
we	define
define	the
the	similarity
similarity	of
of	14
14	to
to	14
14	to
to	be
be	the
the	average
average	
	affinity
affinity	of
of	sentences
sentences	that
that	include
include	14
14	to
to	14
14	the
the	similarity
similarity	of
of	a
a	sentence
sentence	to
to	another
another	
	sentence
sentence	82
82	is
is	a
a	weighted
weighted	average
average	of
of	the
the	affinity
affinity	of
of	the
the	words
words	in
in	to
to	
	simn
simn	sb
sb	weight
weight	w
w	affn
affn	w
w	3
3	
	we
we	
	simn
simn	14
14	14
14	weight
weight	s
s	14
14	affn
affn	s
s	14
14	4
4	
	
	where
where	the
the	weights
weights	sum
sum	to
to	1
1	4
4	these
these	values
values	are
are	used
used	to
to	update
update	the
the	corresponding
corresponding	
	entries
entries	of
of	the
the	word
word	and
and	sentence
sentence	similarity
similarity	matrices
matrices	wsm
wsm	and
and	ssm
ssm	
	2
2	2
2	3
3	the
the	importance
importance	of
of	iteration
iteration	initially
initially	only
only	identical
identical	words
words	are
are	considered
considered	similar
similar	
	so
so	that
that	aft
aft	w
w	s
s	1
1	if
if	14
14	e
e	s
s	the
the	affinity
affinity	is
is	zero
zero	otherwise
otherwise	thus
thus	in
in	the
the	first
first	
	iteration
iteration	the
the	similarity
similarity	between
between	81
81	and
and	depends
depends	on
on	the
the	number
number	of
of	words
words	from
from	
	that
that	appear
appear	in
in	divided
divided	by
by	the
the	length
length	of
of	note
note	that
that	each
each	word
word	may
may	carry
carry	
	different
different	weight
weight	in
in	the
the	subsequent
subsequent	iterations
iterations	each
each	word
word	14
14	c
c	contributes
contributes	to
to	the
the	
	similarity
similarity	of
of	81
81	to
to	a
a	value
value	between
between	0
0	and
and	1
1	indicating
indicating	its
its	affinity
affinity	to
to	instead
instead	of
of	
	voting
voting	either
either	0
0	if
if	14
14	e
e	or
or	1
1	if
if	14
14	82
82	analogously
analogously	sentences
sentences	contribute
contribute	values
values	
	to
to	word
word	similarity
similarity	
	one
one	may
may	view
view	the
the	iterations
iterations	as
as	successively
successively	capturing
capturing	parameterized
parameterized	genealogical
genealogical	
	relationships
relationships	let
let	words
words	that
that	share
share	contexts
contexts	be
be	called
called	direct
direct	relatives
relatives	then
then	words
words	
	that
that	share
share	neighbors
neighbors	have
have	similar
similar	patterns
patterns	are
are	relatives
relatives	
	these
these	two
two	family
family	relationships
relationships	are
are	captured
captured	by
by	the
the	first
first	iteration
iteration	and
and	also
also	by
by	most
most	
	traditional
traditional	similarity
similarity	measures
measures	which
which	are
are	based
based	on
on	the
the	second
second	iteration
iteration	
	then
then	brings
brings	together
together	relatives
relatives	the
the	third
third	iteration
iteration	captures
captures	higher
higher	
	similarity
similarity	relationships
relationships	and
and	so
so	on
on	note
note	that
that	the
the	level
level	of
of	relationship
relationship	here
here	is
is	a
a	gradually
gradually	
	consolidated
consolidated	quantity
quantity	and
and	is
is	dictated
dictated	by
by	the
the	amount
amount	and
and	the
the	quality
quality	
	of
of	the
the	evidence
evidence	gleaned
gleaned	from
from	the
the	corpus
corpus	it
it	is
is	not
not	an
an	relatedness
relatedness	tag
tag	as
as	
	in
in	genealogy
genealogy	
	the
the	following
following	simple
simple	example
example	demonstrates
demonstrates	the
the	difference
difference	between
between	our
our	similarity
similarity	
	measure
measure	and
and	pure
pure	similarity
similarity	measures
measures	which
which	cannot
cannot	capture
capture	
	4
4	the
the	weight
weight	of
of	a
a	word
word	estimates
estimates	its
its	expected
expected	contribution
contribution	to
to	the
the	disambiguation
disambiguation	task
task	and
and	is
is	a
a	product
product	
	of
of	several
several	factors
factors	the
the	frequency
frequency	of
of	the
the	word
word	in
in	the
the	corpus
corpus	its
its	frequency
frequency	in
in	the
the	training
training	set
set	relative
relative	to
to	
	that
that	in
in	the
the	entire
entire	corpus
corpus	the
the	textual
textual	distance
distance	from
from	the
the	target
target	word
word	and
and	its
its	part
part	of
of	speech
speech	more
more	details
details	
	on
on	word
word	weights
weights	appear
appear	in
in	section
section	a
a	3
3	of
of	the
the	appendix
appendix	all
all	the
the	sentences
sentences	that
that	include
include	a
a	given
given	word
word	
	are
are	assigned
assigned	identical
identical	weights
weights	
	45
45	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	relationships
relationships	consider
consider	the
the	set
set	of
of	three
three	sentence
sentence	fragments
fragments	
	sl
sl	eat
eat	banana
banana	
	s2
s2	taste
taste	banana
banana	
	s3
s3	eat
eat	apple
apple	
	in
in	this
this	corpus
corpus	the
the	contextual
contextual	similarity
similarity	of
of	taste
taste	and
and	apple
apple	according
according	to
to	the
the	
	based
based	methods
methods	is
is	0
0	because
because	the
the	contexts
contexts	of
of	these
these	two
two	words
words	are
are	disjoint
disjoint	
	in
in	comparison
comparison	our
our	iterative
iterative	algorithm
algorithm	will
will	capture
capture	some
some	contextual
contextual	similarity
similarity	
	initialization
initialization	every
every	word
word	is
is	similar
similar	to
to	itself
itself	only
only	
	first
first	iteration
iteration	the
the	sentences
sentences	eat
eat	banana
banana	and
and	eat
eat	apple
apple	have
have	contextual
contextual	
	similarity
similarity	of
of	0
0	5
5	because
because	of
of	the
the	common
common	word
word	eat
eat	furthermore
furthermore	the
the	
	sentences
sentences	eat
eat	banana
banana	and
and	taste
taste	banana
banana	have
have	contextual
contextual	similarity
similarity	0
0	5
5	
	banana
banana	is
is	learned
learned	to
to	be
be	similar
similar	to
to	apple
apple	because
because	of
of	their
their	common
common	
	usage
usage	eat
eat	banana
banana	and
and	eat
eat	apple
apple	
	taste
taste	is
is	similar
similar	to
to	eat
eat	because
because	of
of	their
their	common
common	usage
usage	taste
taste	banana
banana	
	and
and	eat
eat	banana
banana	
	taste
taste	and
and	apple
apple	are
are	not
not	similar
similar	yet
yet	
	second
second	iteration
iteration	the
the	sentence
sentence	taste
taste	banana
banana	has
has	now
now	some
some	similarity
similarity	to
to	eat
eat	
	apple
apple	because
because	in
in	the
the	previous
previous	iteration
iteration	taste
taste	was
was	similar
similar	to
to	eat
eat	and
and	banana
banana	
	was
was	similar
similar	to
to	apple
apple	the
the	word
word	taste
taste	is
is	now
now	similar
similar	to
to	apple
apple	because
because	the
the	
	taste
taste	sentence
sentence	taste
taste	banana
banana	is
is	similar
similar	to
to	the
the	apple
apple	sentence
sentence	eat
eat	apple
apple	yet
yet	
	banana
banana	is
is	more
more	similar
similar	to
to	apple
apple	than
than	taste
taste	because
because	the
the	similarity
similarity	value
value	of
of	
	banana
banana	and
and	apple
apple	further
further	increases
increases	in
in	the
the	second
second	iteration
iteration	
	this
this	simple
simple	example
example	demonstrates
demonstrates	the
the	transitivity
transitivity	of
of	our
our	similarity
similarity	measure
measure	which
which	
	allows
allows	it
it	to
to	extract
extract	contextual
contextual	relationships
relationships	in
in	more
more	complex
complex	situations
situations	the
the	
	spread
spread	of
of	similarity
similarity	is
is	slower
slower	because
because	each
each	word
word	is
is	represented
represented	
	by
by	many
many	more
more	sentences
sentences	
	the
the	most
most	important
important	properties
properties	of
of	the
the	similarity
similarity	computation
computation	algorithm
algorithm	are
are	convergence
convergence	
	see
see	section
section	a
a	2
2	in
in	the
the	appendix
appendix	and
and	utility
utility	in
in	supporting
supporting	disambiguation
disambiguation	
	described
described	in
in	section
section	3
3	three
three	other
other	properties
properties	are
are	as
as	follows
follows	first
first	word
word	similarity
similarity	
	computed
computed	according
according	to
to	the
the	above
above	algorithm
algorithm	is
is	asymmetric
asymmetric	for
for	example
example	drug
drug	is
is	more
more	
	similar
similar	to
to	traffic
traffic	than
than	traffic
traffic	is
is	to
to	drug
drug	because
because	traffic
traffic	is
is	mentioned
mentioned	more
more	frequently
frequently	
	in
in	drug
drug	contexts
contexts	than
than	drug
drug	is
is	mentioned
mentioned	in
in	contexts
contexts	of
of	traffic
traffic	which
which	has
has	many
many	other
other	
	usages
usages	likewise
likewise	sentence
sentence	similarity
similarity	is
is	asymmetric
asymmetric	if
if	81
81	is
is	fully
fully	contained
contained	in
in	then
then	
	sire
sire	s1
s1	1
1	whereas
whereas	sim
sim	1
1	second
second	words
words	with
with	a
a	small
small	count
count	in
in	the
the	
	training
training	set
set	will
will	have
have	unreliable
unreliable	similarity
similarity	values
values	these
these	however
however	are
are	multiplied
multiplied	by
by	a
a	
	very
very	low
low	weight
weight	when
when	used
used	in
in	sentence
sentence	similarity
similarity	evaluation
evaluation	because
because	the
the	frequency
frequency	
	in
in	the
the	training
training	set
set	is
is	taken
taken	into
into	account
account	in
in	computing
computing	the
the	word
word	weights
weights	third
third	in
in	
	the
the	computation
computation	of
of	sim
sim	w1
w1	w2
w2	for
for	a
a	very
very	frequent
frequent	w2
w2	the
the	set
set	of
of	its
its	sentences
sentences	is
is	
	very
very	large
large	potentially
potentially	inflating
inflating	the
the	affinity
affinity	of
of	w1
w1	to
to	the
the	sentences
sentences	that
that	contain
contain	w2
w2	we
we	
	counter
counter	this
this	tendency
tendency	by
by	multiplying
multiplying	sire
sire	w1
w1	w2
w2	by
by	a
a	weight
weight	that
that	is
is	reciprocally
reciprocally	
	related
related	to
to	the
the	global
global	frequency
frequency	of
of	w2
w2	this
this	weight
weight	has
has	been
been	left
left	out
out	of
of	equation
equation	4
4	to
to	
	keep
keep	the
the	notation
notation	there
there	simple
simple	
	46
46	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	2
2	3
3	using
using	similarity
similarity	to
to	tag
tag	the
the	training
training	set
set	
	following
following	convergence
convergence	each
each	sentence
sentence	in
in	the
the	training
training	set
set	is
is	assigned
assigned	the
the	sense
sense	of
of	its
its	
	most
most	similar
similar	sentence
sentence	in
in	one
one	of
of	the
the	feedback
feedback	sets
sets	of
of	sense
sense	si
si	using
using	the
the	final
final	sentence
sentence	
	similarity
similarity	matrix
matrix	note
note	that
that	some
some	sentences
sentences	in
in	the
the	training
training	set
set	belong
belong	also
also	to
to	one
one	of
of	the
the	
	feedback
feedback	sets
sets	because
because	they
they	contain
contain	words
words	from
from	the
the	mrd
mrd	definition
definition	of
of	the
the	target
target	word
word	
	those
those	sentences
sentences	are
are	automatically
automatically	assigned
assigned	the
the	sense
sense	of
of	the
the	feedback
feedback	set
set	to
to	which
which	they
they	
	belong
belong	since
since	they
they	are
are	most
most	similar
similar	to
to	themselves
themselves	note
note	also
also	that
that	an
an	original
original	trainingset
trainingset	
	sentence
sentence	s
s	can
can	be
be	attracted
attracted	to
to	a
a	sentence
sentence	t
t	from
from	a
a	feedback
feedback	set
set	even
even	if
if	s
s	and
and	t
t	
	do
do	not
not	share
share	any
any	word
word	because
because	of
of	the
the	transitivity
transitivity	of
of	the
the	similarity
similarity	measure
measure	
	2
2	4
4	learning
learning	the
the	typical
typical	uses
uses	of
of	each
each	sense
sense	
	we
we	partition
partition	the
the	examples
examples	of
of	each
each	sense
sense	into
into	typical
typical	use
use	sets
sets	by
by	grouping
grouping	all
all	the
the	
	sentences
sentences	that
that	were
were	attracted
attracted	to
to	the
the	same
same	sentence
sentence	that
that	sentence
sentence	and
and	
	all
all	the
the	original
original	sentences
sentences	attracted
attracted	to
to	it
it	form
form	a
a	class
class	of
of	examples
examples	for
for	a
a	typical
typical	usage
usage	
	examples
examples	that
that	did
did	not
not	attract
attract	any
any	original
original	sentences
sentences	are
are	discarded
discarded	if
if	the
the	
	number
number	of
of	resulting
resulting	classes
classes	is
is	too
too	high
high	further
further	clustering
clustering	can
can	be
be	carried
carried	out
out	on
on	the
the	
	basis
basis	of
of	the
the	distance
distance	metric
metric	defined
defined	by
by	1
1	sim
sim	x
x	y
y	where
where	sire
sire	x
x	y
y	are
are	values
values	taken
taken	
	from
from	the
the	final
final	sentence
sentence	similarity
similarity	matrix
matrix	
	a
a	typical
typical	usage
usage	of
of	a
a	sense
sense	is
is	represented
represented	by
by	the
the	affinity
affinity	information
information	generalized
generalized	
	from
from	its
its	examples
examples	for
for	each
each	word
word	14
14	and
and	each
each	cluster
cluster	c
c	of
of	examples
examples	of
of	the
the	same
same	
	usage
usage	we
we	define
define	
	aff
aff	w
w	c
c	max
max	aff
aff	w
w	s
s	5
5	
	sec
sec	
	max
max	max
max	sim
sim	w
w	wi
wi	6
6	
	sec
sec	wics
wics	
	for
for	each
each	cluster
cluster	we
we	construct
construct	its
its	affinity
affinity	vector
vector	whose
whose	ith
ith	component
component	indicates
indicates	the
the	
	affinity
affinity	of
of	word
word	i
i	to
to	the
the	cluster
cluster	it
it	suffices
suffices	to
to	generalize
generalize	the
the	affinity
affinity	information
information	rather
rather	
	than
than	similarity
similarity	because
because	new
new	examples
examples	are
are	judged
judged	on
on	the
the	basis
basis	of
of	their
their	similarity
similarity	to
to	
	each
each	cluster
cluster	in
in	the
the	computation
computation	of
of	sire
sire	s1
s1	equation
equation	3
3	the
the	only
only	information
information	
	concerning
concerning	is
is	its
its	affinity
affinity	values
values	
	2
2	5
5	testing
testing	new
new	examples
examples	
	given
given	a
a	new
new	sentence
sentence	s
s	containing
containing	a
a	target
target	word
word	w
w	we
we	determine
determine	its
its	sense
sense	by
by	computing
computing	
	the
the	similarity
similarity	of
of	s
s	to
to	each
each	of
of	the
the	previously
previously	obtained
obtained	clusters
clusters	ck
ck	and
and	returning
returning	
	the
the	sense
sense	si
si	of
of	the
the	most
most	similar
similar	cluster
cluster	
	sim
sim	snew
snew	ck
ck	weight
weight	w
w	snew
snew	aff
aff	w
w	ck
ck	7
7	
	w
w	e
e	s
s	ew
ew	
	sim
sim	snew
snew	si
si	max
max	sim
sim	snew
snew	c
c	8
8	
	ccsi
ccsi	
	3
3	experimental
experimental	evaluation
evaluation	of
of	the
the	method
method	
	we
we	tested
tested	the
the	algorithm
algorithm	on
on	the
the	corpus
corpus	which
which	contains
contains	one
one	million
million	words
words	
	from
from	the
the	wall
wall	street
street	journal
journal	1989
1989	and
and	is
is	considered
considered	a
a	small
small	corpus
corpus	for
for	the
the	present
present	
	task
task	during
during	the
the	development
development	and
and	the
the	tuning
tuning	of
of	the
the	algorithm
algorithm	we
we	used
used	the
the	method
method	
	of
of	pseudowords
pseudowords	gale
gale	church
church	and
and	yarowsky
yarowsky	1992
1992	schitze
schitze	1992
1992	to
to	avoid
avoid	the
the	need
need	
	for
for	manual
manual	verification
verification	of
of	the
the	resulting
resulting	sense
sense	tags
tags	
	the
the	method
method	of
of	pseudowords
pseudowords	is
is	based
based	on
on	the
the	observation
observation	that
that	a
a	disambiguation
disambiguation	
	process
process	designed
designed	to
to	distinguish
distinguish	between
between	two
two	meanings
meanings	of
of	the
the	same
same	word
word	should
should	also
also	
	47
47	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	table
table	1
1	
	the
the	four
four	polysemous
polysemous	test
test	words
words	and
and	the
the	seed
seed	words
words	they
they	generated
generated	with
with	the
the	use
use	of
of	the
the	mrd
mrd	
	word
word	seed
seed	words
words	
	drug
drug	1
1	stimulant
stimulant	alcoholist
alcoholist	alcohol
alcohol	trafficker
trafficker	crime
crime	
	2
2	medicine
medicine	pharmaceutical
pharmaceutical	remedy
remedy	cure
cure	medication
medication	pharmacists
pharmacists	
	prescription
prescription	
	1
1	conviction
conviction	judgment
judgment	acquittal
acquittal	term
term	
	2
2	string
string	word
word	constituent
constituent	dialogue
dialogue	talk
talk	conversation
conversation	text
text	
	1
1	trial
trial	litigation
litigation	receivership
receivership	bankruptcy
bankruptcy	appeal
appeal	action
action	case
case	lawsuit
lawsuit	
	foreclosure
foreclosure	proceeding
proceeding	
	2
2	garment
garment	fabric
fabric	trousers
trousers	pants
pants	dress
dress	frock
frock	fur
fur	silks
silks	hat
hat	boots
boots	coat
coat	shirt
shirt	
	sweater
sweater	vest
vest	waistcoat
waistcoat	skirt
skirt	jacket
jacket	cloth
cloth	
	1
1	musician
musician	instrumentalist
instrumentalist	performer
performer	artist
artist	actor
actor	twirler
twirler	comedian
comedian	dancer
dancer	
	impersonator
impersonator	imitator
imitator	bandsman
bandsman	jazz
jazz	recorder
recorder	singer
singer	vocalist
vocalist	actress
actress	
	barnstormer
barnstormer	playactor
playactor	trouper
trouper	character
character	actor
actor	star
star	baseball
baseball	
	ball
ball	football
football	basketball
basketball	
	2
2	participant
participant	contestant
contestant	trader
trader	analyst
analyst	dealer
dealer	
	sentence
sentence	
	suit
suit	
	player
player	
	be
be	able
able	to
to	separate
separate	the
the	meanings
meanings	of
of	two
two	different
different	words
words	thus
thus	a
a	data
data	set
set	for
for	testing
testing	
	a
a	disambiguation
disambiguation	algorithm
algorithm	can
can	be
be	obtained
obtained	by
by	starting
starting	with
with	two
two	collections
collections	of
of	sentences
sentences	
	one
one	containing
containing	a
a	word
word	x
x	and
and	the
the	other
other	a
a	word
word	y
y	and
and	inserting
inserting	y
y	instead
instead	of
of	
	every
every	appearance
appearance	of
of	in
in	the
the	first
first	collection
collection	the
the	algorithm
algorithm	is
is	then
then	tested
tested	on
on	the
the	union
union	
	of
of	the
the	two
two	collections
collections	in
in	which
which	x
x	is
is	now
now	a
a	polysemous
polysemous	word
word	the
the	performance
performance	of
of	
	the
the	algorithm
algorithm	is
is	judged
judged	by
by	its
its	ability
ability	to
to	separate
separate	the
the	sentences
sentences	that
that	originally
originally	contained
contained	
	from
from	those
those	that
that	originally
originally	contained
contained	32
32	any
any	mistakes
mistakes	can
can	be
be	used
used	to
to	supervise
supervise	the
the	
	tuning
tuning	of
of	the
the	algorithm
algorithm	5
5	
	3
3	1
1	test
test	data
data	
	the
the	final
final	algorithm
algorithm	was
was	tested
tested	on
on	a
a	total
total	of
of	500
500	examples
examples	of
of	four
four	polysemous
polysemous	words
words	
	drug
drug	sentence
sentence	suit
suit	and
and	player
player	see
see	table
table	2
2	although
although	we
we	confined
confined	the
the	tests
tests	to
to	nouns
nouns	
	the
the	algorithm
algorithm	is
is	applicable
applicable	to
to	any
any	part
part	of
of	speech
speech	the
the	relatively
relatively	small
small	number
number	of
of	
	polysemous
polysemous	words
words	we
we	studied
studied	was
was	dictated
dictated	by
by	the
the	size
size	and
and	nature
nature	of
of	the
the	corpus
corpus	we
we	
	are
are	currently
currently	testing
testing	additional
additional	words
words	using
using	texts
texts	from
from	the
the	british
british	national
national	corpus
corpus	
	as
as	the
the	mrd
mrd	we
we	used
used	a
a	combination
combination	of
of	the
the	versions
versions	of
of	the
the	and
and	
	the
the	oxford
oxford	dictionaries
dictionaries	and
and	the
the	wordnet
wordnet	system
system	the
the	latter
latter	used
used	as
as	a
a	thesaurus
thesaurus	only
only	
	see
see	section
section	4
4	3
3	the
the	resulting
resulting	collection
collection	of
of	seed
seed	words
words	that
that	is
is	words
words	used
used	to
to	generate
generate	
	the
the	feedback
feedback	sets
sets	is
is	listed
listed	in
in	table
table	1
1	
	we
we	found
found	that
that	the
the	single
single	best
best	source
source	of
of	seed
seed	words
words	was
was	wordnet
wordnet	used
used	as
as	thesaurus
thesaurus	
	only
only	the
the	number
number	of
of	seed
seed	words
words	per
per	sense
sense	turned
turned	out
out	to
to	be
be	of
of	little
little	significance
significance	
	for
for	example
example	whereas
whereas	the
the	mrd
mrd	yielded
yielded	many
many	words
words	to
to	be
be	used
used	as
as	
	seeds
seeds	for
for	suit
suit	in
in	the
the	sense
sense	these
these	generated
generated	a
a	small
small	feedback
feedback	set
set	because
because	
	of
of	the
the	low
low	frequency
frequency	of
of	words
words	in
in	the
the	training
training	corpus
corpus	in
in	comparison
comparison	
	there
there	was
was	a
a	strong
strong	correlation
correlation	between
between	the
the	size
size	of
of	the
the	feedback
feedback	set
set	and
and	the
the	disambiguation
disambiguation	
	performance
performance	indicating
indicating	that
that	a
a	larger
larger	corpus
corpus	is
is	likely
likely	to
to	improve
improve	the
the	results
results	
	as
as	can
can	be
be	seen
seen	from
from	the
the	above
above	the
the	original
original	training
training	data
data	before
before	the
the	addition
addition	of
of	
	5
5	note
note	that
that	our
our	disambiguation
disambiguation	algorithm
algorithm	works
works	the
the	best
best	for
for	polysemous
polysemous	words
words	whose
whose	senses
senses	are
are	
	unrelated
unrelated	to
to	each
each	other
other	in
in	which
which	case
case	the
the	overlap
overlap	between
between	the
the	feedback
feedback	sets
sets	is
is	minimized
minimized	likewise
likewise	
	the
the	method
method	of
of	training
training	with
with	pseudowords
pseudowords	amounts
amounts	to
to	an
an	assumption
assumption	of
of	independence
independence	of
of	the
the	different
different	
	senses
senses	
	48
48	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	table
table	2
2	
	a
a	summary
summary	of
of	the
the	performance
performance	on
on	the
the	four
four	test
test	words
words	
	word
word	senses
senses	sample
sample	size
size	feedback
feedback	size
size	correct
correct	correct
correct	
	per
per	sense
sense	total
total	
	drug
drug	narcotic
narcotic	65
65	100
100	92
92	3
3	90
90	5
5	
	medicine
medicine	83
83	65
65	89
89	1
1	
	sentence
sentence	judgment
judgment	23
23	327
327	100
100	0
0	92
92	5
5	
	grammar
grammar	4
4	42
42	50
50	0
0	
	suit
suit	court
court	212
212	1
1	461
461	98
98	6
6	94
94	8
8	
	garment
garment	21
21	81
81	55
55	0
0	
	player
player	performer
performer	48
48	230
230	87
87	5
5	92
92	3
3	
	participant
participant	44
44	1
1	552
552	97
97	7
7	
	the
the	feedback
feedback	sets
sets	consisted
consisted	of
of	a
a	few
few	dozen
dozen	examples
examples	in
in	comparison
comparison	to
to	thousands
thousands	of
of	
	examples
examples	needed
needed	in
in	other
other	methods
methods	sch
sch	1992
1992	yarowsky
yarowsky	1995
1995	the
the	
	average
average	success
success	rate
rate	of
of	our
our	algorithm
algorithm	on
on	the
the	500
500	appearances
appearances	of
of	the
the	four
four	test
test	words
words	
	was
was	92
92	
	3
3	2
2	the
the	drug
drug	experiment
experiment	
	we
we	now
now	present
present	in
in	detail
detail	several
several	of
of	the
the	results
results	obtained
obtained	with
with	the
the	word
word	drug
drug	consider
consider	
	first
first	the
the	effects
effects	of
of	iteration
iteration	a
a	plot
plot	of
of	the
the	improvement
improvement	in
in	the
the	performance
performance	vs
vs	iteration
iteration	
	number
number	appears
appears	in
in	figure
figure	2
2	the
the	success
success	rate
rate	is
is	plotted
plotted	for
for	each
each	sense
sense	and
and	for
for	the
the	
	weighted
weighted	average
average	of
of	both
both	senses
senses	we
we	considered
considered	the
the	weights
weights	are
are	proportional
proportional	to
to	the
the	
	number
number	of
of	examples
examples	of
of	each
each	sense
sense	iterations
iterations	2
2	and
and	5
5	can
can	be
be	seen
seen	to
to	yield
yield	the
the	best
best	
	performance
performance	iteration
iteration	5
5	is
is	to
to	be
be	preferred
preferred	because
because	of
of	the
the	smaller
smaller	difference
difference	between
between	
	the
the	success
success	rates
rates	for
for	the
the	two
two	senses
senses	of
of	the
the	target
target	word
word	
	figure
figure	3
3	shows
shows	how
how	the
the	similarity
similarity	values
values	develop
develop	with
with	iteration
iteration	number
number	for
for	each
each	
	example
example	s
s	of
of	the
the	sense
sense	of
of	drug
drug	the
the	value
value	of
of	simn
simn	s
s	narcotic
narcotic	increases
increases	with
with	n
n	
	figure
figure	4
4	compares
compares	the
the	similarities
similarities	of
of	a
a	example
example	to
to	the
the	sense
sense	
	and
and	to
to	the
the	sense
sense	for
for	each
each	iteration
iteration	one
one	can
can	see
see	that
that	the
the	sense
sense	
	assignment
assignment	made
made	in
in	the
the	first
first	iteration
iteration	is
is	gradually
gradually	suppressed
suppressed	the
the	word
word	menace
menace	
	which
which	is
is	a
a	hint
hint	for
for	the
the	sense
sense	in
in	the
the	sentence
sentence	used
used	in
in	this
this	example
example	did
did	
	not
not	help
help	in
in	the
the	first
first	iteration
iteration	because
because	it
it	did
did	not
not	appear
appear	in
in	the
the	feedback
feedback	
	set
set	at
at	all
all	thus
thus	in
in	iteration
iteration	1
1	the
the	similarity
similarity	of
of	the
the	sentence
sentence	to
to	the
the	sense
sense	
	was
was	0
0	15
15	vs
vs	a
a	similarity
similarity	of
of	0
0	1
1	to
to	the
the	sense
sense	in
in	iteration
iteration	2
2	menace
menace	was
was	
	learned
learned	to
to	be
be	similar
similar	to
to	other
other	words
words	yielding
yielding	a
a	small
small	advantage
advantage	for
for	
	the
the	sense
sense	in
in	iteration
iteration	3
3	further
further	similarity
similarity	values
values	were
were	updated
updated	and
and	there
there	
	was
was	a
a	clear
clear	advantage
advantage	to
to	the
the	sense
sense	0
0	93
93	vs
vs	0
0	89
89	for
for	eventually
eventually	
	all
all	similarity
similarity	values
values	become
become	close
close	to
to	1
1	and
and	because
because	they
they	are
are	bounded
bounded	by
by	1
1	they
they	cannot
cannot	
	change
change	significantly
significantly	with
with	further
further	iterations
iterations	the
the	decision
decision	is
is	therefore
therefore	best
best	made
made	after
after	
	relatively
relatively	few
few	iterations
iterations	as
as	we
we	just
just	saw
saw	
	table
table	3
3	shows
shows	the
the	most
most	similar
similar	words
words	found
found	for
for	the
the	words
words	with
with	the
the	highest
highest	weights
weights	
	in
in	the
the	drug
drug	example
example	words
words	have
have	been
been	omitted
omitted	note
note	that
that	the
the	similarity
similarity	
	is
is	contextual
contextual	and
and	is
is	affected
affected	by
by	the
the	polysemous
polysemous	target
target	word
word	for
for	example
example	trafficking
trafficking	
	was
was	found
found	to
to	be
be	similar
similar	to
to	crime
crime	because
because	in
in	drug
drug	contexts
contexts	the
the	expressions
expressions	drug
drug	trafficking
trafficking	
	and
and	crime
crime	are
are	highly
highly	related
related	in
in	general
general	trafficking
trafficking	and
and	crime
crime	need
need	not
not	be
be	similar
similar	of
of	
	course
course	
	49
49	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	table
table	3
3	
	the
the	drug
drug	experiment
experiment	the
the	nearest
nearest	neighbors
neighbors	of
of	the
the	words
words	
	word
word	most
most	contextually
contextually	similar
similar	words
words	
	the
the	sense
sense	
	medication
medication	antibiotic
antibiotic	blood
blood	prescription
prescription	medicine
medicine	percentage
percentage	pressure
pressure	
	prescription
prescription	analyst
analyst	antibiotic
antibiotic	blood
blood	campaign
campaign	introduction
introduction	law
law	medication
medication	medicine
medicine	
	percentage
percentage	print
print	profit
profit	publicity
publicity	quarter
quarter	sedative
sedative	state
state	television
television	tranquilizer
tranquilizer	use
use	
	medicine
medicine	prescription
prescription	campaign
campaign	competition
competition	dollar
dollar	earnings
earnings	law
law	manufacturing
manufacturing	
	margin
margin	print
print	product
product	publicity
publicity	quarter
quarter	result
result	sale
sale	saving
saving	sedative
sedative	
	staff
staff	state
state	television
television	tranquilizer
tranquilizer	unit
unit	use
use	
	disease
disease	antibiotic
antibiotic	blood
blood	medication
medication	medicine
medicine	prescription
prescription	
	symptom
symptom	hypoglycemia
hypoglycemia	insulin
insulin	warning
warning	manufacturer
manufacturer	product
product	
	plant
plant	animal
animal	death
death	diabetic
diabetic	evidence
evidence	finding
finding	metabolism
metabolism	study
study	
	insulin
insulin	hypoglycemia
hypoglycemia	manufacturer
manufacturer	product
product	symptom
symptom	warning
warning	
	death
death	diabetic
diabetic	finding
finding	report
report	study
study	
	tranquilizer
tranquilizer	campaign
campaign	law
law	medicine
medicine	prescription
prescription	print
print	publicity
publicity	sedative
sedative	
	television
television	use
use	analyst
analyst	profit
profit	state
state	
	dose
dose	appeal
appeal	death
death	impact
impact	injury
injury	liability
liability	manufacturer
manufacturer	miscarriage
miscarriage	refusing
refusing	ruling
ruling	
	diethylstilbestrol
diethylstilbestrol	hormone
hormone	damage
damage	effect
effect	female
female	prospect
prospect	state
state	
	the
the	sense
sense	
	consumer
consumer	distributor
distributor	effort
effort	cessation
cessation	consumption
consumption	country
country	reduction
reduction	requirement
requirement	
	victory
victory	battle
battle	capacity
capacity	cartel
cartel	government
government	mafia
mafia	newspaper
newspaper	people
people	
	mafia
mafia	terrorism
terrorism	censorship
censorship	dictatorship
dictatorship	newspaper
newspaper	press
press	brother
brother	nothing
nothing	aspiration
aspiration	
	assassination
assassination	editor
editor	leader
leader	politics
politics	rise
rise	action
action	country
country	doubt
doubt	freedom
freedom	
	mafioso
mafioso	medium
medium	menace
menace	solidarity
solidarity	structure
structure	trade
trade	world
world	
	terrorism
terrorism	censorship
censorship	doubt
doubt	freedom
freedom	mafia
mafia	medium
medium	menace
menace	newspaper
newspaper	
	press
press	solidarity
solidarity	structure
structure	
	murder
murder	symbolism
symbolism	trafficking
trafficking	furor
furor	killing
killing	substance
substance	crime
crime	
	restaurant
restaurant	law
law	bill
bill	case
case	problem
problem	
	menace
menace	terrorism
terrorism	freedom
freedom	solidarity
solidarity	structure
structure	medium
medium	press
press	censorship
censorship	country
country	doubt
doubt	
	mafia
mafia	newspaper
newspaper	way
way	attack
attack	government
government	magnitude
magnitude	people
people	relation
relation	threat
threat	
	world
world	
	trafficking
trafficking	crime
crime	furor
furor	killing
killing	murder
murder	restaurant
restaurant	substance
substance	symbolism
symbolism	
	dictatorship
dictatorship	aspiration
aspiration	brother
brother	editor
editor	mafia
mafia	nothing
nothing	politics
politics	press
press	
	assassination
assassination	censorship
censorship	leader
leader	newspaper
newspaper	rise
rise	terrorism
terrorism	
	assassination
assassination	brother
brother	censorship
censorship	dictatorship
dictatorship	mafia
mafia	nothing
nothing	press
press	terrorism
terrorism	
	aspiration
aspiration	editor
editor	leader
leader	newspaper
newspaper	politics
politics	rise
rise	
	laundering
laundering	army
army	lot
lot	money
money	arsenal
arsenal	baron
baron	economy
economy	explosive
explosive	government
government	hand
hand	
	material
material	military
military	none
none	opinion
opinion	portion
portion	talk
talk	
	censorship
censorship	mafia
mafia	newspaper
newspaper	press
press	terrorism
terrorism	country
country	doubt
doubt	freedom
freedom	
	medium
medium	menace
menace	solidarity
solidarity	structure
structure	
	50
50	
	98
98	
	x
x	
	96
96	
	94
94	
	92
92	
	90
90	
	88
88	
	86
86	
	84
84	
	82
82	
	80
80	
	78
78	
	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	2
2	3
3	4
4	5
5	6
6	7
7	8
8	9
9	
	figure
figure	2
2	
	the
the	drug
drug	experiment
experiment	the
the	change
change	in
in	the
the	disambiguation
disambiguation	performance
performance	with
with	iteration
iteration	number
number	is
is	
	plotted
plotted	separately
separately	for
for	each
each	sense
sense	the
the	asterisk
asterisk	marks
marks	the
the	plot
plot	of
of	the
the	success
success	rate
rate	for
for	the
the	
	sense
sense	the
the	other
other	two
two	plots
plots	are
are	the
the	sense
sense	and
and	the
the	weighted
weighted	average
average	of
of	the
the	
	two
two	senses
senses	in
in	our
our	experiments
experiments	the
the	typical
typical	number
number	of
of	iterations
iterations	was
was	3
3	
	similarity
similarity	of
of	sense1
sense1	examples
examples	to
to	sense1
sense1	feedback
feedback	set
set	
	0
0	
	
	
	o
o	
	example
example	
	figure
figure	3
3	
	0
0	0
0	
	iteration
iteration	
	10
10	
	the
the	drug
drug	experiment
experiment	an
an	illustration
illustration	of
of	the
the	development
development	of
of	the
the	support
support	for
for	a
a	particular
particular	sense
sense	
	with
with	iteration
iteration	the
the	plot
plot	shows
shows	the
the	similarity
similarity	of
of	a
a	number
number	of
of	drug
drug	sentences
sentences	to
to	the
the	
	sense
sense	to
to	facilitate
facilitate	visualization
visualization	the
the	curves
curves	are
are	sorted
sorted	by
by	the
the	values
values	of
of	
	similarity
similarity	
	4
4	discussion
discussion	
	we
we	now
now	discuss
discuss	in
in	some
some	detail
detail	the
the	choices
choices	made
made	at
at	the
the	different
different	stages
stages	of
of	the
the	development
development	
	of
of	the
the	present
present	method
method	and
and	its
its	relationship
relationship	to
to	some
some	of
of	the
the	previous
previous	works
works	on
on	
	word
word	sense
sense	disambiguation
disambiguation	
	51
51	
	1
1	t_
t_	
	0
0	9
9	
	0
0	8
8	
	0
0	7
7	
	0
0	6
6	
	0
0	5
5	
	0
0	4
4	
	0
0	3
3	
	0
0	2
2	
	0
0	1
1	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	i
i	i
i	i
i	i
i	i
i	i
i	i
i	t
t	
	2
2	3
3	4
4	5
5	6
6	7
7	8
8	9
9	10
10	
	figure
figure	4
4	
	the
the	drug
drug	experiment
experiment	the
the	similarity
similarity	of
of	a
a	example
example	to
to	each
each	of
of	the
the	two
two	senses
senses	the
the	
	sentence
sentence	here
here	was
was	the
the	american
american	people
people	and
and	their
their	government
government	also
also	woke
woke	up
up	too
too	late
late	to
to	the
the	menace
menace	drugs
drugs	
	posed
posed	to
to	the
the	moral
moral	structure
structure	of
of	their
their	country
country	the
the	asterisk
asterisk	marks
marks	the
the	plot
plot	for
for	the
the	sense
sense	
	4
4	1
1	flexible
flexible	sense
sense	distinctions
distinctions	
	the
the	possibility
possibility	of
of	strict
strict	definition
definition	of
of	each
each	sense
sense	of
of	a
a	polysemous
polysemous	word
word	and
and	the
the	possibility
possibility	
	of
of	unambiguous
unambiguous	assignment
assignment	of
of	a
a	given
given	sense
sense	in
in	a
a	given
given	situation
situation	are
are	in
in	themselves
themselves	
	nontrivial
nontrivial	issues
issues	in
in	philosophy
philosophy	quine
quine	1960
1960	and
and	linguistics
linguistics	weinreich
weinreich	1980
1980	cruse
cruse	
	1986
1986	different
different	dictionaries
dictionaries	often
often	disagree
disagree	on
on	the
the	definitions
definitions	the
the	split
split	into
into	senses
senses	may
may	
	also
also	depend
depend	on
on	the
the	task
task	at
at	hand
hand	thus
thus	it
it	is
is	important
important	to
to	maintain
maintain	the
the	possibility
possibility	of
of	
	flexible
flexible	distinction
distinction	of
of	the
the	different
different	senses
senses	e
e	g
g	letting
letting	this
this	distinction
distinction	be
be	determined
determined	
	by
by	an
an	external
external	knowledge
knowledge	source
source	such
such	as
as	a
a	thesaurus
thesaurus	or
or	a
a	dictionary
dictionary	although
although	this
this	
	requirement
requirement	may
may	seem
seem	trivial
trivial	most
most	methods
methods	do
do	not
not	in
in	fact
fact	allow
allow	such
such	
	flexibility
flexibility	for
for	example
example	defining
defining	the
the	senses
senses	by
by	the
the	possible
possible	translations
translations	of
of	the
the	word
word	
	dagan
dagan	itai
itai	and
and	schwall
schwall	1991
1991	brown
brown	et
et	al
al	1991
1991	gale
gale	church
church	and
and	yarowsky
yarowsky	1992
1992	
	by
by	the
the	categories
categories	yarowsky
yarowsky	1992
1992	or
or	by
by	clustering
clustering	schi
schi	1992
1992	yields
yields	a
a	
	grouping
grouping	that
that	does
does	not
not	always
always	conform
conform	to
to	the
the	desired
desired	sense
sense	distinctions
distinctions	
	in
in	comparison
comparison	to
to	these
these	approaches
approaches	our
our	reliance
reliance	on
on	the
the	mrd
mrd	for
for	the
the	definition
definition	of
of	
	senses
senses	in
in	the
the	initialization
initialization	of
of	the
the	learning
learning	process
process	guarantees
guarantees	the
the	required
required	flexibility
flexibility	
	in
in	setting
setting	the
the	sense
sense	distinctions
distinctions	specifically
specifically	the
the	user
user	of
of	our
our	system
system	may
may	choose
choose	a
a	
	certain
certain	dictionary
dictionary	definition
definition	a
a	combination
combination	of
of	definitions
definitions	from
from	several
several	dictionaries
dictionaries	
	or
or	manually
manually	listed
listed	seed
seed	words
words	for
for	every
every	sense
sense	that
that	needs
needs	to
to	be
be	defined
defined	whereas
whereas	
	pure
pure	methods
methods	allow
allow	the
the	same
same	flexibility
flexibility	their
their	potential
potential	so
so	far
far	has
has	not
not	
	been
been	fully
fully	tapped
tapped	because
because	definitions
definitions	alone
alone	do
do	not
not	contain
contain	enough
enough	information
information	for
for	
	disambiguation
disambiguation	
	4
4	2
2	sentence
sentence	features
features	
	different
different	polysemous
polysemous	words
words	may
may	benefit
benefit	from
from	different
different	types
types	of
of	features
features	of
of	the
the	context
context	
	sentences
sentences	polysemous
polysemous	words
words	for
for	which
which	distinct
distinct	senses
senses	tend
tend	to
to	appear
appear	in
in	different
different	topics
topics	
	can
can	be
be	disambiguated
disambiguated	using
using	single
single	words
words	as
as	the
the	context
context	features
features	as
as	we
we	did
did	here
here	
	disambiguation
disambiguation	of
of	other
other	polysemous
polysemous	words
words	may
may	require
require	taking
taking	the
the	sentence
sentence	structure
structure	
	into
into	account
account	using
using	or
or	syntactic
syntactic	constructs
constructs	as
as	features
features	this
this	additional
additional	information
information	
	can
can	be
be	incorporated
incorporated	into
into	our
our	method
method	by
by	1
1	extracting
extracting	features
features	such
such	as
as	nouns
nouns	
	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	verbs
verbs	adjectives
adjectives	of
of	the
the	target
target	word
word	bigrams
bigrams	trigrams
trigrams	and
and	or
or	
	pairs
pairs	2
2	discarding
discarding	features
features	with
with	a
a	low
low	weight
weight	cf
cf	section
section	a
a	3
3	of
of	the
the	appendix
appendix	and
and	
	3
3	using
using	the
the	remaining
remaining	features
features	instead
instead	of
of	single
single	words
words	i
i	e
e	representing
representing	a
a	sentence
sentence	
	by
by	the
the	set
set	of
of	significant
significant	features
features	it
it	contains
contains	and
and	a
a	feature
feature	by
by	the
the	set
set	of
of	sentences
sentences	
	in
in	which
which	it
it	appears
appears	
	4
4	3
3	using
using	wordnet
wordnet	
	the
the	initialization
initialization	of
of	the
the	word
word	similarity
similarity	matrix
matrix	using
using	wordnet
wordnet	a
a	semantic
semantic	
	network
network	arranged
arranged	in
in	a
a	hierarchical
hierarchical	structure
structure	miller
miller	et
et	al
al	1993
1993	may
may	seem
seem	to
to	
	be
be	advantageous
advantageous	over
over	simply
simply	setting
setting	it
it	to
to	the
the	identity
identity	matrix
matrix	as
as	we
we	have
have	done
done	to
to	
	compare
compare	these
these	two
two	approaches
approaches	we
we	tried
tried	to
to	set
set	the
the	initial
initial	dis
dis	similarity
similarity	between
between	two
two	
	words
words	to
to	the
the	wordnet
wordnet	path
path	length
length	between
between	their
their	nodes
nodes	lee
lee	kim
kim	and
and	lee
lee	1993
1993	and
and	
	then
then	learn
learn	the
the	similarity
similarity	values
values	iteratively
iteratively	this
this	however
however	led
led	to
to	worse
worse	performance
performance	
	than
than	the
the	simple
simple	initialization
initialization	
	there
there	are
are	several
several	possible
possible	reasons
reasons	for
for	the
the	poor
poor	performance
performance	of
of	wordnet
wordnet	in
in	this
this	
	comparison
comparison	first
first	wordnet
wordnet	is
is	not
not	designed
designed	to
to	capture
capture	contextual
contextual	similarity
similarity	for
for	example
example	
	in
in	wordnet
wordnet	hospital
hospital	and
and	doctor
doctor	have
have	no
no	common
common	ancestor
ancestor	and
and	hence
hence	their
their	
	similarity
similarity	is
is	0
0	while
while	doctor
doctor	and
and	lawyer
lawyer	are
are	quite
quite	similar
similar	because
because	both
both	designate
designate	professionals
professionals	
	humans
humans	and
and	living
living	things
things	note
note	that
that	contextually
contextually	doctor
doctor	should
should	be
be	more
more	
	similar
similar	to
to	hospital
hospital	than
than	to
to	lawyer
lawyer	second
second	we
we	found
found	that
that	the
the	wordnet
wordnet	similarity
similarity	values
values	
	dominated
dominated	the
the	contextual
contextual	similarity
similarity	computed
computed	in
in	the
the	iterative
iterative	process
process	preventing
preventing	the
the	
	transitive
transitive	effects
effects	of
of	contextual
contextual	similarity
similarity	from
from	taking
taking	over
over	third
third	the
the	tree
tree	distance
distance	in
in	
	itself
itself	does
does	not
not	always
always	correspond
correspond	to
to	the
the	intuitive
intuitive	notion
notion	of
of	similarity
similarity	because
because	different
different	
	concepts
concepts	appear
appear	at
at	different
different	levels
levels	of
of	abstraction
abstraction	and
and	have
have	a
a	different
different	number
number	of
of	
	nested
nested	subconcepts
subconcepts	for
for	example
example	a
a	certain
certain	distance
distance	between
between	two
two	nodes
nodes	may
may	result
result	
	from
from	1
1	the
the	nodes
nodes	being
being	semantically
semantically	close
close	but
but	separated
separated	by
by	a
a	large
large	distance
distance	stemming
stemming	
	from
from	a
a	high
high	level
level	of
of	detail
detail	in
in	the
the	related
related	synsets
synsets	or
or	from
from	2
2	the
the	nodes
nodes	being
being	
	semantically
semantically	far
far	from
from	each
each	other
other	6
6	
	4
4	4
4	ignoring
ignoring	irrelevant
irrelevant	examples
examples	
	the
the	feedback
feedback	sets
sets	we
we	use
use	in
in	training
training	the
the	system
system	may
may	contain
contain	noise
noise	in
in	the
the	form
form	of
of	
	irrelevant
irrelevant	examples
examples	that
that	are
are	collected
collected	along
along	with
with	the
the	relevant
relevant	and
and	useful
useful	ones
ones	for
for	
	instance
instance	in
in	one
one	of
of	the
the	definitions
definitions	of
of	bank
bank	in
in	wordnet
wordnet	we
we	find
find	bar
bar	which
which	in
in	turn
turn	
	has
has	many
many	other
other	senses
senses	that
that	are
are	not
not	related
related	to
to	bank
bank	although
although	these
these	unrelated
unrelated	senses
senses	
	contribute
contribute	examples
examples	to
to	the
the	feedback
feedback	set
set	our
our	system
system	is
is	hardly
hardly	affected
affected	by
by	this
this	noise
noise	
	because
because	we
we	do
do	not
not	collect
collect	statistics
statistics	on
on	the
the	feedback
feedback	sets
sets	i
i	e
e	method
method	is
is	not
not	based
based	
	on
on	mere
mere	frequencies
frequencies	as
as	most
most	other
other	methods
methods	are
are	the
the	
	relevant
relevant	examples
examples	in
in	the
the	feedback
feedback	set
set	of
of	the
the	sense
sense	si
si	will
will	attract
attract	the
the	examples
examples	of
of	si
si	
	the
the	irrelevant
irrelevant	examples
examples	will
will	not
not	attract
attract	the
the	examples
examples	of
of	si
si	but
but	neither
neither	will
will	they
they	do
do	
	damage
damage	because
because	they
they	are
are	not
not	expected
expected	to
to	attract
attract	examples
examples	of
of	sj
sj	i
i	
	4
4	5
5	related
related	work
work	
	4
4	5
5	1
1	the
the	knowledge
knowledge	acquisition
acquisition	bottleneck
bottleneck	brown
brown	et
et	al
al	1991
1991	and
and	gale
gale	church
church	
	and
and	yarowsky
yarowsky	1992
1992	used
used	the
the	translations
translations	of
of	ambiguous
ambiguous	words
words	in
in	a
a	bilingual
bilingual	corpus
corpus	
	as
as	sense
sense	tags
tags	this
this	does
does	not
not	obviate
obviate	the
the	need
need	for
for	manual
manual	work
work	as
as	producing
producing	bilingual
bilingual	
	corpora
corpora	requires
requires	manual
manual	translation
translation	work
work	dagan
dagan	itai
itai	and
and	schwall
schwall	1991
1991	used
used	a
a	
	bilingual
bilingual	lexicon
lexicon	and
and	a
a	monolingual
monolingual	corpus
corpus	to
to	save
save	the
the	need
need	for
for	translating
translating	the
the	corpus
corpus	
	6
6	resnik
resnik	1995
1995	recently
recently	suggested
suggested	that
that	this
this	particular
particular	difficulty
difficulty	can
can	be
be	overcome
overcome	by
by	a
a	different
different	measure
measure	
	that
that	takes
takes	into
into	account
account	the
the	informativeness
informativeness	of
of	the
the	most
most	specific
specific	common
common	ancestor
ancestor	of
of	the
the	two
two	words
words	
	53
53	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	the
the	problem
problem	remains
remains	however
however	that
that	the
the	word
word	translations
translations	do
do	not
not	necessarily
necessarily	overlap
overlap	
	with
with	the
the	desired
desired	sense
sense	distinctions
distinctions	
	sch
sch	itze
itze	1992
1992	clustered
clustered	the
the	examples
examples	in
in	the
the	training
training	set
set	and
and	manually
manually	assigned
assigned	
	each
each	cluster
cluster	a
a	sense
sense	by
by	observing
observing	members
members	of
of	the
the	cluster
cluster	each
each	sense
sense	was
was	usually
usually	
	represented
represented	by
by	several
several	clusters
clusters	although
although	this
this	approach
approach	significantly
significantly	decreased
decreased	the
the	
	need
need	for
for	manual
manual	intervention
intervention	about
about	a
a	hundred
hundred	examples
examples	had
had	still
still	to
to	be
be	tagged
tagged	manually
manually	
	for
for	each
each	word
word	moreover
moreover	the
the	resulting
resulting	clusters
clusters	did
did	not
not	necessarily
necessarily	correspond
correspond	
	to
to	the
the	desired
desired	sense
sense	distinctions
distinctions	
	yarowsky
yarowsky	1992
1992	learned
learned	discriminators
discriminators	for
for	each
each	category
category	saving
saving	the
the	need
need	
	to
to	separate
separate	the
the	training
training	set
set	into
into	senses
senses	however
however	using
using	such
such	categories
categories	
	usually
usually	leads
leads	to
to	a
a	coverage
coverage	problem
problem	for
for	specific
specific	domains
domains	or
or	for
for	domains
domains	other
other	than
than	
	the
the	one
one	for
for	which
which	the
the	list
list	of
of	categories
categories	has
has	been
been	prepared
prepared	
	using
using	amsler
amsler	1984
1984	for
for	word
word	sense
sense	disambiguation
disambiguation	was
was	popularized
popularized	by
by	
	lesk
lesk	1986
1986	several
several	researchers
researchers	subsequently
subsequently	continued
continued	and
and	improved
improved	this
this	line
line	of
of	
	work
work	krovetz
krovetz	and
and	croft
croft	1989
1989	guthrie
guthrie	et
et	al
al	1991
1991	v
v	and
and	ide
ide	1990
1990	unlike
unlike	the
the	
	information
information	in
in	a
a	corpus
corpus	the
the	information
information	in
in	the
the	mrd
mrd	definitions
definitions	is
is	presorted
presorted	into
into	
	senses
senses	however
however	as
as	noted
noted	above
above	the
the	mrd
mrd	definitions
definitions	alone
alone	do
do	not
not	contain
contain	enough
enough	
	information
information	to
to	allow
allow	reliable
reliable	disambiguation
disambiguation	recently
recently	yarowsky
yarowsky	1995
1995	combined
combined	an
an	
	mrd
mrd	and
and	a
a	corpus
corpus	in
in	a
a	bootstrapping
bootstrapping	process
process	in
in	that
that	work
work	the
the	definition
definition	words
words	
	were
were	used
used	as
as	initial
initial	sense
sense	indicators
indicators	automatically
automatically	tagging
tagging	the
the	target
target	word
word	examples
examples	
	containing
containing	them
them	these
these	tagged
tagged	examples
examples	were
were	then
then	used
used	as
as	seed
seed	examples
examples	in
in	the
the	
	bootstrapping
bootstrapping	process
process	in
in	comparison
comparison	we
we	suggest
suggest	to
to	further
further	combine
combine	the
the	corpus
corpus	and
and	
	the
the	mrd
mrd	by
by	using
using	all
all	the
the	corpus
corpus	examples
examples	of
of	the
the	mrd
mrd	definition
definition	words
words	instead
instead	of
of	
	those
those	words
words	alone
alone	this
this	yields
yields	much
much	more
more	training
training	information
information	
	4
4	5
5	2
2	the
the	problem
problem	of
of	sparse
sparse	data
data	most
most	previous
previous	works
works	define
define	word
word	similarity
similarity	based
based	
	on
on	information
information	and
and	hence
hence	face
face	a
a	severe
severe	problem
problem	of
of	sparse
sparse	data
data	many
many	
	of
of	the
the	possible
possible	are
are	not
not	observed
observed	even
even	in
in	a
a	very
very	large
large	corpus
corpus	church
church	
	and
and	mercer
mercer	1993
1993	our
our	algorithm
algorithm	addresses
addresses	this
this	problem
problem	in
in	two
two	ways
ways	first
first	we
we	replace
replace	
	the
the	indicator
indicator	of
of	by
by	a
a	graded
graded	measure
measure	of
of	contextual
contextual	similarity
similarity	
	our
our	measure
measure	of
of	similarity
similarity	is
is	transitive
transitive	allowing
allowing	two
two	words
words	to
to	be
be	considered
considered	similar
similar	
	even
even	if
if	they
they	neither
neither	appear
appear	in
in	the
the	same
same	sentence
sentence	nor
nor	share
share	neighbor
neighbor	words
words	second
second	
	we
we	extend
extend	the
the	training
training	set
set	by
by	adding
adding	examples
examples	of
of	related
related	words
words	the
the	performance
performance	of
of	
	our
our	system
system	compares
compares	favorably
favorably	to
to	that
that	of
of	systems
systems	trained
trained	on
on	sets
sets	larger
larger	by
by	a
a	factor
factor	
	of
of	100
100	the
the	results
results	described
described	in
in	section
section	3
3	were
were	obtained
obtained	following
following	learning
learning	from
from	several
several	
	dozen
dozen	examples
examples	in
in	comparison
comparison	to
to	thousands
thousands	of
of	examples
examples	in
in	other
other	automatic
automatic	methods
methods	
	traditionally
traditionally	the
the	problem
problem	of
of	sparse
sparse	data
data	is
is	approached
approached	by
by	estimating
estimating	the
the	probability
probability	
	of
of	unobserved
unobserved	using
using	the
the	actual
actual	in
in	the
the	training
training	
	set
set	this
this	can
can	be
be	done
done	by
by	smoothing
smoothing	the
the	observed
observed	frequencies
frequencies	7
7	church
church	and
and	mercer
mercer	
	1993
1993	or
or	by
by	methods
methods	brown
brown	et
et	al
al	1991
1991	pereira
pereira	and
and	tishby
tishby	1992
1992	pereira
pereira	
	tishby
tishby	and
and	lee
lee	1993
1993	hirschman
hirschman	1986
1986	resnik
resnik	1992
1992	brill
brill	et
et	al
al	1990
1990	dagan
dagan	marcus
marcus	
	and
and	markovitch
markovitch	1993
1993	in
in	comparison
comparison	to
to	these
these	approaches
approaches	we
we	use
use	similarity
similarity	information
information	
	throughout
throughout	training
training	and
and	not
not	merely
merely	for
for	estimating
estimating	statistics
statistics	this
this	
	allows
allows	the
the	system
system	to
to	learn
learn	successfully
successfully	from
from	very
very	sparse
sparse	data
data	
	7
7	smoothing
smoothing	is
is	a
a	technique
technique	widely
widely	used
used	in
in	applications
applications	such
such	as
as	statistical
statistical	pattern
pattern	recognition
recognition	and
and	
	probabilistic
probabilistic	language
language	modeling
modeling	that
that	require
require	a
a	probability
probability	density
density	to
to	be
be	estimated
estimated	from
from	data
data	for
for	
	sparse
sparse	data
data	this
this	estimation
estimation	problem
problem	is
is	severely
severely	underconstrained
underconstrained	and
and	thus
thus	smoothing
smoothing	
	regularizes
regularizes	the
the	problem
problem	by
by	adopting
adopting	a
a	prior
prior	constraint
constraint	that
that	assumes
assumes	that
that	the
the	probability
probability	density
density	does
does	
	not
not	change
change	too
too	fast
fast	in
in	between
between	the
the	examples
examples	
	54
54	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	4
4	6
6	summary
summary	
	we
we	have
have	described
described	an
an	approach
approach	to
to	wsd
wsd	that
that	combines
combines	a
a	corpus
corpus	and
and	an
an	mrd
mrd	to
to	generate
generate	
	an
an	extensive
extensive	data
data	set
set	for
for	learning
learning	disambiguation
disambiguation	our
our	system
system	
	combines
combines	the
the	advantages
advantages	of
of	approaches
approaches	large
large	number
number	of
of	examples
examples	with
with	
	those
those	of
of	the
the	approaches
approaches	data
data	presorted
presorted	by
by	senses
senses	by
by	using
using	the
the	mrd
mrd	definitions
definitions	
	to
to	direct
direct	the
the	extraction
extraction	of
of	training
training	information
information	in
in	the
the	form
form	of
of	feedback
feedback	sets
sets	
	from
from	the
the	corpus
corpus	
	in
in	our
our	system
system	a
a	word
word	is
is	represented
represented	by
by	the
the	set
set	of
of	sentences
sentences	in
in	which
which	it
it	appears
appears	
	accordingly
accordingly	words
words	are
are	considered
considered	similar
similar	if
if	they
they	appear
appear	in
in	similar
similar	sentences
sentences	and
and	sentences
sentences	
	are
are	considered
considered	similar
similar	if
if	they
they	contain
contain	similar
similar	words
words	applying
applying	this
this	definition
definition	
	iteratively
iteratively	yields
yields	a
a	transitive
transitive	measure
measure	of
of	similarity
similarity	under
under	which
which	two
two	sentences
sentences	may
may	be
be	
	considered
considered	similar
similar	even
even	if
if	they
they	do
do	not
not	share
share	any
any	word
word	and
and	two
two	words
words	may
may	be
be	considered
considered	
	similar
similar	even
even	if
if	they
they	do
do	not
not	share
share	neighbor
neighbor	words
words	our
our	experiments
experiments	show
show	that
that	the
the	
	resulting
resulting	alternative
alternative	to
to	raw
raw	similarity
similarity	leads
leads	to
to	better
better	performance
performance	
	on
on	very
very	sparse
sparse	data
data	
	appendix
appendix	
	a
a	1
1	stopping
stopping	conditions
conditions	of
of	the
the	iterative
iterative	algorithm
algorithm	
	let
let	fi
fi	be
be	the
the	increase
increase	in
in	the
the	similarity
similarity	value
value	in
in	iteration
iteration	i
i	
	f
f	x
x	y
y	simi
simi	x
x	y
y	simi_l
simi_l	x
x	32
32	9
9	
	where
where	x
x	y
y	can
can	be
be	either
either	words
words	or
or	sentences
sentences	for
for	each
each	item
item	x
x	the
the	algorithm
algorithm	stops
stops	
	updating
updating	its
its	similarity
similarity	values
values	to
to	other
other	items
items	that
that	is
is	updating
updating	its
its	row
row	in
in	the
the	similarity
similarity	
	matrix
matrix	in
in	the
the	first
first	iteration
iteration	that
that	satisfies
satisfies	maxyfi
maxyfi	x
x	3
3	_
_	where
where	c
c	0
0	is
is	a
a	preset
preset	
	threshold
threshold	
	according
according	to
to	this
this	stopping
stopping	condition
condition	the
the	algorithm
algorithm	terminates
terminates	after
after	at
at	most
most	
	iterations
iterations	otherwise
otherwise	in
in	iterations
iterations	with
with	eachfi
eachfi	c
c	we
we	obtain
obtain	sim
sim	v
v	3
3	1
1	
	in
in	contradiction
contradiction	to
to	upper
upper	bound
bound	of
of	1
1	on
on	the
the	similarity
similarity	values
values	see
see	section
section	a
a	2
2	below
below	
	we
we	found
found	that
that	the
the	best
best	results
results	are
are	obtained
obtained	within
within	three
three	iterations
iterations	after
after	that
that	
	the
the	disambiguation
disambiguation	results
results	tend
tend	not
not	to
to	change
change	significantly
significantly	although
although	the
the	similarity
similarity	
	values
values	may
may	continue
continue	to
to	increase
increase	intuitively
intuitively	the
the	transitive
transitive	exploration
exploration	of
of	similarities
similarities	
	is
is	exhausted
exhausted	after
after	three
three	iterations
iterations	
	a
a	2
2	proofs
proofs	
	in
in	the
the	following
following	x
x	3
3	can
can	be
be	either
either	words
words	or
or	sentences
sentences	
	theorem
theorem	1
1	
	similarity
similarity	is
is	bounded
bounded	simn
simn	x
x	3
3	_
_	1
1	
	proof
proof	
	by
by	induction
induction	on
on	the
the	number
number	of
of	iteration
iteration	at
at	the
the	first
first	iteration
iteration	sim0
sim0	x
x	3
3	_
_	1
1	by
by	
	initialization
initialization	assume
assume	that
that	the
the	claim
claim	holds
holds	for
for	n
n	and
and	prove
prove	for
for	n
n	1
1	
	simn
simn	x
x	y
y	e
e	weight
weight	k
k	x
x	maxsimn
maxsimn	xj
xj	yk
yk	
	ykey
ykey	
	weight
weight	x
x	1
1	by
by	the
the	induction
induction	hypothesis
hypothesis	
	1
1	
	55
55	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	theorem
theorem	2
2	
	similarity
similarity	is
is	reflexive
reflexive	vx
vx	sim
sim	x
x	x
x	1
1	
	proof
proof	
	by
by	induction
induction	on
on	the
the	number
number	of
of	iteration
iteration	sim0
sim0	x
x	x
x	1
1	by
by	initialization
initialization	assume
assume	
	that
that	the
the	claim
claim	holds
holds	for
for	n
n	and
and	prove
prove	for
for	n
n	1
1	
	simn
simn	x
x	x
x	e
e	weight
weight	x
x	max
max	simn
simn	xi
xi	
	x
x	
	e
e	weight
weight	x
x	x
x	simn
simn	xi
xi	xi
xi	
	xie
xie	x
x	
	weight
weight	xi
xi	x
x	1
1	by
by	the
the	induction
induction	hypothesis
hypothesis	
	xiex
xiex	
	1
1	
	thus
thus	simn
simn	v
v	x
x	_
_	1
1	by
by	theorem
theorem	1
1	simn
simn	x
x	x
x	_
_	1
1	so
so	simn
simn	x
x	x
x	1
1	
	theorem
theorem	3
3	
	similarity
similarity	sim
sim	x
x	3
3	is
is	a
a	nondecreasing
nondecreasing	function
function	of
of	the
the	number
number	of
of	iteration
iteration	n
n	
	proof
proof	
	by
by	induction
induction	on
on	the
the	number
number	of
of	iteration
iteration	consider
consider	the
the	case
case	of
of	n
n	1
1	siml
siml	x
x	y
y	
	sim0
sim0	x
x	3
3	if
if	sim0
sim0	x
x	y
y	1
1	then
then	x
x	3
3	and
and	siml
siml	x
x	3
3	1
1	as
as	well
well	else
else	
	sim0
sim0	x
x	3
3	0
0	and
and	siml
siml	x
x	3
3	_
_	0
0	sim0
sim0	x
x	3
3	now
now	assume
assume	that
that	the
the	claim
claim	
	holds
holds	for
for	n
n	and
and	prove
prove	for
for	n
n	1
1	
	simn
simn	x
x	y
y	siren
siren	x
x	y
y	
	weight
weight	k
k	x
x	maxsim
maxsim	xj
xj	yk
yk	
	x
x	ykey
ykey	
	e
e	weight
weight	xj
xj	maxsim
maxsim	_l
_l	xj
xj	yt
yt	
	x
x	nkey
nkey	
	weight
weight	xj
xj	x
x	maxsimn
maxsimn	xi
xi	yk
yk	maxsim
maxsim	yk
yk	
	ykey
ykey	
	0
0	
	the
the	last
last	inequality
inequality	holds
holds	because
because	by
by	the
the	induction
induction	hypothesis
hypothesis	
	v
v	yk
yk	simn
simn	yk
yk	_
_	simn_l
simn_l	xj
xj	yk
yk	
	maxsimn
maxsimn	xj
xj	yk
yk	_
_	maxsim
maxsim	_l
_l	xj
xj	yk
yk	
	ykey
ykey	ykey
ykey	
	maxsimn
maxsimn	xj
xj	yk
yk	maxsimn_l
maxsimn_l	xj
xj	yk
yk	_
_	0
0	
	yk
yk	y
y	
	thus
thus	all
all	the
the	items
items	under
under	the
the	sum
sum	are
are	nonnegative
nonnegative	and
and	so
so	must
must	be
be	their
their	weighted
weighted	
	average
average	as
as	a
a	consequence
consequence	we
we	may
may	conclude
conclude	that
that	the
the	iterative
iterative	estimation
estimation	of
of	similarity
similarity	
	converges
converges	
	56
56	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	a
a	3
3	word
word	weights
weights	
	in
in	our
our	algorithm
algorithm	the
the	weight
weight	of
of	a
a	word
word	estimates
estimates	its
its	expected
expected	contribution
contribution	to
to	the
the	disambiguation
disambiguation	
	task
task	and
and	the
the	extent
extent	to
to	which
which	the
the	word
word	is
is	indicative
indicative	in
in	sentence
sentence	similarity
similarity	
	the
the	weights
weights	do
do	not
not	change
change	with
with	iterations
iterations	they
they	are
are	used
used	to
to	reduce
reduce	the
the	number
number	of
of	
	features
features	to
to	a
a	manageable
manageable	size
size	and
and	to
to	exclude
exclude	words
words	that
that	are
are	expected
expected	to
to	be
be	given
given	
	unreliable
unreliable	similarity
similarity	values
values	the
the	weight
weight	of
of	a
a	word
word	is
is	a
a	product
product	of
of	several
several	factors
factors	frequency
frequency	
	in
in	the
the	corpus
corpus	the
the	bias
bias	inherent
inherent	in
in	the
the	training
training	set
set	distance
distance	from
from	the
the	target
target	
	word
word	and
and	label
label	
	
	
	global
global	frequency
frequency	frequent
frequent	words
words	are
are	less
less	informative
informative	of
of	sense
sense	and
and	of
of	
	sentence
sentence	similarity
similarity	for
for	example
example	the
the	appearance
appearance	of
of	year
year	a
a	frequent
frequent	word
word	
	in
in	two
two	different
different	sentences
sentences	in
in	the
the	corpus
corpus	we
we	employed
employed	would
would	not
not	
	necessarily
necessarily	indicate
indicate	similarity
similarity	between
between	them
them	and
and	would
would	not
not	be
be	effective
effective	
	in
in	disambiguating
disambiguating	the
the	sense
sense	of
of	most
most	target
target	words
words	the
the	contribution
contribution	of
of	
	freq
freq	w
w	frequency
frequency	is
is	max
max	1
1	max5xfreq
max5xfreq	x
x	where
where	max5xfreq
max5xfreq	x
x	is
is	a
a	function
function	
	of
of	the
the	five
five	highest
highest	frequencies
frequencies	in
in	the
the	global
global	corpus
corpus	and
and	x
x	is
is	any
any	noun
noun	
	or
or	verb
verb	or
or	adjective
adjective	there
there	this
this	factor
factor	excludes
excludes	only
only	the
the	most
most	frequent
frequent	
	words
words	from
from	further
further	consideration
consideration	as
as	long
long	as
as	the
the	frequencies
frequencies	are
are	not
not	
	very
very	high
high	it
it	does
does	not
not	label
label	14
14	whose
whose	frequency
frequency	is
is	twice
twice	that
that	of
of	w2
w2	as
as	
	less
less	informative
informative	
	factor
factor	words
words	that
that	are
are	indicative
indicative	of
of	the
the	sense
sense	usually
usually	appear
appear	
	in
in	the
the	training
training	set
set	more
more	than
than	what
what	would
would	have
have	been
been	expected
expected	from
from	their
their	
	frequency
frequency	in
in	the
the	general
general	corpus
corpus	the
the	factor
factor	captures
captures	this
this	
	tendency
tendency	it
it	is
is	computed
computed	as
as	
	
	
	pr
pr	wi
wi	114
114	10
10	
	log
log	pr
pr	wi
wi	
	where
where	pr
pr	14
14	is
is	estimated
estimated	from
from	the
the	frequency
frequency	of
of	14
14	in
in	the
the	entire
entire	corpus
corpus	
	and
and	pr
pr	wi
wi	i
i	14
14	from
from	the
the	frequency
frequency	of
of	wi
wi	in
in	the
the	training
training	set
set	given
given	the
the	
	examples
examples	of
of	the
the	current
current	ambiguous
ambiguous	word
word	w
w	cf
cf	gale
gale	church
church	and
and	
	yarowsky
yarowsky	8
8	to
to	avoid
avoid	poor
poor	estimation
estimation	for
for	words
words	with
with	a
a	low
low	count
count	
	in
in	the
the	training
training	set
set	we
we	multiply
multiply	the
the	log
log	likelihood
likelihood	by
by	min
min	w
w	10
10	
	where
where	count
count	w
w	is
is	the
the	number
number	of
of	occurrences
occurrences	of
of	14
14	in
in	the
the	training
training	set
set	
	part
part	of
of	speech
speech	each
each	part
part	of
of	speech
speech	is
is	assigned
assigned	a
a	weight
weight	1
1	0
0	for
for	nouns
nouns	0
0	6
6	
	for
for	verbs
verbs	and
and	1
1	0
0	for
for	the
the	adjectives
adjectives	of
of	the
the	target
target	word
word	
	distance
distance	from
from	the
the	target
target	word
word	context
context	words
words	that
that	are
are	far
far	from
from	the
the	target
target	
	word
word	are
are	less
less	indicative
indicative	than
than	nearby
nearby	ones
ones	the
the	contribution
contribution	of
of	this
this	factor
factor	
	is
is	reciprocally
reciprocally	related
related	to
to	the
the	normalized
normalized	distance
distance	the
the	weight
weight	of
of	context
context	
	words
words	that
that	appear
appear	in
in	the
the	same
same	sentence
sentence	as
as	the
the	target
target	word
word	is
is	taken
taken	to
to	
	be
be	1
1	0
0	the
the	weight
weight	of
of	words
words	that
that	appear
appear	in
in	the
the	adjacent
adjacent	sentences
sentences	is
is	0
0	5
5	
	the
the	total
total	weight
weight	of
of	a
a	word
word	is
is	the
the	product
product	of
of	the
the	above
above	factors
factors	each
each	normalized
normalized	by
by	
	factor
factor	wi
wi	s
s	the
the	sum
sum	of
of	factors
factors	of
of	the
the	words
words	in
in	the
the	sentence
sentence	weight
weight	wi
wi	z_
z_	factor
factor	w
w	j
j	s
s	
	8
8	because
because	this
this	estimate
estimate	is
is	unreliable
unreliable	for
for	words
words	with
with	low
low	frequencies
frequencies	in
in	each
each	sense
sense	set
set	gale
gale	church
church	and
and	
	yarowsky
yarowsky	1992
1992	suggested
suggested	to
to	interpolate
interpolate	between
between	probabilities
probabilities	computed
computed	within
within	the
the	subcorpus
subcorpus	and
and	
	probabilities
probabilities	computed
computed	over
over	the
the	entire
entire	corpus
corpus	in
in	our
our	case
case	the
the	denominator
denominator	is
is	the
the	frequency
frequency	in
in	the
the	
	general
general	corpus
corpus	instead
instead	of
of	the
the	frequency
frequency	in
in	the
the	sense
sense	examples
examples	so
so	it
it	is
is	more
more	reliable
reliable	
	57
57	
	computational
computational	linguistics
linguistics	volume
volume	24
24	number
number	1
1	
	where
where	factor
factor	is
is	the
the	weight
weight	before
before	normalization
normalization	the
the	use
use	of
of	weights
weights	contributed
contributed	
	about
about	5
5	to
to	the
the	disambiguation
disambiguation	performance
performance	
	a
a	4
4	other
other	uses
uses	of
of	context
context	similarity
similarity	
	the
the	similarity
similarity	measure
measure	developed
developed	in
in	the
the	present
present	paper
paper	can
can	be
be	used
used	for
for	tasks
tasks	other
other	than
than	
	word
word	sense
sense	disambiguation
disambiguation	here
here	we
we	illustrate
illustrate	a
a	possible
possible	application
application	to
to	automatic
automatic	
	construction
construction	of
of	a
a	thesaurus
thesaurus	
	following
following	the
the	training
training	phase
phase	for
for	a
a	word
word	x
x	we
we	have
have	a
a	word
word	similarity
similarity	matrix
matrix	for
for	
	the
the	words
words	in
in	the
the	contexts
contexts	of
of	using
using	this
this	matrix
matrix	we
we	construct
construct	for
for	each
each	sense
sense	si
si	of
of	
	a
a	set
set	of
of	related
related	words
words	r
r	
	
	2
2	
	
	initialize
initialize	r
r	to
to	the
the	set
set	of
of	words
words	appearing
appearing	in
in	the
the	mrd
mrd	definition
definition	of
of	si
si	
	extend
extend	r
r	recursively
recursively	for
for	each
each	word
word	in
in	r
r	added
added	in
in	the
the	previous
previous	step
step	add
add	
	its
its	k
k	nearest
nearest	neighbors
neighbors	using
using	the
the	similarity
similarity	matrix
matrix	
	stop
stop	when
when	no
no	new
new	words
words	or
or	too
too	few
few	new
new	words
words	are
are	added
added	
	upon
upon	termination
termination	output
output	for
for	each
each	sense
sense	si
si	the
the	set
set	of
of	its
its	contextually
contextually	similar
similar	words
words	r
r	
	acknowledgments
acknowledgments	
	we
we	thank
thank	dan
dan	roth
roth	for
for	many
many	useful
useful	
	discussions
discussions	and
and	the
the	anonymous
anonymous	reviewers
reviewers	
	for
for	constructive
constructive	comments
comments	on
on	the
the	
	manuscript
manuscript	this
this	work
work	was
was	first
first	presented
presented	
	at
at	the
the	4th
4th	international
international	workshop
workshop	on
on	large
large	
	corpora
corpora	copenhagen
copenhagen	august
august	1996
1996	
	references
references	
	brill
brill	eric
eric	david
david	magerman
magerman	mitchell
mitchell	p
p	
	marcus
marcus	and
and	beatrice
beatrice	santorini
santorini	1990
1990	
	deducing
deducing	linguistic
linguistic	structure
structure	from
from	the
the	
	statistics
statistics	of
of	large
large	corpora
corpora	in
in	darpa
darpa	
	speech
speech	and
and	natural
natural	language
language	workshop
workshop	
	pages
pages	june
june	
	brown
brown	peter
peter	f
f	della
della	pietra
pietra	
	vincent
vincent	j
j	della
della	pietra
pietra	and
and	robert
robert	l
l	
	mercer
mercer	1991
1991	word
word	sense
sense	disambiguation
disambiguation	
	using
using	statistical
statistical	methods
methods	in
in	proceedings
proceedings	of
of	
	the
the	29th
29th	annual
annual	meeting
meeting	pages
pages	
	association
association	for
for	computational
computational	
	linguistics
linguistics	
	church
church	kenneth
kenneth	w
w	and
and	robert
robert	l
l	mercer
mercer	
	1993
1993	introduction
introduction	to
to	the
the	special
special	issue
issue	on
on	
	computational
computational	linguistics
linguistics	using
using	large
large	
	corpora
corpora	computational
computational	linguistics
linguistics	19
19	
	cruse
cruse	d
d	alan
alan	1986
1986	lexical
lexical	semantics
semantics	
	cambridge
cambridge	university
university	press
press	cambridge
cambridge	
	england
england	
	dagan
dagan	ido
ido	alon
alon	itai
itai	and
and	ulrike
ulrike	schwall
schwall	
	1991
1991	two
two	languages
languages	are
are	more
more	informative
informative	
	than
than	one
one	in
in	proceedings
proceedings	of
of	the
the	29th
29th	annual
annual	
	meeting
meeting	pages
pages	association
association	for
for	
	computational
computational	linguistics
linguistics	
	dagan
dagan	ido
ido	shaul
shaul	marcus
marcus	and
and	shaul
shaul	
	markovitch
markovitch	1993
1993	contextual
contextual	word
word	
	similarity
similarity	and
and	estimation
estimation	from
from	sparse
sparse	
	data
data	in
in	proceedings
proceedings	of
of	the
the	31st
31st	annual
annual	
	meeting
meeting	pages
pages	association
association	for
for	
	computational
computational	linguistics
linguistics	
	gale
gale	william
william	kenneth
kenneth	w
w	church
church	and
and	
	david
david	yarowsky
yarowsky	1992
1992	a
a	method
method	for
for	
	disambiguating
disambiguating	word
word	senses
senses	in
in	a
a	large
large	
	corpus
corpus	computers
computers	and
and	the
the	humanities
humanities	
	26
26	
	guthrie
guthrie	joe
joe	a
a	guthrie
guthrie	yorick
yorick	
	wilks
wilks	and
and	homa
homa	aidinejad
aidinejad	1991
1991	
	cooccurrence
cooccurrence	and
and	
	word
word	sense
sense	disambiguation
disambiguation	in
in	proceedings
proceedings	
	of
of	the
the	29th
29th	annual
annual	meeting
meeting	pages
pages	
	association
association	for
for	computational
computational	
	linguistics
linguistics	
	hirschman
hirschman	lynette
lynette	1986
1986	discovering
discovering	
	sublanguage
sublanguage	structure
structure	in
in	ralph
ralph	
	grishman
grishman	and
and	richard
richard	kittredge
kittredge	editors
editors	
	analyzing
analyzing	language
language	in
in	restricted
restricted	domains
domains	
	sublanguage
sublanguage	description
description	and
and	processing
processing	
	lawrence
lawrence	erlbaum
erlbaum	hillsdale
hillsdale	nj
nj	pages
pages	
	
	krovetz
krovetz	robert
robert	and
and	w
w	bruce
bruce	croft
croft	1989
1989	
	word
word	sense
sense	disambiguation
disambiguation	using
using	
	machine
machine	readable
readable	dictionaries
dictionaries	in
in	
	proceedings
proceedings	of
of	acm
acm	pages
pages	
	
	lee
lee	joon
joon	h
h	h
h	kim
kim	and
and	yoon
yoon	j
j	
	lee
lee	1993
1993	information
information	retrieval
retrieval	based
based	on
on	
	conceptual
conceptual	distance
distance	in
in	hierarchies
hierarchies	
	journal
journal	of
of	documentation
documentation	49
49	
	lesk
lesk	michael
michael	1986
1986	automatic
automatic	sense
sense	
	disambiguation
disambiguation	how
how	to
to	tell
tell	a
a	pine
pine	cone
cone	
	from
from	an
an	ice
ice	cream
cream	cone
cone	in
in	proceedings
proceedings	of
of	
	the
the	1986
1986	acm
acm	sigdoc
sigdoc	conference
conference	pages
pages	
	
	58
58	
	karov
karov	and
and	edelman
edelman	word
word	sense
sense	disambiguation
disambiguation	
	miller
miller	george
george	a
a	beckwith
beckwith	
	christiane
christiane	fellbaum
fellbaum	derek
derek	gross
gross	and
and	
	katherine
katherine	j
j	miller
miller	1993
1993	introduction
introduction	to
to	
	wordnet
wordnet	an
an	lexical
lexical	database
database	
	csl
csl	43
43	cognitive
cognitive	science
science	laboratory
laboratory	
	princeton
princeton	university
university	princeton
princeton	nj
nj	
	pereira
pereira	fernando
fernando	and
and	naftali
naftali	tishby
tishby	1992
1992	
	distibutional
distibutional	similarity
similarity	phase
phase	transitions
transitions	
	and
and	hierarchical
hierarchical	clustering
clustering	in
in	working
working	
	notes
notes	of
of	the
the	aaai
aaai	fall
fall	symposium
symposium	on
on	
	probabilistic
probabilistic	approaches
approaches	to
to	natural
natural	language
language	
	pages
pages	
	pereira
pereira	fernando
fernando	naftali
naftali	tishby
tishby	and
and	
	lillian
lillian	lee
lee	1993
1993	distibutional
distibutional	clustering
clustering	
	of
of	english
english	words
words	in
in	proceedings
proceedings	of
of	the
the	31st
31st	
	annual
annual	meeting
meeting	pages
pages	
	association
association	for
for	computational
computational	
	linguistics
linguistics	
	quine
quine	willard
willard	v
v	o
o	1960
1960	word
word	and
and	object
object	
	mit
mit	press
press	cambridge
cambridge	ma
ma	
	resnik
resnik	philip
philip	july
july	1992
1992	wordnet
wordnet	and
and	
	distribuitional
distribuitional	analysis
analysis	a
a	
	approach
approach	to
to	lexical
lexical	discovery
discovery	in
in	aaai
aaai	
	workshop
workshop	on
on	natural
natural	
	language
language	processing
processing	techniques
techniques	pages
pages	
	
	resnik
resnik	philip
philip	june
june	1995
1995	disambiguating
disambiguating	
	noun
noun	groupings
groupings	with
with	respect
respect	to
to	wordnet
wordnet	
	senses
senses	in
in	third
third	workshop
workshop	on
on	very
very	large
large	
	corpora
corpora	pages
pages	cambridge
cambridge	ma
ma	
	schi
schi	hinrich
hinrich	1992
1992	dimensions
dimensions	of
of	
	meaning
meaning	in
in	proceedings
proceedings	of
of	supercomputing
supercomputing	
	symposium
symposium	pages
pages	minneapolis
minneapolis	
	mn
mn	
	v6ronis
v6ronis	jean
jean	and
and	nancy
nancy	ide
ide	1990
1990	word
word	
	sense
sense	disambiguation
disambiguation	with
with	very
very	large
large	
	neural
neural	networks
networks	extracted
extracted	from
from	machine
machine	
	readable
readable	dictionaries
dictionaries	in
in	proceedings
proceedings	of
of	
	pages
pages	
	walker
walker	donald
donald	e
e	and
and	robert
robert	a
a	amsler
amsler	
	1986
1986	the
the	use
use	of
of	
	dictionaries
dictionaries	in
in	sublanguage
sublanguage	analysis
analysis	in
in	
	r
r	grisham
grisham	editor
editor	analyzing
analyzing	languages
languages	in
in	
	restricted
restricted	domains
domains	sublanguage
sublanguage	description
description	
	and
and	processing
processing	lawrence
lawrence	erlbaum
erlbaum	
	associates
associates	hillsdale
hillsdale	nj
nj	
	weinreich
weinreich	uriel
uriel	1980
1980	on
on	semantics
semantics	
	university
university	of
of	pennsylvania
pennsylvania	press
press	
	philadelphia
philadelphia	pa
pa	
	yarowsky
yarowsky	david
david	1992
1992	word
word	sense
sense	
	disambiguation
disambiguation	using
using	statistical
statistical	models
models	of
of	
	categories
categories	trained
trained	on
on	large
large	
	corpora
corpora	in
in	proceedings
proceedings	of
of	
	pages
pages	nantes
nantes	france
france	
	yarowsky
yarowsky	david
david	1995
1995	unsupervised
unsupervised	word
word	
	sense
sense	disambiguation
disambiguation	rivaling
rivaling	supervised
supervised	
	methods
methods	in
in	proceedings
proceedings	of
of	the
the	33rd
33rd	annual
annual	
	meeting
meeting	pages
pages	cambridge
cambridge	ma
ma	
	association
association	for
for	computational
computational	
	linguistics
linguistics	
	59
59	
	
