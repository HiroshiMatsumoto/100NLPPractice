Similarity-based	similarity-based
Word	word
Sense	sense
	
Disambiguation	disambiguation
	
Yael	yael
Karov	karov
*	*
	
Weizmann	weizmann
Institute	institute
	
Shimon	shimon
Edelman	edelman
t	t
	
MIT	mit
	
We	we
describe	describe
a	a
method	method
for	for
automatic	automatic
word	word
sense	sense
disambiguation	disambiguation
using	using
a	a
text	text
corpus	corpus
and	and
a	a
machinereadable	machinereadable
	
dictionary	dictionary
(	(
MRD	mrd
)	)
The	the
method	method
is	is
based	based
on	on
word	word
similarity	similarity
and	and
context	context
similarity	similarity
	
measures	measures
Words	words
are	are
considered	considered
similar	similar
if	if
they	they
appear	appear
in	in
similar	similar
contexts	contexts
;	;
contexts	contexts
are	are
similar	similar
if	if
	
they	they
contain	contain
similar	similar
words	words
The	the
circularity	circularity
of	of
this	this
definition	definition
is	is
resolved	resolved
by	by
an	an
iterative	iterative
converging	converging
	
process	process
in	in
which	which
the	the
system	system
learns	learns
from	from
the	the
corpus	corpus
a	a
set	set
of	of
typical	typical
usages	usages
for	for
each	each
of	of
the	the
senses	senses
	
of	of
the	the
polysemous	polysemous
word	word
listed	listed
in	in
the	the
MRD	mrd
A	a
new	new
instance	instance
of	of
a	a
polysemous	polysemous
word	word
is	is
assigned	assigned
the	the
	
sense	sense
associated	associated
with	with
the	the
typical	typical
usage	usage
most	most
similar	similar
to	to
its	its
context	context
Experiments	experiments
show	show
that	that
this	this
	
method	method
can	can
learn	learn
even	even
from	from
very	very
sparse	sparse
training	training
data	data
achieving	achieving
over	over
92	92
%	%
correct	correct
disambiguation	disambiguation
	
performance	performance
	
1	1
Introduction	introduction
	
Word	word
sense	sense
disambiguation	disambiguation
(	(
WSD	wsd
)	)
is	is
the	the
problem	problem
of	of
assigning	assigning
a	a
sense	sense
to	to
an	an
ambiguous	ambiguous
	
word	word
using	using
its	its
context	context
We	we
assume	assume
that	that
different	different
senses	senses
of	of
a	a
word	word
correspond	correspond
to	to
	
different	different
entries	entries
in	in
its	its
dictionary	dictionary
definition	definition
For	for
example	example
suit	suit
has	has
two	two
senses	senses
listed	listed
in	in
	
a	a
dictionary	dictionary
:	:
'an	'an
action	action
in	in
court	court
'	'
and	and
'suit	'suit
of	of
clothes	clothes
'	'
Given	given
the	the
sentence	sentence
The	the
union's	union's
	
lawyers	lawyers
are	are
reviewing	reviewing
the	the
suit	suit
we	we
would	would
like	like
the	the
system	system
to	to
decide	decide
automatically	automatically
that	that
	
suit	suit
is	is
used	used
there	there
in	in
its	its
court-related	court-related
sense	sense
(	(
we	we
assume	assume
that	that
the	the
part	part
of	of
speech	speech
of	of
the	the
	
polysemous	polysemous
word	word
is	is
known	known
)	)
	
In	in
recent	recent
years	years
text	text
corpora	corpora
have	have
been	been
the	the
main	main
source	source
of	of
information	information
for	for
learning	learning
	
automatic	automatic
WSD	wsd
(	(
see	see
for	for
example	example
Gale	gale
Church	church
and	and
Yarowsky	yarowsky
[1992	[1992
]	]
)	)
A	a
typical	typical
	
corpus-based	corpus-based
algorithm	algorithm
constructs	constructs
a	a
training	training
set	set
from	from
all	all
contexts	contexts
of	of
a	a
polysemous	polysemous
	
word	word
W	w
in	in
the	the
corpus	corpus
and	and
uses	uses
it	it
to	to
learn	learn
a	a
classifier	classifier
that	that
maps	maps
instances	instances
of	of
W	w
(	(
each	each
	
supplied	supplied
with	with
its	its
context	context
)	)
into	into
the	the
senses	senses
Because	because
learning	learning
requires	requires
that	that
the	the
examples	examples
	
in	in
the	the
training	training
set	set
be	be
partitioned	partitioned
into	into
the	the
different	different
senses	senses
and	and
because	because
sense	sense
information	information
	
is	is
not	not
available	available
in	in
the	the
corpus	corpus
explicitly	explicitly
this	this
approach	approach
depends	depends
critically	critically
on	on
manual	manual
	
sense	sense
tagging--a	tagging--a
laborious	laborious
and	and
time-consuming	time-consuming
process	process
that	that
has	has
to	to
be	be
repeated	repeated
for	for
	
every	every
word	word
in	in
every	every
language	language
and	and
more	more
likely	likely
than	than
not	not
for	for
every	every
topic	topic
of	of
discourse	discourse
	
or	or
source	source
of	of
information	information
	
The	the
need	need
for	for
tagged	tagged
examples	examples
creates	creates
a	a
problem	problem
referred	referred
to	to
in	in
previous	previous
works	works
as	as
	
the	the
knowledge	knowledge
acquisition	acquisition
bottleneck	bottleneck
:	:
training	training
a	a
disambiguator	disambiguator
for	for
W	w
requires	requires
that	that
	
the	the
examples	examples
in	in
the	the
corpus	corpus
be	be
partitioned	partitioned
into	into
senses	senses
which	which
in	in
turn	turn
requires	requires
a	a
fully	fully
	
operational	operational
disambiguator	disambiguator
The	the
method	method
we	we
propose	propose
circumvents	circumvents
this	this
problem	problem
by	by
automatically	automatically
	
tagging	tagging
the	the
training	training
set	set
examples	examples
for	for
W	w
using	using
other	other
examples	examples
that	that
do	do
not	not
	
contain	contain
W	w
but	but
do	do
contain	contain
related	related
words	words
extracted	extracted
from	from
its	its
dictionary	dictionary
definition	definition
For	for
	
instance	instance
in	in
the	the
training	training
set	set
for	for
suit	suit
we	we
would	would
use	use
in	in
addition	addition
to	to
the	the
contexts	contexts
of	of
suit	suit
	
*	*
Dept	dept
of	of
Applied	applied
Mathematics	mathematics
and	and
Computer	computer
Science	science
Rehovot	rehovot
76100	76100
Israel	israel
	
t	t
Center	center
for	for
Biological	biological
&	&
Computational	computational
Learning	learning
MIT	mit
E25-201	e25-201
Cambridge	cambridge
MA	ma
02142	02142
Present	present
address	address
:	:
	
School	school
of	of
Cognitive	cognitive
and	and
Computing	computing
Sciences	sciences
University	university
of	of
Sussex	sussex
Falmer	falmer
BN1	bn1
9QH	9qh
UK	uk
	
(	(
~	~
)	)
1998	1998
Association	association
for	for
Computational	computational
Linguistics	linguistics
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
all	all
the	the
contexts	contexts
of	of
court	court
and	and
of	of
clothes	clothes
in	in
the	the
corpus	corpus
because	because
court	court
and	and
clothes	clothes
appear	appear
in	in
	
the	the
machine-readable	machine-readable
dictionary	dictionary
(	(
MRD	mrd
)	)
entry	entry
of	of
suit	suit
that	that
defines	defines
its	its
two	two
senses	senses
Note	note
	
that	that
unlike	unlike
the	the
contexts	contexts
of	of
suit	suit
which	which
may	may
discuss	discuss
either	either
court	court
action	action
or	or
clothing	clothing
the	the
	
contexts	contexts
of	of
court	court
are	are
not	not
likely	likely
to	to
be	be
especially	especially
related	related
to	to
clothing	clothing
and	and
similarly	similarly
those	those
	
of	of
clothes	clothes
will	will
normally	normally
have	have
little	little
to	to
do	do
with	with
lawsuits	lawsuits
We	we
will	will
use	use
this	this
observation	observation
to	to
	
tag	tag
the	the
original	original
contexts	contexts
of	of
suit	suit
	
Another	another
problem	problem
that	that
affects	affects
the	the
corpus-based	corpus-based
WSD	wsd
methods	methods
is	is
the	the
sparseness	sparseness
of	of
	
data	data
:	:
these	these
methods	methods
typically	typically
rely	rely
on	on
the	the
statistics	statistics
of	of
co-occurrences	co-occurrences
of	of
words	words
while	while
	
many	many
of	of
the	the
possible	possible
co-occurrences	co-occurrences
are	are
not	not
observed	observed
even	even
in	in
a	a
very	very
large	large
corpus	corpus
	
(	(
Church	church
and	and
Mercer	mercer
1993	1993
)	)
We	we
address	address
this	this
problem	problem
in	in
several	several
ways	ways
First	first
instead	instead
of	of
	
tallying	tallying
word	word
statistics	statistics
from	from
the	the
examples	examples
for	for
each	each
sense	sense
(	(
which	which
may	may
be	be
unreliable	unreliable
	
when	when
the	the
examples	examples
are	are
few	few
)	)
we	we
collect	collect
sentence-level	sentence-level
statistics	statistics
representing	representing
each	each
	
sentence	sentence
by	by
the	the
set	set
of	of
features	features
it	it
contains	contains
(	(
for	for
more	more
on	on
features	features
see	see
Section	section
4	4
2	2
)	)
Second	second
	
we	we
define	define
a	a
similarity	similarity
measure	measure
on	on
the	the
feature	feature
space	space
which	which
allows	allows
us	us
to	to
pool	pool
the	the
	
statistics	statistics
of	of
similar	similar
features	features
Third	third
in	in
addition	addition
to	to
the	the
examples	examples
of	of
the	the
polysemous	polysemous
word	word
	
142	142
in	in
the	the
corpus	corpus
we	we
learn	learn
also	also
from	from
the	the
examples	examples
of	of
all	all
the	the
words	words
in	in
the	the
dictionary	dictionary
	
definition	definition
of	of
W	w
In	in
our	our
experiments	experiments
this	this
resulted	resulted
in	in
a	a
training	training
set	set
that	that
could	could
be	be
up	up
to	to
	
20	20
times	times
larger	larger
than	than
the	the
set	set
of	of
original	original
examples	examples
	
The	the
rest	rest
of	of
this	this
paper	paper
is	is
organized	organized
as	as
follows	follows
Section	section
2	2
describes	describes
the	the
approach	approach
	
we	we
have	have
developed	developed
In	in
Section	section
3	3
we	we
report	report
the	the
results	results
of	of
tests	tests
we	we
have	have
conducted	conducted
on	on
	
the	the
Treebank-2	treebank-2
corpus	corpus
Section	section
4	4
concludes	concludes
with	with
a	a
discussion	discussion
of	of
related	related
methods	methods
and	and
	
a	a
summary	summary
Proofs	proofs
and	and
other	other
details	details
of	of
our	our
scheme	scheme
can	can
be	be
found	found
in	in
the	the
appendix	appendix
	
2	2
Similarity-based	similarity-based
Disambiguation	disambiguation
	
Our	our
aim	aim
is	is
to	to
have	have
the	the
system	system
learn	learn
to	to
disambiguate	disambiguate
the	the
appearances	appearances
of	of
a	a
polysemous	polysemous
	
word	word
W	w
(	(
noun	noun
verb	verb
or	or
adjective	adjective
)	)
with	with
senses	senses
Sl	sl
 sk	 sk
using	using
as	as
examples	examples
the	the
appearances	appearances
	
of	of
W	w
in	in
an	an
untagged	untagged
corpus	corpus
To	to
avoid	avoid
the	the
need	need
to	to
tag	tag
the	the
training	training
examples	examples
	
manually	manually
we	we
augment	augment
the	the
training	training
set	set
by	by
additional	additional
sense-related	sense-related
examples	examples
which	which
	
we	we
call	call
a	a
feedback	feedback
set	set
The	the
feedback	feedback
set	set
for	for
sense	sense
si	si
of	of
word	word
W	w
is	is
the	the
union	union
of	of
all	all
	
contexts	contexts
that	that
contain	contain
some	some
noun	noun
found	found
in	in
the	the
entry	entry
of	of
si	si
(	(
W	w
)	)
in	in
an	an
MRD	mrd
1	1
Words	words
in	in
	
the	the
intersection	intersection
of	of
any	any
two	two
sense	sense
entries	entries
as	as
well	well
as	as
examples	examples
in	in
the	the
intersection	intersection
of	of
	
two	two
feedback	feedback
sets	sets
are	are
discarded	discarded
during	during
initialization	initialization
;	;
we	we
also	also
use	use
a	a
stop	stop
list	list
to	to
discard	discard
	
from	from
the	the
MRD	mrd
definition	definition
high-frequency	high-frequency
words	words
such	such
as	as
that	that
which	which
do	do
not	not
contribute	contribute
	
to	to
the	the
disambiguation	disambiguation
process	process
The	the
feedback	feedback
sets	sets
can	can
be	be
augmented	augmented
in	in
turn	turn
by	by
original	original
	
training-set	training-set
sentences	sentences
that	that
are	are
closely	closely
related	related
(	(
in	in
a	a
sense	sense
defined	defined
below	below
)	)
to	to
one	one
of	of
	
the	the
feedback-set	feedback-set
sentences	sentences
;	;
these	these
additional	additional
examples	examples
can	can
then	then
attract	attract
other	other
original	original
	
examples	examples
	
The	the
feedback	feedback
sets	sets
constitute	constitute
a	a
rich	rich
source	source
of	of
data	data
that	that
are	are
known	known
to	to
be	be
sorted	sorted
by	by
	
sense	sense
Specifically	specifically
the	the
feedback	feedback
set	set
of	of
si	si
is	is
known	known
to	to
be	be
more	more
closely	closely
related	related
to	to
si	si
than	than
	
to	to
the	the
other	other
senses	senses
of	of
the	the
same	same
word	word
We	we
rely	rely
on	on
this	this
observation	observation
to	to
tag	tag
automatically	automatically
	
the	the
examples	examples
of	of
W	w
as	as
follows	follows
Each	each
original	original
sentence	sentence
containing	containing
W	w
is	is
assigned	assigned
the	the
	
sense	sense
of	of
its	its
most	most
similar	similar
sentence	sentence
in	in
the	the
feedback	feedback
sets	sets
Two	two
sentences	sentences
are	are
considered	considered
to	to
	
be	be
similar	similar
insofar	insofar
as	as
they	they
contain	contain
similar	similar
words	words
(	(
they	they
do	do
not	not
have	have
to	to
share	share
any	any
word	word
)	)
;	;
	
words	words
are	are
considered	considered
to	to
be	be
similar	similar
if	if
they	they
appear	appear
in	in
similar	similar
sentences	sentences
The	the
circularity	circularity
	
of	of
this	this
definition	definition
is	is
resolved	resolved
by	by
an	an
iterative	iterative
converging	converging
process	process
described	described
below	below
	
1	1
By	by
MRD	mrd
we	we
mean	mean
a	a
machine-readable	machine-readable
dictionary	dictionary
or	or
a	a
thesaurus	thesaurus
or	or
any	any
combination	combination
of	of
such	such
	
knowledge	knowledge
sources	sources
	
42	42
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
2	2
1	1
Terminology	terminology
	
A	a
context	context
or	or
example	example
of	of
the	the
target	target
word	word
W	w
is	is
any	any
sentence	sentence
that	that
contains	contains
W	w
and	and
	
(	(
optionally	optionally
)	)
the	the
two	two
adjacent	adjacent
sentences	sentences
in	in
the	the
corpus	corpus
The	the
features	features
of	of
a	a
sentence	sentence
8	8
are	are
	
its	its
nouns	nouns
verbs	verbs
and	and
the	the
adjectives	adjectives
of	of
W	w
and	and
of	of
any	any
noun	noun
that	that
appears	appears
both	both
in	in
$	$
and	and
	
in	in
W's	w's
MRD	mrd
definition	definition
(	(
s	s
)	)
all	all
used	used
after	after
stemming	stemming
(	(
it	it
is	is
also	also
possible	possible
to	to
use	use
other	other
types	types
	
of	of
features	features
such	such
as	as
word	word
n-grams	n-grams
or	or
syntactic	syntactic
information	information
;	;
see	see
Section	section
4	4
2	2
)	)
As	as
the	the
	
number	number
of	of
features	features
in	in
the	the
training	training
data	data
can	can
be	be
very	very
large	large
we	we
automatically	automatically
assign	assign
each	each
	
relevant	relevant
feature	feature
a	a
weight	weight
indicating	indicating
the	the
extent	extent
to	to
which	which
it	it
is	is
indicative	indicative
of	of
the	the
sense	sense
	
(	(
see	see
Section	section
A	a
3	3
in	in
the	the
appendix	appendix
)	)
Features	features
that	that
appear	appear
less	less
than	than
two	two
times	times
in	in
the	the
	
training	training
set	set
and	and
features	features
whose	whose
weight	weight
falls	falls
under	under
a	a
certain	certain
threshold	threshold
are	are
excluded	excluded
	
A	a
sentence	sentence
is	is
represented	represented
by	by
the	the
set	set
of	of
the	the
remaining	remaining
relevant	relevant
features	features
it	it
contains	contains
	
2	2
2	2
Computation	computation
of	of
Similarity	similarity
	
Our	our
method	method
hinges	hinges
on	on
the	the
possibility	possibility
of	of
computing	computing
similarity	similarity
between	between
the	the
original	original
	
contexts	contexts
of	of
W	w
and	and
the	the
sentences	sentences
in	in
the	the
feedback	feedback
sets	sets
We	we
concentrate	concentrate
on	on
similarities	similarities
	
in	in
the	the
way	way
sentences	sentences
use	use
W	w
not	not
on	on
similarities	similarities
in	in
the	the
meaning	meaning
of	of
the	the
sentences	sentences
Thus	thus
	
similar	similar
words	words
tend	tend
to	to
appear	appear
in	in
similar	similar
contexts	contexts
and	and
their	their
textual	textual
proximity	proximity
to	to
the	the
	
ambiguous	ambiguous
word	word
W	w
is	is
indicative	indicative
of	of
the	the
sense	sense
of	of
W	w
Note	note
that	that
contextually	contextually
similar	similar
	
words	words
do	do
not	not
have	have
to	to
be	be
synonyms	synonyms
or	or
to	to
belong	belong
to	to
the	the
same	same
lexical	lexical
category	category
For	for
example	example
	
we	we
consider	consider
the	the
words	words
doctor	doctor
and	and
health	health
to	to
be	be
similar	similar
because	because
they	they
frequently	frequently
	
share	share
contexts	contexts
although	although
they	they
are	are
far	far
removed	removed
from	from
each	each
other	other
in	in
a	a
typical	typical
semantic	semantic
	
hierarchy	hierarchy
such	such
as	as
the	the
WordNet	wordnet
(	(
Miller	miller
et	et
al	al
1993	1993
)	)
Note	note
further	further
that	that
because	because
we	we
	
learn	learn
similarity	similarity
from	from
the	the
training	training
set	set
of	of
W	w
and	and
not	not
from	from
the	the
entire	entire
corpus	corpus
it	it
tends	tends
	
to	to
capture	capture
regularities	regularities
with	with
respect	respect
to	to
the	the
usage	usage
of	of
14	14
;	;
rather	rather
than	than
abstract	abstract
or	or
general	general
	
regularities	regularities
For	for
example	example
the	the
otherwise	otherwise
unrelated	unrelated
words	words
war	war
and	and
trafficking	trafficking
are	are
similar	similar
	
in	in
the	the
contexts	contexts
of	of
the	the
polysemous	polysemous
word	word
drug	drug
(	(
'narcotic'	'narcotic'
/'medicine'	/'medicine'
)	)
because	because
the	the
	
expressions	expressions
drug	drug
trafficking	trafficking
and	and
the	the
war	war
on	on
drugs	drugs
appear	appear
in	in
related	related
contexts	contexts
of	of
drug	drug
As	as
a	a
	
result	result
both	both
war	war
and	and
trafficking	trafficking
are	are
similar	similar
in	in
being	being
strongly	strongly
indicative	indicative
of	of
the	the
'narcotic'	'narcotic'
	
sense	sense
of	of
drug	drug
	
Words	words
and	and
sentences	sentences
play	play
complementary	complementary
roles	roles
in	in
our	our
approach	approach
:	:
a	a
sentence	sentence
is	is
	
represented	represented
by	by
the	the
set	set
of	of
words	words
it	it
contains	contains
and	and
a	a
word	word
by	by
the	the
set	set
of	of
sentences	sentences
in	in
	
which	which
it	it
appears	appears
Sentences	sentences
are	are
similar	similar
to	to
the	the
extent	extent
they	they
contain	contain
similar	similar
words	words
;	;
2	2
words	words
	
are	are
similar	similar
to	to
the	the
extent	extent
they	they
appear	appear
in	in
similar	similar
sentences	sentences
Although	although
this	this
definition	definition
is	is
	
circular	circular
it	it
turns	turns
out	out
to	to
be	be
of	of
great	great
use	use
if	if
applied	applied
iteratively	iteratively
as	as
described	described
below	below
	
In	in
each	each
iteration	iteration
n	n
we	we
update	update
a	a
word	word
similarity	similarity
matrix	matrix
WSMn	wsmn
(	(
one	one
matrix	matrix
for	for
each	each
	
polysemous	polysemous
word	word
)	)
whose	whose
rows	rows
and	and
columns	columns
are	are
labeled	labeled
by	by
all	all
the	the
words	words
encountered	encountered
	
in	in
the	the
training	training
set	set
of	of
W	w
In	in
that	that
matrix	matrix
the	the
cell	cell
(	(
i	i
j	j
)	)
holds	holds
a	a
value	value
between	between
0	0
and	and
1	1
	
indicating	indicating
the	the
extent	extent
to	to
which	which
word	word
Wi	wi
is	is
contextually	contextually
similar	similar
to	to
word	word
Wj	wj
In	in
addition	addition
	
we	we
keep	keep
and	and
update	update
a	a
separate	separate
sentence	sentence
similarity	similarity
matrix	matrix
SSM	ssm
~	~
for	for
each	each
sense	sense
Sk	sk
of	of
W	w
	
(	(
including	including
a	a
matrix	matrix
SSMok	ssmok
that	that
contains	contains
the	the
similarities	similarities
of	of
the	the
original	original
examples	examples
to	to
	
themselves	themselves
)	)
The	the
rows	rows
in	in
a	a
sentence	sentence
matrix	matrix
SSM	ssm
~	~
correspond	correspond
to	to
the	the
original	original
examples	examples
	
of	of
W	w
and	and
the	the
columns	columns
to	to
the	the
original	original
examples	examples
of	of
W	w
for	for
n	n
=	=
0	0
and	and
to	to
the	the
feedback-set	feedback-set
	
examples	examples
for	for
sense	sense
Sk	sk
for	for
n	n
>	>
0	0
	
To	to
compute	compute
the	the
similarities	similarities
we	we
initialize	initialize
the	the
word	word
similarity	similarity
matrix	matrix
to	to
the	the
identity	identity
	
matrix	matrix
(	(
each	each
word	word
is	is
fully	fully
similar	similar
to	to
itself	itself
and	and
completely	completely
dissimilar	dissimilar
to	to
other	other
words	words
)	)
	
and	and
iterate	iterate
(	(
see	see
Figure	figure
1	1
)	)
:	:
	
1	1
update	update
the	the
sentence	sentence
similarity	similarity
matrices	matrices
SSM	ssm
~	~
using	using
the	the
word	word
similarity	similarity
	
matrix	matrix
WSMn	wsmn
;	;
	
2	2
Ignoring	ignoring
word	word
order	order
This	this
information	information
can	can
be	be
put	put
to	to
use	use
by	by
including	including
n-grams	n-grams
in	in
the	the
feature	feature
set	set
;	;
see	see
	
Section	section
4	4
2	2
	
43	43
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
Word	word
	
Similarity	similarity
	
Matrix	matrix
	
Sentence	sentence
	
Similarity	similarity
	
Matrix	matrix
	
Figure	figure
1	1
	
Iterative	iterative
computation	computation
of	of
word	word
and	and
sentence	sentence
similarities	similarities
	
update	update
the	the
word	word
similarity	similarity
matrix	matrix
WSMn	wsmn
using	using
the	the
sentence	sentence
similarity	similarity
	
matrices	matrices
SSM	ssm
~	~
	
until	until
the	the
changes	changes
in	in
the	the
similarity	similarity
values	values
are	are
small	small
enough	enough
(	(
see	see
Section	section
A	a
1	1
of	of
the	the
	
appendix	appendix
for	for
a	a
detailed	detailed
description	description
of	of
the	the
stopping	stopping
conditions	conditions
;	;
a	a
proof	proof
of	of
convergence	convergence
	
appears	appears
in	in
the	the
appendix	appendix
)	)
	
2	2
2	2
1	1
The	the
Affinity	affinity
Formulae	formulae
The	the
algorithm	algorithm
for	for
updating	updating
the	the
similarity	similarity
matrices	matrices
involves	involves
	
an	an
auxiliary	auxiliary
relation	relation
between	between
words	words
and	and
sentences	sentences
which	which
we	we
call	call
affinity	affinity
	
introduced	introduced
to	to
simplify	simplify
the	the
symmetric	symmetric
iterative	iterative
treatment	treatment
of	of
similarity	similarity
between	between
words	words
	
and	and
sentences	sentences
A	a
word	word
W	w
is	is
assumed	assumed
to	to
have	have
a	a
certain	certain
affinity	affinity
to	to
every	every
sentence	sentence
Affinity	affinity
	
(	(
a	a
real	real
number	number
between	between
0	0
and	and
1	1
)	)
reflects	reflects
the	the
contextual	contextual
relationships	relationships
between	between
W	w
	
and	and
the	the
words	words
of	of
the	the
sentence	sentence
If	if
W	w
belongs	belongs
to	to
a	a
sentence	sentence
S	s
its	its
affinity	affinity
to	to
S	s
is	is
1	1
;	;
if	if
	
W	w
is	is
totally	totally
unrelated	unrelated
to	to
S	s
the	the
affinity	affinity
is	is
close	close
to	to
0	0
(	(
this	this
is	is
the	the
most	most
common	common
case	case
)	)
;	;
	
if	if
W	w
is	is
contextually	contextually
similar	similar
to	to
the	the
words	words
of	of
S	s
its	its
affinity	affinity
to	to
S	s
is	is
between	between
0	0
and	and
1	1
	
In	in
a	a
symmetric	symmetric
manner	manner
a	a
sentence	sentence
S	s
has	has
some	some
affinity	affinity
to	to
every	every
word	word
reflecting	reflecting
the	the
	
similarity	similarity
of	of
S	s
to	to
sentences	sentences
involving	involving
that	that
word	word
	
We	we
say	say
that	that
a	a
word	word
belongs	belongs
to	to
a	a
sentence	sentence
denoted	denoted
as	as
W	w
E	e
S	s
if	if
it	it
is	is
textually	textually
	
contained	contained
there	there
;	;
in	in
this	this
case	case
sentence	sentence
is	is
said	said
to	to
include	include
the	the
word	word
:	:
S	s
9	9
W	w
Affinity	affinity
is	is
	
then	then
defined	defined
as	as
follows	follows
:	:
	
affn	affn
(	(
W	w
S	s
)	)
=	=
max	max
simn	simn
(	(
W	w
Wi	wi
)	)
(	(
1	1
)	)
	
WiCS	wics
	
aff	aff
(	(
S	s
W	w
)	)
=	=
max	max
simn	simn
(	(
S	s
Sj	sj
)	)
(	(
2	2
)	)
	
8j	8j
~w	~w
	
where	where
n	n
denotes	denotes
the	the
iteration	iteration
number	number
and	and
the	the
similarity	similarity
values	values
are	are
defined	defined
by	by
the	the
	
word	word
and	and
sentence	sentence
similarity	similarity
matrices	matrices
WSMn	wsmn
and	and
SSMn	ssmn
3	3
The	the
initial	initial
representation	representation
	
of	of
a	a
sentence	sentence
as	as
the	the
set	set
of	of
words	words
that	that
it	it
directly	directly
contains	contains
is	is
now	now
augmented	augmented
by	by
a	a
	
3	3
At	at
first	first
glance	glance
it	it
may	may
seem	seem
that	that
the	the
mean	mean
rather	rather
than	than
the	the
maximal	maximal
similarity	similarity
of	of
W	w
to	to
the	the
words	words
of	of
a	a
	
sentence	sentence
should	should
determine	determine
the	the
affinity	affinity
between	between
the	the
two	two
However	however
any	any
definition	definition
of	of
affinity	affinity
that	that
takes	takes
	
into	into
account	account
more	more
words	words
than	than
just	just
the	the
one	one
with	with
the	the
maximal	maximal
similarity	similarity
to	to
W	w
may	may
result	result
in	in
a	a
word	word
	
being	being
directly	directly
contained	contained
in	in
the	the
sentence	sentence
but	but
having	having
an	an
affinity	affinity
to	to
it	it
that	that
is	is
smaller	smaller
than	than
1	1
	
44	44
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
similarity-based	similarity-based
representation	representation
;	;
the	the
sentence	sentence
contains	contains
more	more
information	information
or	or
features	features
	
than	than
the	the
words	words
directly	directly
contained	contained
in	in
it	it
Every	every
word	word
has	has
some	some
affinity	affinity
to	to
the	the
sentence	sentence
	
and	and
the	the
sentence	sentence
can	can
be	be
represented	represented
by	by
a	a
vector	vector
indicating	indicating
the	the
affinity	affinity
of	of
each	each
word	word
	
to	to
it	it
Similarly	similarly
every	every
word	word
can	can
be	be
represented	represented
by	by
the	the
affinity	affinity
of	of
every	every
sentence	sentence
to	to
it	it
	
Note	note
that	that
affinity	affinity
is	is
asymmetric	asymmetric
:	:
aft	aft
(	(
S	s
14	14
;	;
)	)
~	~
aft	aft
(	(
W	w
S	s
)	)
because	because
14	14
;	;
may	may
be	be
similar	similar
to	to
	
one	one
of	of
the	the
words	words
in	in
S	s
which	which
however	however
is	is
not	not
one	one
of	of
the	the
topic	topic
words	words
of	of
S	s
;	;
it	it
is	is
not	not
an	an
	
important	important
word	word
in	in
S	s
In	in
this	this
case	case
aft	aft
(	(
W	w
S	s
)	)
is	is
high	high
because	because
14	14
;	;
is	is
similar	similar
to	to
a	a
word	word
	
in	in
S	s
but	but
aft	aft
(	(
S	s
14	14
;	;
)	)
is	is
low	low
because	because
S	s
is	is
not	not
a	a
representative	representative
example	example
of	of
the	the
usage	usage
of	of
	
the	the
word	word
14	14
;	;
	
2	2
2	2
2	2
The	the
Similarity	similarity
Formulae	formulae
We	we
define	define
the	the
similarity	similarity
of	of
14	14
;1	;1
to	to
14	14
;2	;2
to	to
be	be
the	the
average	average
	
affinity	affinity
of	of
sentences	sentences
that	that
include	include
14	14
;1	;1
to	to
14	14
;2	;2
The	the
similarity	similarity
of	of
a	a
sentence	sentence
$1	$1
to	to
another	another
	
sentence	sentence
82	82
is	is
a	a
weighted	weighted
average	average
of	of
the	the
affinity	affinity
of	of
the	the
words	words
in	in
$1	$1
to	to
$2	$2
:	:
	
simn	simn
+l	+l
(	(
Sb	sb
$2	$2
)	)
=	=
~	~
weight	weight
(	(
W	w
$1	$1
)	)
affn	affn
(	(
W	w
$2	$2
)	)
(	(
3	3
)	)
	
WE	we
$1	$1
	
simn	simn
+l	+l
(	(
14	14
;1	;1
14	14
;2	;2
)	)
=	=
~	~
weight	weight
(	(
S	s
14	14
;1	;1
)	)
affn	affn
(	(
S	s
14	14
;2	;2
)	)
(	(
4	4
)	)
	
$3W1	$3w1
	
where	where
the	the
weights	weights
sum	sum
to	to
1	1
4	4
These	these
values	values
are	are
used	used
to	to
update	update
the	the
corresponding	corresponding
	
entries	entries
of	of
the	the
word	word
and	and
sentence	sentence
similarity	similarity
matrices	matrices
WSM	wsm
and	and
SSM	ssm
	
2	2
2	2
3	3
The	the
Importance	importance
of	of
Iteration	iteration
Initially	initially
only	only
identical	identical
words	words
are	are
considered	considered
similar	similar
	
so	so
that	that
aft	aft
(	(
W	w
S	s
)	)
=	=
1	1
if	if
14	14
;	;
E	e
S	s
;	;
the	the
affinity	affinity
is	is
zero	zero
otherwise	otherwise
Thus	thus
in	in
the	the
first	first
	
iteration	iteration
the	the
similarity	similarity
between	between
81	81
and	and
$2	$2
depends	depends
on	on
the	the
number	number
of	of
words	words
from	from
	
$1	$1
that	that
appear	appear
in	in
$2	$2
divided	divided
by	by
the	the
length	length
of	of
$2	$2
(	(
note	note
that	that
each	each
word	word
may	may
carry	carry
	
different	different
weight	weight
)	)
In	in
the	the
subsequent	subsequent
iterations	iterations
each	each
word	word
14	14
;	;
c	c
$1	$1
contributes	contributes
to	to
the	the
	
similarity	similarity
of	of
81	81
to	to
$2	$2
a	a
value	value
between	between
0	0
and	and
1	1
indicating	indicating
its	its
affinity	affinity
to	to
$2	$2
instead	instead
of	of
	
voting	voting
either	either
0	0
(	(
if	if
14	14
;	;
E	e
$2	$2
)	)
or	or
1	1
(	(
if	if
14	14
;	;
~	~
82	82
)	)
Analogously	analogously
sentences	sentences
contribute	contribute
values	values
	
to	to
word	word
similarity	similarity
	
One	one
may	may
view	view
the	the
iterations	iterations
as	as
successively	successively
capturing	capturing
parameterized	parameterized
genealogical	genealogical
	
relationships	relationships
Let	let
words	words
that	that
share	share
contexts	contexts
be	be
called	called
direct	direct
relatives	relatives
;	;
then	then
words	words
	
that	that
share	share
neighbors	neighbors
(	(
have	have
similar	similar
co-occurrence	co-occurrence
patterns	patterns
)	)
are	are
once-removed	once-removed
relatives	relatives
	
These	these
two	two
family	family
relationships	relationships
are	are
captured	captured
by	by
the	the
first	first
iteration	iteration
and	and
also	also
by	by
most	most
	
traditional	traditional
similarity	similarity
measures	measures
which	which
are	are
based	based
on	on
co-occurrences	co-occurrences
The	the
second	second
iteration	iteration
	
then	then
brings	brings
together	together
twice-removed	twice-removed
relatives	relatives
The	the
third	third
iteration	iteration
captures	captures
higher	higher
	
similarity	similarity
relationships	relationships
and	and
so	so
on	on
Note	note
that	that
the	the
level	level
of	of
relationship	relationship
here	here
is	is
a	a
gradually	gradually
	
consolidated	consolidated
real-valued	real-valued
quantity	quantity
and	and
is	is
dictated	dictated
by	by
the	the
amount	amount
and	and
the	the
quality	quality
	
of	of
the	the
evidence	evidence
gleaned	gleaned
from	from
the	the
corpus	corpus
;	;
it	it
is	is
not	not
an	an
all-or-none	all-or-none
relatedness	relatedness
tag	tag
as	as
	
in	in
genealogy	genealogy
	
The	the
following	following
simple	simple
example	example
demonstrates	demonstrates
the	the
difference	difference
between	between
our	our
similarity	similarity
	
measure	measure
and	and
pure	pure
co-occurrence-based	co-occurrence-based
similarity	similarity
measures	measures
which	which
cannot	cannot
capture	capture
	
4	4
The	the
weight	weight
of	of
a	a
word	word
estimates	estimates
its	its
expected	expected
contribution	contribution
to	to
the	the
disambiguation	disambiguation
task	task
and	and
is	is
a	a
product	product
	
of	of
several	several
factors	factors
:	:
the	the
frequency	frequency
of	of
the	the
word	word
in	in
the	the
corpus	corpus
;	;
its	its
frequency	frequency
in	in
the	the
training	training
set	set
relative	relative
to	to
	
that	that
in	in
the	the
entire	entire
corpus	corpus
;	;
the	the
textual	textual
distance	distance
from	from
the	the
target	target
word	word
;	;
and	and
its	its
part	part
of	of
speech	speech
(	(
more	more
details	details
	
on	on
word	word
weights	weights
appear	appear
in	in
Section	section
A	a
3	3
of	of
the	the
appendix	appendix
)	)
All	all
the	the
sentences	sentences
that	that
include	include
a	a
given	given
word	word
	
are	are
assigned	assigned
identical	identical
weights	weights
	
45	45
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
higher-order	higher-order
relationships	relationships
Consider	consider
the	the
set	set
of	of
three	three
sentence	sentence
fragments	fragments
:	:
	
sl	sl
eat	eat
banana	banana
	
s2	s2
taste	taste
banana	banana
	
s3	s3
eat	eat
apple	apple
	
In	in
this	this
corpus	corpus
the	the
contextual	contextual
similarity	similarity
of	of
taste	taste
and	and
apple	apple
according	according
to	to
the	the
cooccurrence-	cooccurrence-
	
based	based
methods	methods
is	is
0	0
because	because
the	the
contexts	contexts
of	of
these	these
two	two
words	words
are	are
disjoint	disjoint
	
In	in
comparison	comparison
our	our
iterative	iterative
algorithm	algorithm
will	will
capture	capture
some	some
contextual	contextual
similarity	similarity
:	:
	
â	â
€	€
¢	¢
Initialization	initialization
Every	every
word	word
is	is
similar	similar
to	to
itself	itself
only	only
	
â	â
€	€
¢	¢
First	first
iteration	iteration
The	the
sentences	sentences
eat	eat
banana	banana
and	and
eat	eat
apple	apple
have	have
contextual	contextual
	
similarity	similarity
of	of
0	0
5	5
because	because
of	of
the	the
common	common
word	word
eat	eat
Furthermore	furthermore
the	the
	
sentences	sentences
eat	eat
banana	banana
and	and
taste	taste
banana	banana
have	have
contextual	contextual
similarity	similarity
0	0
5	5
:	:
	
--	--
banana	banana
is	is
learned	learned
to	to
be	be
similar	similar
to	to
apple	apple
because	because
of	of
their	their
common	common
	
usage	usage
(	(
eat	eat
banana	banana
and	and
eat	eat
apple	apple
)	)
;	;
	
-	-
-	-
taste	taste
is	is
similar	similar
to	to
eat	eat
because	because
of	of
their	their
common	common
usage	usage
(	(
taste	taste
banana	banana
	
and	and
eat	eat
banana	banana
)	)
;	;
	
-	-
-	-
taste	taste
and	and
apple	apple
are	are
not	not
similar	similar
(	(
yet	yet
)	)
	
â	â
€	€
¢	¢
Second	second
iteration	iteration
The	the
sentence	sentence
taste	taste
banana	banana
has	has
now	now
some	some
similarity	similarity
to	to
eat	eat
	
apple	apple
because	because
in	in
the	the
previous	previous
iteration	iteration
taste	taste
was	was
similar	similar
to	to
eat	eat
and	and
banana	banana
	
was	was
similar	similar
to	to
apple	apple
The	the
word	word
taste	taste
is	is
now	now
similar	similar
to	to
apple	apple
because	because
the	the
	
taste	taste
sentence	sentence
(	(
taste	taste
banana	banana
)	)
is	is
similar	similar
to	to
the	the
apple	apple
sentence	sentence
(	(
eat	eat
apple	apple
)	)
Yet	yet
	
banana	banana
is	is
more	more
similar	similar
to	to
apple	apple
than	than
taste	taste
because	because
the	the
similarity	similarity
value	value
of	of
	
banana	banana
and	and
apple	apple
further	further
increases	increases
in	in
the	the
second	second
iteration	iteration
	
This	this
simple	simple
example	example
demonstrates	demonstrates
the	the
transitivity	transitivity
of	of
our	our
similarity	similarity
measure	measure
which	which
	
allows	allows
it	it
to	to
extract	extract
high-order	high-order
contextual	contextual
relationships	relationships
In	in
more	more
complex	complex
situations	situations
the	the
	
transitivity-dependent	transitivity-dependent
spread	spread
of	of
similarity	similarity
is	is
slower	slower
because	because
each	each
word	word
is	is
represented	represented
	
by	by
many	many
more	more
sentences	sentences
	
The	the
most	most
important	important
properties	properties
of	of
the	the
similarity	similarity
computation	computation
algorithm	algorithm
are	are
convergence	convergence
	
(	(
see	see
Section	section
A	a
2	2
in	in
the	the
appendix	appendix
)	)
and	and
utility	utility
in	in
supporting	supporting
disambiguation	disambiguation
	
(	(
described	described
in	in
Section	section
3	3
)	)
;	;
three	three
other	other
properties	properties
are	are
as	as
follows	follows
First	first
word	word
similarity	similarity
	
computed	computed
according	according
to	to
the	the
above	above
algorithm	algorithm
is	is
asymmetric	asymmetric
For	for
example	example
drug	drug
is	is
more	more
	
similar	similar
to	to
traffic	traffic
than	than
traffic	traffic
is	is
to	to
drug	drug
because	because
traffic	traffic
is	is
mentioned	mentioned
more	more
frequently	frequently
	
in	in
drug	drug
contexts	contexts
than	than
drug	drug
is	is
mentioned	mentioned
in	in
contexts	contexts
of	of
traffic	traffic
(	(
which	which
has	has
many	many
other	other
	
usages	usages
)	)
Likewise	likewise
sentence	sentence
similarity	similarity
is	is
asymmetric	asymmetric
:	:
if	if
81	81
is	is
fully	fully
contained	contained
in	in
$2	$2
then	then
	
sire	sire
(	(
S1	s1
$2	$2
)	)
=	=
1	1
whereas	whereas
sim	sim
(	(
$2	$2
$1	$1
)	)
<	<
1	1
Second	second
words	words
with	with
a	a
small	small
count	count
in	in
the	the
	
training	training
set	set
will	will
have	have
unreliable	unreliable
similarity	similarity
values	values
These	these
however	however
are	are
multiplied	multiplied
by	by
a	a
	
very	very
low	low
weight	weight
when	when
used	used
in	in
sentence	sentence
similarity	similarity
evaluation	evaluation
because	because
the	the
frequency	frequency
	
in	in
the	the
training	training
set	set
is	is
taken	taken
into	into
account	account
in	in
computing	computing
the	the
word	word
weights	weights
Third	third
in	in
	
the	the
computation	computation
of	of
sim	sim
(	(
W1	w1
W2	w2
)	)
for	for
a	a
very	very
frequent	frequent
W2	w2
the	the
set	set
of	of
its	its
sentences	sentences
is	is
	
very	very
large	large
potentially	potentially
inflating	inflating
the	the
affinity	affinity
of	of
W1	w1
to	to
the	the
sentences	sentences
that	that
contain	contain
W2	w2
We	we
	
counter	counter
this	this
tendency	tendency
by	by
multiplying	multiplying
sire	sire
(	(
W1	w1
W2	w2
)	)
by	by
a	a
weight	weight
that	that
is	is
reciprocally	reciprocally
	
related	related
to	to
the	the
global	global
frequency	frequency
of	of
W2	w2
(	(
this	this
weight	weight
has	has
been	been
left	left
out	out
of	of
Equation	equation
4	4
to	to
	
keep	keep
the	the
notation	notation
there	there
simple	simple
)	)
	
46	46
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
2	2
3	3
Using	using
Similarity	similarity
to	to
Tag	tag
the	the
Training	training
Set	set
	
Following	following
convergence	convergence
each	each
sentence	sentence
in	in
the	the
training	training
set	set
is	is
assigned	assigned
the	the
sense	sense
of	of
its	its
	
most	most
similar	similar
sentence	sentence
in	in
one	one
of	of
the	the
feedback	feedback
sets	sets
of	of
sense	sense
si	si
using	using
the	the
final	final
sentence	sentence
	
similarity	similarity
matrix	matrix
Note	note
that	that
some	some
sentences	sentences
in	in
the	the
training	training
set	set
belong	belong
also	also
to	to
one	one
of	of
the	the
	
feedback	feedback
sets	sets
because	because
they	they
contain	contain
words	words
from	from
the	the
MRD	mrd
definition	definition
of	of
the	the
target	target
word	word
	
Those	those
sentences	sentences
are	are
automatically	automatically
assigned	assigned
the	the
sense	sense
of	of
the	the
feedback	feedback
set	set
to	to
which	which
they	they
	
belong	belong
since	since
they	they
are	are
most	most
similar	similar
to	to
themselves	themselves
Note	note
also	also
that	that
an	an
original	original
trainingset	trainingset
	
sentence	sentence
S	s
can	can
be	be
attracted	attracted
to	to
a	a
sentence	sentence
T	t
from	from
a	a
feedback	feedback
set	set
even	even
if	if
S	s
and	and
T	t
	
do	do
not	not
share	share
any	any
word	word
because	because
of	of
the	the
transitivity	transitivity
of	of
the	the
similarity	similarity
measure	measure
	
2	2
4	4
Learning	learning
the	the
typical	typical
uses	uses
of	of
each	each
sense	sense
	
We	we
partition	partition
the	the
examples	examples
of	of
each	each
sense	sense
into	into
typical	typical
use	use
sets	sets
by	by
grouping	grouping
all	all
the	the
	
sentences	sentences
that	that
were	were
attracted	attracted
to	to
the	the
same	same
feedback-set	feedback-set
sentence	sentence
That	that
sentence	sentence
and	and
	
all	all
the	the
original	original
sentences	sentences
attracted	attracted
to	to
it	it
form	form
a	a
class	class
of	of
examples	examples
for	for
a	a
typical	typical
usage	usage
	
Feedback-set	feedback-set
examples	examples
that	that
did	did
not	not
attract	attract
any	any
original	original
sentences	sentences
are	are
discarded	discarded
If	if
the	the
	
number	number
of	of
resulting	resulting
classes	classes
is	is
too	too
high	high
further	further
clustering	clustering
can	can
be	be
carried	carried
out	out
on	on
the	the
	
basis	basis
of	of
the	the
distance	distance
metric	metric
defined	defined
by	by
1	1
-	-
sim	sim
(	(
x	x
y	y
)	)
where	where
sire	sire
(	(
x	x
y	y
)	)
are	are
values	values
taken	taken
	
from	from
the	the
final	final
sentence	sentence
similarity	similarity
matrix	matrix
	
A	a
typical	typical
usage	usage
of	of
a	a
sense	sense
is	is
represented	represented
by	by
the	the
affinity	affinity
information	information
generalized	generalized
	
from	from
its	its
examples	examples
For	for
each	each
word	word
14	14
;	;
and	and
each	each
cluster	cluster
C	c
of	of
examples	examples
of	of
the	the
same	same
	
usage	usage
we	we
define	define
:	:
	
aff	aff
(	(
W	w
C	c
)	)
=	=
max	max
aff	aff
(	(
W	w
S	s
)	)
(	(
5	5
)	)
	
SEC	sec
	
=	=
max	max
max	max
sim	sim
(	(
W	w
Wi	wi
)	)
(	(
6	6
)	)
	
SEC	sec
WiCS	wics
	
For	for
each	each
cluster	cluster
we	we
construct	construct
its	its
affinity	affinity
vector	vector
whose	whose
ith	ith
component	component
indicates	indicates
the	the
	
affinity	affinity
of	of
word	word
i	i
to	to
the	the
cluster	cluster
It	it
suffices	suffices
to	to
generalize	generalize
the	the
affinity	affinity
information	information
(	(
rather	rather
	
than	than
similarity	similarity
)	)
because	because
new	new
examples	examples
are	are
judged	judged
on	on
the	the
basis	basis
of	of
their	their
similarity	similarity
to	to
	
each	each
cluster	cluster
:	:
in	in
the	the
computation	computation
of	of
sire	sire
(	(
S1	s1
$2	$2
)	)
(	(
Equation	equation
3	3
)	)
the	the
only	only
information	information
	
concerning	concerning
$2	$2
is	is
its	its
affinity	affinity
values	values
	
2	2
5	5
Testing	testing
New	new
Examples	examples
	
Given	given
a	a
new	new
sentence	sentence
S	s
containing	containing
a	a
target	target
word	word
W	w
we	we
determine	determine
its	its
sense	sense
by	by
computing	computing
	
the	the
similarity	similarity
of	of
S	s
to	to
each	each
of	of
the	the
previously	previously
obtained	obtained
clusters	clusters
Ck	ck
and	and
returning	returning
	
the	the
sense	sense
si	si
of	of
the	the
most	most
similar	similar
cluster	cluster
:	:
	
sim	sim
(	(
Snew	snew
Ck	ck
)	)
=	=
~	~
weight	weight
(	(
W	w
Snew	snew
)	)
aff	aff
(	(
W	w
Ck	ck
)	)
(	(
7	7
)	)
	
W	w
E	e
S	s
ew	ew
	
sim	sim
(	(
Snew	snew
si	si
)	)
=	=
max	max
sim	sim
(	(
Snew	snew
C	c
)	)
(	(
8	8
)	)
	
Ccsi	ccsi
	
3	3
Experimental	experimental
Evaluation	evaluation
of	of
the	the
Method	method
	
We	we
tested	tested
the	the
algorithm	algorithm
on	on
the	the
Treebank-2	treebank-2
corpus	corpus
which	which
contains	contains
one	one
million	million
words	words
	
from	from
the	the
Wall	wall
Street	street
Journal	journal
1989	1989
and	and
is	is
considered	considered
a	a
small	small
corpus	corpus
for	for
the	the
present	present
	
task	task
During	during
the	the
development	development
and	and
the	the
tuning	tuning
of	of
the	the
algorithm	algorithm
we	we
used	used
the	the
method	method
	
of	of
pseudowords	pseudowords
(	(
Gale	gale
Church	church
and	and
Yarowsky	yarowsky
1992	1992
;	;
Schitze	schitze
1992	1992
)	)
to	to
avoid	avoid
the	the
need	need
	
for	for
manual	manual
verification	verification
of	of
the	the
resulting	resulting
sense	sense
tags	tags
	
The	the
method	method
of	of
pseudowords	pseudowords
is	is
based	based
on	on
the	the
observation	observation
that	that
a	a
disambiguation	disambiguation
	
process	process
designed	designed
to	to
distinguish	distinguish
between	between
two	two
meanings	meanings
of	of
the	the
same	same
word	word
should	should
also	also
	
47	47
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
Table	table
1	1
	
The	the
four	four
polysemous	polysemous
test	test
words	words
and	and
the	the
seed	seed
words	words
they	they
generated	generated
with	with
the	the
use	use
of	of
the	the
MRD	mrd
	
Word	word
Seed	seed
Words	words
	
drug	drug
1	1
stimulant	stimulant
alcoholist	alcoholist
alcohol	alcohol
trafficker	trafficker
crime	crime
	
2	2
medicine	medicine
pharmaceutical	pharmaceutical
remedy	remedy
cure	cure
medication	medication
pharmacists	pharmacists
	
prescription	prescription
	
1	1
conviction	conviction
judgment	judgment
acquittal	acquittal
term	term
	
2	2
string	string
word	word
constituent	constituent
dialogue	dialogue
talk	talk
conversation	conversation
text	text
	
1	1
trial	trial
litigation	litigation
receivership	receivership
bankruptcy	bankruptcy
appeal	appeal
action	action
case	case
lawsuit	lawsuit
	
foreclosure	foreclosure
proceeding	proceeding
	
2	2
garment	garment
fabric	fabric
trousers	trousers
pants	pants
dress	dress
frock	frock
fur	fur
silks	silks
hat	hat
boots	boots
coat	coat
shirt	shirt
	
sweater	sweater
vest	vest
waistcoat	waistcoat
skirt	skirt
jacket	jacket
cloth	cloth
	
1	1
musician	musician
instrumentalist	instrumentalist
performer	performer
artist	artist
actor	actor
twirler	twirler
comedian	comedian
dancer	dancer
	
impersonator	impersonator
imitator	imitator
bandsman	bandsman
jazz	jazz
recorder	recorder
singer	singer
vocalist	vocalist
actress	actress
	
barnstormer	barnstormer
playactor	playactor
trouper	trouper
character	character
actor	actor
scene-stealer	scene-stealer
star	star
baseball	baseball
	
ball	ball
football	football
basketball	basketball
	
2	2
participant	participant
contestant	contestant
trader	trader
analyst	analyst
dealer	dealer
	
sentence	sentence
	
suit	suit
	
player	player
	
be	be
able	able
to	to
separate	separate
the	the
meanings	meanings
of	of
two	two
different	different
words	words
Thus	thus
a	a
data	data
set	set
for	for
testing	testing
	
a	a
disambiguation	disambiguation
algorithm	algorithm
can	can
be	be
obtained	obtained
by	by
starting	starting
with	with
two	two
collections	collections
of	of
sentences	sentences
	
one	one
containing	containing
a	a
word	word
X	x
and	and
the	the
other	other
a	a
word	word
y	y
and	and
inserting	inserting
y	y
instead	instead
of	of
	
every	every
appearance	appearance
of	of
-Y	-y
in	in
the	the
first	first
collection	collection
The	the
algorithm	algorithm
is	is
then	then
tested	tested
on	on
the	the
union	union
	
of	of
the	the
two	two
collections	collections
in	in
which	which
X	x
is	is
now	now
a	a
polysemous	polysemous
word	word
The	the
performance	performance
of	of
	
the	the
algorithm	algorithm
is	is
judged	judged
by	by
its	its
ability	ability
to	to
separate	separate
the	the
sentences	sentences
that	that
originally	originally
contained	contained
	
~'	~'
from	from
those	those
that	that
originally	originally
contained	contained
32	32
;	;
any	any
mistakes	mistakes
can	can
be	be
used	used
to	to
supervise	supervise
the	the
	
tuning	tuning
of	of
the	the
algorithm	algorithm
5	5
	
3	3
1	1
Test	test
data	data
	
The	the
final	final
algorithm	algorithm
was	was
tested	tested
on	on
a	a
total	total
of	of
500	500
examples	examples
of	of
four	four
polysemous	polysemous
words	words
:	:
	
drug	drug
sentence	sentence
suit	suit
and	and
player	player
(	(
see	see
Table	table
2	2
;	;
although	although
we	we
confined	confined
the	the
tests	tests
to	to
nouns	nouns
	
the	the
algorithm	algorithm
is	is
applicable	applicable
to	to
any	any
part	part
of	of
speech	speech
)	)
The	the
relatively	relatively
small	small
number	number
of	of
	
polysemous	polysemous
words	words
we	we
studied	studied
was	was
dictated	dictated
by	by
the	the
size	size
and	and
nature	nature
of	of
the	the
corpus	corpus
(	(
we	we
	
are	are
currently	currently
testing	testing
additional	additional
words	words
using	using
texts	texts
from	from
the	the
British	british
National	national
Corpus	corpus
)	)
	
As	as
the	the
MRD	mrd
we	we
used	used
a	a
combination	combination
of	of
the	the
on-line	on-line
versions	versions
of	of
the	the
Webster's	webster's
and	and
	
the	the
Oxford	oxford
dictionaries	dictionaries
and	and
the	the
WordNet	wordnet
system	system
(	(
the	the
latter	latter
used	used
as	as
a	a
thesaurus	thesaurus
only	only
;	;
	
see	see
Section	section
4	4
3	3
)	)
The	the
resulting	resulting
collection	collection
of	of
seed	seed
words	words
(	(
that	that
is	is
words	words
used	used
to	to
generate	generate
	
the	the
feedback	feedback
sets	sets
)	)
is	is
listed	listed
in	in
Table	table
1	1
	
We	we
found	found
that	that
the	the
single	single
best	best
source	source
of	of
seed	seed
words	words
was	was
WordNet	wordnet
(	(
used	used
as	as
thesaurus	thesaurus
	
only	only
)	)
The	the
number	number
of	of
seed	seed
words	words
per	per
sense	sense
turned	turned
out	out
to	to
be	be
of	of
little	little
significance	significance
	
For	for
example	example
whereas	whereas
the	the
MRD	mrd
yielded	yielded
many	many
garment-related	garment-related
words	words
to	to
be	be
used	used
as	as
	
seeds	seeds
for	for
suit	suit
in	in
the	the
'garment'	'garment'
sense	sense
these	these
generated	generated
a	a
small	small
feedback	feedback
set	set
because	because
	
of	of
the	the
low	low
frequency	frequency
of	of
garment-related	garment-related
words	words
in	in
the	the
training	training
corpus	corpus
In	in
comparison	comparison
	
there	there
was	was
a	a
strong	strong
correlation	correlation
between	between
the	the
size	size
of	of
the	the
feedback	feedback
set	set
and	and
the	the
disambiguation	disambiguation
	
performance	performance
indicating	indicating
that	that
a	a
larger	larger
corpus	corpus
is	is
likely	likely
to	to
improve	improve
the	the
results	results
	
As	as
can	can
be	be
seen	seen
from	from
the	the
above	above
the	the
original	original
training	training
data	data
(	(
before	before
the	the
addition	addition
of	of
	
5	5
Note	note
that	that
our	our
disambiguation	disambiguation
algorithm	algorithm
works	works
the	the
best	best
for	for
polysemous	polysemous
words	words
whose	whose
senses	senses
are	are
	
unrelated	unrelated
to	to
each	each
other	other
in	in
which	which
case	case
the	the
overlap	overlap
between	between
the	the
feedback	feedback
sets	sets
is	is
minimized	minimized
;	;
likewise	likewise
	
the	the
method	method
of	of
training	training
with	with
pseudowords	pseudowords
amounts	amounts
to	to
an	an
assumption	assumption
of	of
independence	independence
of	of
the	the
different	different
	
senses	senses
	
48	48
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
Table	table
2	2
	
A	a
summary	summary
of	of
the	the
algorithm's	algorithm's
performance	performance
on	on
the	the
four	four
test	test
words	words
	
Word	word
Senses	senses
Sample	sample
Size	size
Feedback	feedback
Size	size
%	%
Correct	correct
%	%
Correct	correct
	
per	per
Sense	sense
Total	total
	
drug	drug
narcotic	narcotic
65	65
100	100
92	92
3	3
90	90
5	5
	
medicine	medicine
83	83
65	65
89	89
1	1
	
sentence	sentence
judgment	judgment
23	23
327	327
100	100
0	0
92	92
5	5
	
grammar	grammar
4	4
42	42
50	50
0	0
	
suit	suit
court	court
212	212
1	1
461	461
98	98
6	6
94	94
8	8
	
garment	garment
21	21
81	81
55	55
0	0
	
player	player
performer	performer
48	48
230	230
87	87
5	5
92	92
3	3
	
participant	participant
44	44
1	1
552	552
97	97
7	7
	
the	the
feedback	feedback
sets	sets
)	)
consisted	consisted
of	of
a	a
few	few
dozen	dozen
examples	examples
in	in
comparison	comparison
to	to
thousands	thousands
of	of
	
examples	examples
needed	needed
in	in
other	other
corpus-based	corpus-based
methods	methods
(	(
Sch	sch
~itze	~itze
1992	1992
;	;
Yarowsky	yarowsky
1995	1995
)	)
The	the
	
average	average
success	success
rate	rate
of	of
our	our
algorithm	algorithm
on	on
the	the
500	500
appearances	appearances
of	of
the	the
four	four
test	test
words	words
	
was	was
92	92
%	%
	
3	3
2	2
The	the
Drug	drug
Experiment	experiment
	
We	we
now	now
present	present
in	in
detail	detail
several	several
of	of
the	the
results	results
obtained	obtained
with	with
the	the
word	word
drug	drug
Consider	consider
	
first	first
the	the
effects	effects
of	of
iteration	iteration
A	a
plot	plot
of	of
the	the
improvement	improvement
in	in
the	the
performance	performance
vs	vs
iteration	iteration
	
number	number
appears	appears
in	in
Figure	figure
2	2
The	the
success	success
rate	rate
is	is
plotted	plotted
for	for
each	each
sense	sense
and	and
for	for
the	the
	
weighted	weighted
average	average
of	of
both	both
senses	senses
we	we
considered	considered
(	(
the	the
weights	weights
are	are
proportional	proportional
to	to
the	the
	
number	number
of	of
examples	examples
of	of
each	each
sense	sense
)	)
Iterations	iterations
2	2
and	and
5	5
can	can
be	be
seen	seen
to	to
yield	yield
the	the
best	best
	
performance	performance
;	;
iteration	iteration
5	5
is	is
to	to
be	be
preferred	preferred
because	because
of	of
the	the
smaller	smaller
difference	difference
between	between
	
the	the
success	success
rates	rates
for	for
the	the
two	two
senses	senses
of	of
the	the
target	target
word	word
	
Figure	figure
3	3
shows	shows
how	how
the	the
similarity	similarity
values	values
develop	develop
with	with
iteration	iteration
number	number
For	for
each	each
	
example	example
S	s
of	of
the	the
'narcotic'	'narcotic'
sense	sense
of	of
drug	drug
the	the
value	value
of	of
simn	simn
(	(
S	s
narcotic	narcotic
)	)
increases	increases
with	with
n	n
	
Figure	figure
4	4
compares	compares
the	the
similarities	similarities
of	of
a	a
'narcotic'-sense	'narcotic'-sense
example	example
to	to
the	the
'narcotic'	'narcotic'
sense	sense
	
and	and
to	to
the	the
'medicine'	'medicine'
sense	sense
for	for
each	each
iteration	iteration
One	one
can	can
see	see
that	that
the	the
'medicine'	'medicine'
sense	sense
	
assignment	assignment
made	made
in	in
the	the
first	first
iteration	iteration
is	is
gradually	gradually
suppressed	suppressed
The	the
word	word
menace	menace
	
which	which
is	is
a	a
hint	hint
for	for
the	the
'narcotic'	'narcotic'
sense	sense
in	in
the	the
sentence	sentence
used	used
in	in
this	this
example	example
did	did
	
not	not
help	help
in	in
the	the
first	first
iteration	iteration
because	because
it	it
did	did
not	not
appear	appear
in	in
the	the
'narcotic'	'narcotic'
feedback	feedback
	
set	set
at	at
all	all
Thus	thus
in	in
iteration	iteration
1	1
the	the
similarity	similarity
of	of
the	the
sentence	sentence
to	to
the	the
'medicine'	'medicine'
sense	sense
	
was	was
0	0
15	15
vs	vs
a	a
similarity	similarity
of	of
0	0
1	1
to	to
the	the
'narcotic'	'narcotic'
sense	sense
In	in
iteration	iteration
2	2
menace	menace
was	was
	
learned	learned
to	to
be	be
similar	similar
to	to
other	other
'narcotic'-related	'narcotic'-related
words	words
yielding	yielding
a	a
small	small
advantage	advantage
for	for
	
the	the
'narcotic'	'narcotic'
sense	sense
In	in
iteration	iteration
3	3
further	further
similarity	similarity
values	values
were	were
updated	updated
and	and
there	there
	
was	was
a	a
clear	clear
advantage	advantage
to	to
the	the
'narcotic'	'narcotic'
sense	sense
(	(
0	0
93	93
vs	vs
0	0
89	89
for	for
'medicine'	'medicine'
)	)
Eventually	eventually
	
all	all
similarity	similarity
values	values
become	become
close	close
to	to
1	1
and	and
because	because
they	they
are	are
bounded	bounded
by	by
1	1
they	they
cannot	cannot
	
change	change
significantly	significantly
with	with
further	further
iterations	iterations
The	the
decision	decision
is	is
therefore	therefore
best	best
made	made
after	after
	
relatively	relatively
few	few
iterations	iterations
as	as
we	we
just	just
saw	saw
	
Table	table
3	3
shows	shows
the	the
most	most
similar	similar
words	words
found	found
for	for
the	the
words	words
with	with
the	the
highest	highest
weights	weights
	
in	in
the	the
drug	drug
example	example
(	(
low-similarity	low-similarity
words	words
have	have
been	been
omitted	omitted
)	)
Note	note
that	that
the	the
similarity	similarity
	
is	is
contextual	contextual
and	and
is	is
affected	affected
by	by
the	the
polysemous	polysemous
target	target
word	word
For	for
example	example
trafficking	trafficking
	
was	was
found	found
to	to
be	be
similar	similar
to	to
crime	crime
because	because
in	in
drug	drug
contexts	contexts
the	the
expressions	expressions
drug	drug
trafficking	trafficking
	
and	and
crime	crime
are	are
highly	highly
related	related
In	in
general	general
trafficking	trafficking
and	and
crime	crime
need	need
not	not
be	be
similar	similar
of	of
	
course	course
	
49	49
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
Table	table
3	3
	
The	the
drug	drug
experiment	experiment
;	;
the	the
nearest	nearest
neighbors	neighbors
of	of
the	the
highest-weight	highest-weight
words	words
	
Word	word
Most	most
Contextually	contextually
Similar	similar
Words	words
	
The	the
'Medicine'	'medicine'
Sense	sense
	
medication	medication
antibiotic	antibiotic
blood	blood
prescription	prescription
medicine	medicine
percentage	percentage
pressure	pressure
	
prescription	prescription
analyst	analyst
antibiotic	antibiotic
blood	blood
campaign	campaign
introduction	introduction
law	law
line-up	line-up
medication	medication
medicine	medicine
	
percentage	percentage
print	print
profit	profit
publicity	publicity
quarter	quarter
sedative	sedative
state	state
television	television
tranquilizer	tranquilizer
use	use
	
medicine	medicine
prescription	prescription
campaign	campaign
competition	competition
dollar	dollar
earnings	earnings
law	law
manufacturing	manufacturing
	
margin	margin
print	print
product	product
publicity	publicity
quarter	quarter
result	result
sale	sale
saving	saving
sedative	sedative
	
staff	staff
state	state
television	television
tranquilizer	tranquilizer
unit	unit
use	use
	
disease	disease
antibiotic	antibiotic
blood	blood
line-up	line-up
medication	medication
medicine	medicine
prescription	prescription
	
symptom	symptom
hypoglycemia	hypoglycemia
insulin	insulin
warning	warning
manufacturer	manufacturer
product	product
	
plant	plant
animal	animal
death	death
diabetic	diabetic
evidence	evidence
finding	finding
metabolism	metabolism
study	study
	
insulin	insulin
hypoglycemia	hypoglycemia
manufacturer	manufacturer
product	product
symptom	symptom
warning	warning
	
death	death
diabetic	diabetic
finding	finding
report	report
study	study
	
tranquilizer	tranquilizer
campaign	campaign
law	law
medicine	medicine
prescription	prescription
print	print
publicity	publicity
sedative	sedative
	
television	television
use	use
analyst	analyst
profit	profit
state	state
	
dose	dose
appeal	appeal
death	death
impact	impact
injury	injury
liability	liability
manufacturer	manufacturer
miscarriage	miscarriage
refusing	refusing
ruling	ruling
	
diethylstilbestrol	diethylstilbestrol
hormone	hormone
damage	damage
effect	effect
female	female
prospect	prospect
state	state
	
The	the
'Narcotic'	'narcotic'
Sense	sense
	
consumer	consumer
distributor	distributor
effort	effort
cessation	cessation
consumption	consumption
country	country
reduction	reduction
requirement	requirement
	
victory	victory
battle	battle
capacity	capacity
cartel	cartel
government	government
mafia	mafia
newspaper	newspaper
people	people
	
mafia	mafia
terrorism	terrorism
censorship	censorship
dictatorship	dictatorship
newspaper	newspaper
press	press
brother	brother
nothing	nothing
aspiration	aspiration
	
assassination	assassination
editor	editor
leader	leader
politics	politics
rise	rise
action	action
country	country
doubt	doubt
freedom	freedom
	
mafioso	mafioso
medium	medium
menace	menace
solidarity	solidarity
structure	structure
trade	trade
world	world
	
terrorism	terrorism
censorship	censorship
doubt	doubt
freedom	freedom
mafia	mafia
medium	medium
menace	menace
newspaper	newspaper
	
press	press
solidarity	solidarity
structure	structure
	
murder	murder
capital-punishment	capital-punishment
symbolism	symbolism
trafficking	trafficking
furor	furor
killing	killing
substance	substance
crime	crime
	
restaurant	restaurant
law	law
bill	bill
case	case
problem	problem
	
menace	menace
terrorism	terrorism
freedom	freedom
solidarity	solidarity
structure	structure
medium	medium
press	press
censorship	censorship
country	country
doubt	doubt
	
mafia	mafia
newspaper	newspaper
way	way
attack	attack
government	government
magnitude	magnitude
people	people
relation	relation
threat	threat
	
world	world
	
trafficking	trafficking
crime	crime
capital-punishment	capital-punishment
furor	furor
killing	killing
murder	murder
restaurant	restaurant
substance	substance
symbolism	symbolism
	
dictatorship	dictatorship
aspiration	aspiration
brother	brother
editor	editor
mafia	mafia
nothing	nothing
politics	politics
press	press
	
assassination	assassination
censorship	censorship
leader	leader
newspaper	newspaper
rise	rise
terrorism	terrorism
	
assassination	assassination
brother	brother
censorship	censorship
dictatorship	dictatorship
mafia	mafia
nothing	nothing
press	press
terrorism	terrorism
	
aspiration	aspiration
editor	editor
leader	leader
newspaper	newspaper
politics	politics
rise	rise
	
laundering	laundering
army	army
lot	lot
money	money
arsenal	arsenal
baron	baron
economy	economy
explosive	explosive
government	government
hand	hand
	
material	material
military	military
none	none
opinion	opinion
portion	portion
talk	talk
	
censorship	censorship
mafia	mafia
newspaper	newspaper
press	press
terrorism	terrorism
country	country
doubt	doubt
freedom	freedom
	
medium	medium
menace	menace
solidarity	solidarity
structure	structure
	
50	50
	
98	98
	
x	x
<	<
	
96	96
	
94	94
	
92	92
	
90	90
	
88	88
	
86	86
	
84	84
	
82	82
	
80	80
	
78	78
~	~
	
!	!
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
2	2
3	3
4	4
5	5
6	6
7	7
8	8
9	9
	
Figure	figure
2	2
	
The	the
drug	drug
experiment	experiment
;	;
the	the
change	change
in	in
the	the
disambiguation	disambiguation
performance	performance
with	with
iteration	iteration
number	number
is	is
	
plotted	plotted
separately	separately
for	for
each	each
sense	sense
(	(
the	the
asterisk	asterisk
marks	marks
the	the
plot	plot
of	of
the	the
success	success
rate	rate
for	for
the	the
	
'narcotic'	'narcotic'
sense	sense
;	;
the	the
other	other
two	two
plots	plots
are	are
the	the
'medicine'	'medicine'
sense	sense
and	and
the	the
weighted	weighted
average	average
of	of
the	the
	
two	two
senses	senses
)	)
In	in
our	our
experiments	experiments
the	the
typical	typical
number	number
of	of
iterations	iterations
was	was
3	3
	
Similarity	similarity
of	of
sense1	sense1
examples	examples
to	to
sense1	sense1
feedback	feedback
set	set
	
0	0
	
~o	~o
	
~o	~o
	
O	o
	
Example	example
#	#
	
Figure	figure
3	3
	
0	0
0	0
	
Iteration	iteration
#	#
	
10	10
	
The	the
drug	drug
experiment	experiment
;	;
an	an
illustration	illustration
of	of
the	the
development	development
of	of
the	the
support	support
for	for
a	a
particular	particular
sense	sense
	
with	with
iteration	iteration
The	the
plot	plot
shows	shows
the	the
similarity	similarity
of	of
a	a
number	number
of	of
drug	drug
sentences	sentences
to	to
the	the
'narcotic'	'narcotic'
	
sense	sense
To	to
facilitate	facilitate
visualization	visualization
the	the
curves	curves
are	are
sorted	sorted
by	by
the	the
second-iteration	second-iteration
values	values
of	of
	
similarity	similarity
	
4	4
Discussion	discussion
	
We	we
now	now
discuss	discuss
in	in
some	some
detail	detail
the	the
choices	choices
made	made
at	at
the	the
different	different
stages	stages
of	of
the	the
development	development
	
of	of
the	the
present	present
method	method
and	and
its	its
relationship	relationship
to	to
some	some
of	of
the	the
previous	previous
works	works
on	on
	
word	word
sense	sense
disambiguation	disambiguation
	
51	51
	
1	1
t_	t_
	
0	0
9	9
	
0	0
8	8
	
0	0
7	7
	
0	0
6	6
	
0	0
5	5
	
0	0
4	4
	
0	0
3	3
	
0	0
2	2
	
0	0
1	1
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
i	i
i	i
i	i
i	i
i	i
i	i
i	i
t	t
	
2	2
3	3
4	4
5	5
6	6
7	7
8	8
9	9
10	10
	
Figure	figure
4	4
	
The	the
drug	drug
experiment	experiment
;	;
the	the
similarity	similarity
of	of
a	a
'narcotic'-sense	'narcotic'-sense
example	example
to	to
each	each
of	of
the	the
two	two
senses	senses
The	the
	
sentence	sentence
here	here
was	was
The	the
American	american
people	people
and	and
their	their
government	government
also	also
woke	woke
up	up
too	too
late	late
to	to
the	the
menace	menace
drugs	drugs
	
posed	posed
to	to
the	the
moral	moral
structure	structure
of	of
their	their
country	country
The	the
asterisk	asterisk
marks	marks
the	the
plot	plot
for	for
the	the
'narcotic'	'narcotic'
sense	sense
	
4	4
1	1
Flexible	flexible
Sense	sense
Distinctions	distinctions
	
The	the
possibility	possibility
of	of
strict	strict
definition	definition
of	of
each	each
sense	sense
of	of
a	a
polysemous	polysemous
word	word
and	and
the	the
possibility	possibility
	
of	of
unambiguous	unambiguous
assignment	assignment
of	of
a	a
given	given
sense	sense
in	in
a	a
given	given
situation	situation
are	are
in	in
themselves	themselves
	
nontrivial	nontrivial
issues	issues
in	in
philosophy	philosophy
(	(
Quine	quine
1960	1960
)	)
and	and
linguistics	linguistics
(	(
Weinreich	weinreich
1980	1980
;	;
Cruse	cruse
	
1986	1986
)	)
Different	different
dictionaries	dictionaries
often	often
disagree	disagree
on	on
the	the
definitions	definitions
;	;
the	the
split	split
into	into
senses	senses
may	may
	
also	also
depend	depend
on	on
the	the
task	task
at	at
hand	hand
Thus	thus
it	it
is	is
important	important
to	to
maintain	maintain
the	the
possibility	possibility
of	of
	
flexible	flexible
distinction	distinction
of	of
the	the
different	different
senses	senses
e	e
g	g
 by	 by
letting	letting
this	this
distinction	distinction
be	be
determined	determined
	
by	by
an	an
external	external
knowledge	knowledge
source	source
such	such
as	as
a	a
thesaurus	thesaurus
or	or
a	a
dictionary	dictionary
Although	although
this	this
	
requirement	requirement
may	may
seem	seem
trivial	trivial
most	most
corpus-based	corpus-based
methods	methods
do	do
not	not
in	in
fact	fact
allow	allow
such	such
	
flexibility	flexibility
For	for
example	example
defining	defining
the	the
senses	senses
by	by
the	the
possible	possible
translations	translations
of	of
the	the
word	word
	
(	(
Dagan	dagan
Itai	itai
and	and
Schwall	schwall
1991	1991
;	;
Brown	brown
et	et
al	al
1991	1991
;	;
Gale	gale
Church	church
and	and
Yarowsky	yarowsky
1992	1992
)	)
	
by	by
the	the
Roget's	roget's
categories	categories
(	(
Yarowsky	yarowsky
1992	1992
)	)
or	or
by	by
clustering	clustering
(	(
Schi	schi
~tze	~tze
1992	1992
)	)
yields	yields
a	a
	
grouping	grouping
that	that
does	does
not	not
always	always
conform	conform
to	to
the	the
desired	desired
sense	sense
distinctions	distinctions
	
In	in
comparison	comparison
to	to
these	these
approaches	approaches
our	our
reliance	reliance
on	on
the	the
MRD	mrd
for	for
the	the
definition	definition
of	of
	
senses	senses
in	in
the	the
initialization	initialization
of	of
the	the
learning	learning
process	process
guarantees	guarantees
the	the
required	required
flexibility	flexibility
	
in	in
setting	setting
the	the
sense	sense
distinctions	distinctions
Specifically	specifically
the	the
user	user
of	of
our	our
system	system
may	may
choose	choose
a	a
	
certain	certain
dictionary	dictionary
definition	definition
a	a
combination	combination
of	of
definitions	definitions
from	from
several	several
dictionaries	dictionaries
	
or	or
manually	manually
listed	listed
seed	seed
words	words
for	for
every	every
sense	sense
that	that
needs	needs
to	to
be	be
defined	defined
Whereas	whereas
	
pure	pure
MRD-based	mrd-based
methods	methods
allow	allow
the	the
same	same
flexibility	flexibility
their	their
potential	potential
so	so
far	far
has	has
not	not
	
been	been
fully	fully
tapped	tapped
because	because
definitions	definitions
alone	alone
do	do
not	not
contain	contain
enough	enough
information	information
for	for
	
disambiguation	disambiguation
	
4	4
2	2
Sentence	sentence
Features	features
	
Different	different
polysemous	polysemous
words	words
may	may
benefit	benefit
from	from
different	different
types	types
of	of
features	features
of	of
the	the
context	context
	
sentences	sentences
Polysemous	polysemous
words	words
for	for
which	which
distinct	distinct
senses	senses
tend	tend
to	to
appear	appear
in	in
different	different
topics	topics
	
can	can
be	be
disambiguated	disambiguated
using	using
single	single
words	words
as	as
the	the
context	context
features	features
as	as
we	we
did	did
here	here
	
Disambiguation	disambiguation
of	of
other	other
polysemous	polysemous
words	words
may	may
require	require
taking	taking
the	the
sentence	sentence
structure	structure
	
into	into
account	account
using	using
n-grams	n-grams
or	or
syntactic	syntactic
constructs	constructs
as	as
features	features
This	this
additional	additional
information	information
	
can	can
be	be
incorporated	incorporated
into	into
our	our
method	method
by	by
(	(
1	1
)	)
extracting	extracting
features	features
such	such
as	as
nouns	nouns
	
!52	!52
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
verbs	verbs
adjectives	adjectives
of	of
the	the
target	target
word	word
bigrams	bigrams
trigrams	trigrams
and	and
subject-verb	subject-verb
or	or
verb-object	verb-object
	
pairs	pairs
(	(
2	2
)	)
discarding	discarding
features	features
with	with
a	a
low	low
weight	weight
(	(
cf	cf
Section	section
A	a
3	3
of	of
the	the
appendix	appendix
)	)
and	and
	
(	(
3	3
)	)
using	using
the	the
remaining	remaining
features	features
instead	instead
of	of
single	single
words	words
(	(
i	i
e	e
 by	 by
representing	representing
a	a
sentence	sentence
	
by	by
the	the
set	set
of	of
significant	significant
features	features
it	it
contains	contains
and	and
a	a
feature	feature
by	by
the	the
set	set
of	of
sentences	sentences
	
in	in
which	which
it	it
appears	appears
)	)
	
4	4
3	3
Using	using
WordNet	wordnet
	
The	the
initialization	initialization
of	of
the	the
word	word
similarity	similarity
matrix	matrix
using	using
WordNet	wordnet
a	a
hand-crafted	hand-crafted
semantic	semantic
	
network	network
arranged	arranged
in	in
a	a
hierarchical	hierarchical
structure	structure
(	(
Miller	miller
et	et
al	al
1993	1993
)	)
may	may
seem	seem
to	to
	
be	be
advantageous	advantageous
over	over
simply	simply
setting	setting
it	it
to	to
the	the
identity	identity
matrix	matrix
as	as
we	we
have	have
done	done
To	to
	
compare	compare
these	these
two	two
approaches	approaches
we	we
tried	tried
to	to
set	set
the	the
initial	initial
(	(
dis	dis
)	)
similarity	similarity
between	between
two	two
	
words	words
to	to
the	the
WordNet	wordnet
path	path
length	length
between	between
their	their
nodes	nodes
(	(
Lee	lee
Kim	kim
and	and
Lee	lee
1993	1993
)	)
and	and
	
then	then
learn	learn
the	the
similarity	similarity
values	values
iteratively	iteratively
This	this
however	however
led	led
to	to
worse	worse
performance	performance
	
than	than
the	the
simple	simple
identity-matrix	identity-matrix
initialization	initialization
	
There	there
are	are
several	several
possible	possible
reasons	reasons
for	for
the	the
poor	poor
performance	performance
of	of
WordNet	wordnet
in	in
this	this
	
comparison	comparison
First	first
WordNet	wordnet
is	is
not	not
designed	designed
to	to
capture	capture
contextual	contextual
similarity	similarity
For	for
example	example
	
in	in
WordNet	wordnet
hospital	hospital
and	and
doctor	doctor
have	have
no	no
common	common
ancestor	ancestor
and	and
hence	hence
their	their
	
similarity	similarity
is	is
0	0
while	while
doctor	doctor
and	and
lawyer	lawyer
are	are
quite	quite
similar	similar
because	because
both	both
designate	designate
professionals	professionals
	
humans	humans
and	and
living	living
things	things
Note	note
that	that
contextually	contextually
doctor	doctor
should	should
be	be
more	more
	
similar	similar
to	to
hospital	hospital
than	than
to	to
lawyer	lawyer
Second	second
we	we
found	found
that	that
the	the
WordNet	wordnet
similarity	similarity
values	values
	
dominated	dominated
the	the
contextual	contextual
similarity	similarity
computed	computed
in	in
the	the
iterative	iterative
process	process
preventing	preventing
the	the
	
transitive	transitive
effects	effects
of	of
contextual	contextual
similarity	similarity
from	from
taking	taking
over	over
Third	third
the	the
tree	tree
distance	distance
in	in
	
itself	itself
does	does
not	not
always	always
correspond	correspond
to	to
the	the
intuitive	intuitive
notion	notion
of	of
similarity	similarity
because	because
different	different
	
concepts	concepts
appear	appear
at	at
different	different
levels	levels
of	of
abstraction	abstraction
and	and
have	have
a	a
different	different
number	number
of	of
	
nested	nested
subconcepts	subconcepts
For	for
example	example
a	a
certain	certain
distance	distance
between	between
two	two
nodes	nodes
may	may
result	result
	
from	from
(	(
1	1
)	)
the	the
nodes	nodes
being	being
semantically	semantically
close	close
but	but
separated	separated
by	by
a	a
large	large
distance	distance
stemming	stemming
	
from	from
a	a
high	high
level	level
of	of
detail	detail
in	in
the	the
related	related
synsets	synsets
or	or
from	from
(	(
2	2
)	)
the	the
nodes	nodes
being	being
	
semantically	semantically
far	far
from	from
each	each
other	other
6	6
	
4	4
4	4
Ignoring	ignoring
Irrelevant	irrelevant
Examples	examples
	
The	the
feedback	feedback
sets	sets
we	we
use	use
in	in
training	training
the	the
system	system
may	may
contain	contain
noise	noise
in	in
the	the
form	form
of	of
	
irrelevant	irrelevant
examples	examples
that	that
are	are
collected	collected
along	along
with	with
the	the
relevant	relevant
and	and
useful	useful
ones	ones
For	for
	
instance	instance
in	in
one	one
of	of
the	the
definitions	definitions
of	of
bank	bank
in	in
WordNet	wordnet
we	we
find	find
bar	bar
which	which
in	in
turn	turn
	
has	has
many	many
other	other
senses	senses
that	that
are	are
not	not
related	related
to	to
bank	bank
Although	although
these	these
unrelated	unrelated
senses	senses
	
contribute	contribute
examples	examples
to	to
the	the
feedback	feedback
set	set
our	our
system	system
is	is
hardly	hardly
affected	affected
by	by
this	this
noise	noise
	
because	because
we	we
do	do
not	not
collect	collect
statistics	statistics
on	on
the	the
feedback	feedback
sets	sets
(	(
i	i
e	e
 our	 our
method	method
is	is
not	not
based	based
	
on	on
mere	mere
co-occurrence	co-occurrence
frequencies	frequencies
as	as
most	most
other	other
corpus-based	corpus-based
methods	methods
are	are
)	)
The	the
	
relevant	relevant
examples	examples
in	in
the	the
feedback	feedback
set	set
of	of
the	the
sense	sense
si	si
will	will
attract	attract
the	the
examples	examples
of	of
si	si
;	;
	
the	the
irrelevant	irrelevant
examples	examples
will	will
not	not
attract	attract
the	the
examples	examples
of	of
si	si
but	but
neither	neither
will	will
they	they
do	do
	
damage	damage
because	because
they	they
are	are
not	not
expected	expected
to	to
attract	attract
examples	examples
of	of
sj	sj
~	~
~	~
i	i
)	)
	
4	4
5	5
Related	related
work	work
	
4	4
5	5
1	1
The	the
Knowledge	knowledge
Acquisition	acquisition
Bottleneck	bottleneck
Brown	brown
et	et
al	al
(	(
1991	1991
)	)
and	and
Gale	gale
Church	church
	
and	and
Yarowsky	yarowsky
(	(
1992	1992
)	)
used	used
the	the
translations	translations
of	of
ambiguous	ambiguous
words	words
in	in
a	a
bilingual	bilingual
corpus	corpus
	
as	as
sense	sense
tags	tags
This	this
does	does
not	not
obviate	obviate
the	the
need	need
for	for
manual	manual
work	work
as	as
producing	producing
bilingual	bilingual
	
corpora	corpora
requires	requires
manual	manual
translation	translation
work	work
Dagan	dagan
Itai	itai
and	and
Schwall	schwall
(	(
1991	1991
)	)
used	used
a	a
	
bilingual	bilingual
lexicon	lexicon
and	and
a	a
monolingual	monolingual
corpus	corpus
to	to
save	save
the	the
need	need
for	for
translating	translating
the	the
corpus	corpus
	
6	6
Resnik	resnik
(	(
1995	1995
)	)
recently	recently
suggested	suggested
that	that
this	this
particular	particular
difficulty	difficulty
can	can
be	be
overcome	overcome
by	by
a	a
different	different
measure	measure
	
that	that
takes	takes
into	into
account	account
the	the
informativeness	informativeness
of	of
the	the
most	most
specific	specific
common	common
ancestor	ancestor
of	of
the	the
two	two
words	words
	
53	53
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
The	the
problem	problem
remains	remains
however	however
that	that
the	the
word	word
translations	translations
do	do
not	not
necessarily	necessarily
overlap	overlap
	
with	with
the	the
desired	desired
sense	sense
distinctions	distinctions
	
Sch	sch
/	/
itze	itze
(	(
1992	1992
)	)
clustered	clustered
the	the
examples	examples
in	in
the	the
training	training
set	set
and	and
manually	manually
assigned	assigned
	
each	each
cluster	cluster
a	a
sense	sense
by	by
observing	observing
10-20	10-20
members	members
of	of
the	the
cluster	cluster
Each	each
sense	sense
was	was
usually	usually
	
represented	represented
by	by
several	several
clusters	clusters
Although	although
this	this
approach	approach
significantly	significantly
decreased	decreased
the	the
	
need	need
for	for
manual	manual
intervention	intervention
about	about
a	a
hundred	hundred
examples	examples
had	had
still	still
to	to
be	be
tagged	tagged
manually	manually
	
for	for
each	each
word	word
Moreover	moreover
the	the
resulting	resulting
clusters	clusters
did	did
not	not
necessarily	necessarily
correspond	correspond
	
to	to
the	the
desired	desired
sense	sense
distinctions	distinctions
	
Yarowsky	yarowsky
(	(
1992	1992
)	)
learned	learned
discriminators	discriminators
for	for
each	each
Roget's	roget's
category	category
saving	saving
the	the
need	need
	
to	to
separate	separate
the	the
training	training
set	set
into	into
senses	senses
However	however
using	using
such	such
hand-crafted	hand-crafted
categories	categories
	
usually	usually
leads	leads
to	to
a	a
coverage	coverage
problem	problem
for	for
specific	specific
domains	domains
or	or
for	for
domains	domains
other	other
than	than
	
the	the
one	one
for	for
which	which
the	the
list	list
of	of
categories	categories
has	has
been	been
prepared	prepared
	
Using	using
MRD's	mrd's
(	(
Amsler	amsler
1984	1984
)	)
for	for
word	word
sense	sense
disambiguation	disambiguation
was	was
popularized	popularized
by	by
	
Lesk	lesk
(	(
1986	1986
)	)
;	;
several	several
researchers	researchers
subsequently	subsequently
continued	continued
and	and
improved	improved
this	this
line	line
of	of
	
work	work
(	(
Krovetz	krovetz
and	and
Croft	croft
1989	1989
;	;
Guthrie	guthrie
et	et
al	al
1991	1991
;	;
V	v
~ronis	~ronis
and	and
Ide	ide
1990	1990
)	)
Unlike	unlike
the	the
	
information	information
in	in
a	a
corpus	corpus
the	the
information	information
in	in
the	the
MRD	mrd
definitions	definitions
is	is
presorted	presorted
into	into
	
senses	senses
However	however
as	as
noted	noted
above	above
the	the
MRD	mrd
definitions	definitions
alone	alone
do	do
not	not
contain	contain
enough	enough
	
information	information
to	to
allow	allow
reliable	reliable
disambiguation	disambiguation
Recently	recently
Yarowsky	yarowsky
(	(
1995	1995
)	)
combined	combined
an	an
	
MRD	mrd
and	and
a	a
corpus	corpus
in	in
a	a
bootstrapping	bootstrapping
process	process
In	in
that	that
work	work
the	the
definition	definition
words	words
	
were	were
used	used
as	as
initial	initial
sense	sense
indicators	indicators
automatically	automatically
tagging	tagging
the	the
target	target
word	word
examples	examples
	
containing	containing
them	them
These	these
tagged	tagged
examples	examples
were	were
then	then
used	used
as	as
seed	seed
examples	examples
in	in
the	the
	
bootstrapping	bootstrapping
process	process
In	in
comparison	comparison
we	we
suggest	suggest
to	to
further	further
combine	combine
the	the
corpus	corpus
and	and
	
the	the
MRD	mrd
by	by
using	using
all	all
the	the
corpus	corpus
examples	examples
of	of
the	the
MRD	mrd
definition	definition
words	words
instead	instead
of	of
	
those	those
words	words
alone	alone
This	this
yields	yields
much	much
more	more
sense-presorted	sense-presorted
training	training
information	information
	
4	4
5	5
2	2
The	the
Problem	problem
of	of
Sparse	sparse
Data	data
Most	most
previous	previous
works	works
define	define
word	word
similarity	similarity
based	based
	
on	on
co-occurrence	co-occurrence
information	information
and	and
hence	hence
face	face
a	a
severe	severe
problem	problem
of	of
sparse	sparse
data	data
Many	many
	
of	of
the	the
possible	possible
co-occurrences	co-occurrences
are	are
not	not
observed	observed
even	even
in	in
a	a
very	very
large	large
corpus	corpus
(	(
Church	church
	
and	and
Mercer	mercer
1993	1993
)	)
Our	our
algorithm	algorithm
addresses	addresses
this	this
problem	problem
in	in
two	two
ways	ways
First	first
we	we
replace	replace
	
the	the
all-or-none	all-or-none
indicator	indicator
of	of
co-occurrence	co-occurrence
by	by
a	a
graded	graded
measure	measure
of	of
contextual	contextual
similarity	similarity
	
Our	our
measure	measure
of	of
similarity	similarity
is	is
transitive	transitive
allowing	allowing
two	two
words	words
to	to
be	be
considered	considered
similar	similar
	
even	even
if	if
they	they
neither	neither
appear	appear
in	in
the	the
same	same
sentence	sentence
nor	nor
share	share
neighbor	neighbor
words	words
Second	second
	
we	we
extend	extend
the	the
training	training
set	set
by	by
adding	adding
examples	examples
of	of
related	related
words	words
The	the
performance	performance
of	of
	
our	our
system	system
compares	compares
favorably	favorably
to	to
that	that
of	of
systems	systems
trained	trained
on	on
sets	sets
larger	larger
by	by
a	a
factor	factor
	
of	of
100	100
(	(
the	the
results	results
described	described
in	in
Section	section
3	3
were	were
obtained	obtained
following	following
learning	learning
from	from
several	several
	
dozen	dozen
examples	examples
in	in
comparison	comparison
to	to
thousands	thousands
of	of
examples	examples
in	in
other	other
automatic	automatic
methods	methods
)	)
	
Traditionally	traditionally
the	the
problem	problem
of	of
sparse	sparse
data	data
is	is
approached	approached
by	by
estimating	estimating
the	the
probability	probability
	
of	of
unobserved	unobserved
co-occurrences	co-occurrences
using	using
the	the
actual	actual
co-occurrences	co-occurrences
in	in
the	the
training	training
	
set	set
This	this
can	can
be	be
done	done
by	by
smoothing	smoothing
the	the
observed	observed
frequencies	frequencies
7	7
(	(
Church	church
and	and
Mercer	mercer
	
1993	1993
)	)
or	or
by	by
class-based	class-based
methods	methods
(	(
Brown	brown
et	et
al	al
1991	1991
;	;
Pereira	pereira
and	and
Tishby	tishby
1992	1992
;	;
Pereira	pereira
	
Tishby	tishby
and	and
Lee	lee
1993	1993
;	;
Hirschman	hirschman
1986	1986
;	;
Resnik	resnik
1992	1992
;	;
Brill	brill
et	et
al	al
1990	1990
;	;
Dagan	dagan
Marcus	marcus
	
and	and
Markovitch	markovitch
1993	1993
)	)
In	in
comparison	comparison
to	to
these	these
approaches	approaches
we	we
use	use
similarity	similarity
information	information
	
throughout	throughout
training	training
and	and
not	not
merely	merely
for	for
estimating	estimating
co-occurrence	co-occurrence
statistics	statistics
This	this
	
allows	allows
the	the
system	system
to	to
learn	learn
successfully	successfully
from	from
very	very
sparse	sparse
data	data
	
7	7
Smoothing	smoothing
is	is
a	a
technique	technique
widely	widely
used	used
in	in
applications	applications
such	such
as	as
statistical	statistical
pattern	pattern
recognition	recognition
and	and
	
probabilistic	probabilistic
language	language
modeling	modeling
that	that
require	require
a	a
probability	probability
density	density
to	to
be	be
estimated	estimated
from	from
data	data
For	for
	
sparse	sparse
data	data
this	this
estimation	estimation
problem	problem
is	is
severely	severely
underconstrained	underconstrained
and	and
thus	thus
ill-posed	ill-posed
;	;
smoothing	smoothing
	
regularizes	regularizes
the	the
problem	problem
by	by
adopting	adopting
a	a
prior	prior
constraint	constraint
that	that
assumes	assumes
that	that
the	the
probability	probability
density	density
does	does
	
not	not
change	change
too	too
fast	fast
in	in
between	between
the	the
examples	examples
	
54	54
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
4	4
6	6
Summary	summary
	
We	we
have	have
described	described
an	an
approach	approach
to	to
WSD	wsd
that	that
combines	combines
a	a
corpus	corpus
and	and
an	an
MRD	mrd
to	to
generate	generate
	
an	an
extensive	extensive
data	data
set	set
for	for
learning	learning
similarity-based	similarity-based
disambiguation	disambiguation
Our	our
system	system
	
combines	combines
the	the
advantages	advantages
of	of
corpus-based	corpus-based
approaches	approaches
(	(
large	large
number	number
of	of
examples	examples
)	)
with	with
	
those	those
of	of
the	the
MRD-based	mrd-based
approaches	approaches
(	(
data	data
presorted	presorted
by	by
senses	senses
)	)
by	by
using	using
the	the
MRD	mrd
definitions	definitions
	
to	to
direct	direct
the	the
extraction	extraction
of	of
training	training
information	information
(	(
in	in
the	the
form	form
of	of
feedback	feedback
sets	sets
)	)
	
from	from
the	the
corpus	corpus
	
In	in
our	our
system	system
a	a
word	word
is	is
represented	represented
by	by
the	the
set	set
of	of
sentences	sentences
in	in
which	which
it	it
appears	appears
	
Accordingly	accordingly
words	words
are	are
considered	considered
similar	similar
if	if
they	they
appear	appear
in	in
similar	similar
sentences	sentences
and	and
sentences	sentences
	
are	are
considered	considered
similar	similar
if	if
they	they
contain	contain
similar	similar
words	words
Applying	applying
this	this
definition	definition
	
iteratively	iteratively
yields	yields
a	a
transitive	transitive
measure	measure
of	of
similarity	similarity
under	under
which	which
two	two
sentences	sentences
may	may
be	be
	
considered	considered
similar	similar
even	even
if	if
they	they
do	do
not	not
share	share
any	any
word	word
and	and
two	two
words	words
may	may
be	be
considered	considered
	
similar	similar
even	even
if	if
they	they
do	do
not	not
share	share
neighbor	neighbor
words	words
Our	our
experiments	experiments
show	show
that	that
the	the
	
resulting	resulting
alternative	alternative
to	to
raw	raw
co-occurrence-based	co-occurrence-based
similarity	similarity
leads	leads
to	to
better	better
performance	performance
	
on	on
very	very
sparse	sparse
data	data
	
Appendix	appendix
	
A	a
1	1
Stopping	stopping
Conditions	conditions
of	of
the	the
Iterative	iterative
Algorithm	algorithm
	
Let	let
fi	fi
be	be
the	the
increase	increase
in	in
the	the
similarity	similarity
value	value
in	in
iteration	iteration
i	i
:	:
	
f	f
~	~
(	(
X	x
y	y
)	)
=	=
simi	simi
(	(
X	x
y	y
)	)
-	-
simi_l	simi_l
(	(
X	x
32	32
)	)
(	(
9	9
)	)
	
where	where
X	x
y	y
can	can
be	be
either	either
words	words
or	or
sentences	sentences
For	for
each	each
item	item
X	x
the	the
algorithm	algorithm
stops	stops
	
updating	updating
its	its
similarity	similarity
values	values
to	to
other	other
items	items
(	(
that	that
is	is
updating	updating
its	its
row	row
in	in
the	the
similarity	similarity
	
matrix	matrix
)	)
in	in
the	the
first	first
iteration	iteration
that	that
satisfies	satisfies
maxyfi	maxyfi
(	(
X	x
3	3
;	;
)	)
_	_
<	<
Â	Â
¢	¢
where	where
c	c
>	>
0	0
is	is
a	a
preset	preset
	
threshold	threshold
	
According	according
to	to
this	this
stopping	stopping
condition	condition
the	the
algorithm	algorithm
terminates	terminates
after	after
at	at
most	most
	
iterations	iterations
(	(
otherwise	otherwise
in	in
!	!
iterations	iterations
with	with
eachfi	eachfi
>	>
c	c
we	we
obtain	obtain
sim	sim
(	(
V	v
3	3
;	;
)	)
>	>
Â	Â
¢-	¢-
~	~
=	=
1	1
	
in	in
contradiction	contradiction
to	to
upper	upper
bound	bound
of	of
1	1
on	on
the	the
similarity	similarity
values	values
;	;
see	see
Section	section
A	a
2	2
below	below
)	)
	
We	we
found	found
that	that
the	the
best	best
results	results
are	are
obtained	obtained
within	within
three	three
iterations	iterations
After	after
that	that
	
the	the
disambiguation	disambiguation
results	results
tend	tend
not	not
to	to
change	change
significantly	significantly
although	although
the	the
similarity	similarity
	
values	values
may	may
continue	continue
to	to
increase	increase
Intuitively	intuitively
the	the
transitive	transitive
exploration	exploration
of	of
similarities	similarities
	
is	is
exhausted	exhausted
after	after
three	three
iterations	iterations
	
A	a
2	2
Proofs	proofs
	
In	in
the	the
following	following
X	x
3	3
;	;
can	can
be	be
either	either
words	words
or	or
sentences	sentences
	
Theorem	theorem
1	1
	
Similarity	similarity
is	is
bounded	bounded
:	:
simn	simn
(	(
X	x
3	3
;	;
)	)
_	_
<	<
1	1
	
Proof	proof
	
By	by
induction	induction
on	on
the	the
number	number
of	of
iteration	iteration
At	at
the	the
first	first
iteration	iteration
sim0	sim0
(	(
X	x
3	3
;	;
)	)
_	_
<	<
1	1
by	by
	
initialization	initialization
Assume	assume
that	that
the	the
claim	claim
holds	holds
for	for
n	n
and	and
prove	prove
for	for
n	n
+	+
1	1
:	:
	
simn	simn
+l	+l
(	(
X	x
Y	y
)	)
=	=
E	e
weight	weight
(	(
K	k
X	x
)	)
maxsimn	maxsimn
(	(
Xj	xj
Yk	yk
)	)
	
~CX	~cx
YkEY	ykey
	
<	<
~	~
weight	weight
(	(
~	~
X	x
)	)
â	â
€	€
¢	¢
1	1
(	(
by	by
the	the
induction	induction
hypothesis	hypothesis
)	)
	
--	--
1	1
	
55	55
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
Theorem	theorem
2	2
	
Similarity	similarity
is	is
reflexive	reflexive
:	:
VX	vx
sim	sim
(	(
X	x
X	x
)	)
=	=
1	1
	
Proof	proof
	
By	by
induction	induction
on	on
the	the
number	number
of	of
iteration	iteration
sim0	sim0
(	(
X	x
X	x
)	)
=	=
1	1
by	by
initialization	initialization
Assume	assume
	
that	that
the	the
claim	claim
holds	holds
for	for
n	n
and	and
prove	prove
for	for
n	n
+	+
1	1
:	:
	
Simn	simn
+l	+l
(	(
X	x
X	x
)	)
=	=
E	e
weight	weight
(	(
~	~
X	x
)	)
max	max
simn	simn
(	(
Xi	xi
,	,
~	~
)	)
	
x	x
~cx	~cx
~cx	~cx
	
>	>
-	-
E	e
weight	weight
(	(
X	x
/	/
X	x
)	)
simn	simn
(	(
Xi	xi
Xi	xi
)	)
	
XiE	xie
X	x
	
=	=
~	~
weight	weight
(	(
Xi	xi
X	x
)	)
â	â
€	€
¢	¢
1	1
(	(
by	by
the	the
induction	induction
hypothesis	hypothesis
)	)
	
XiEX	xiex
	
=	=
1	1
	
Thus	thus
simn	simn
+l	+l
(	(
V	v
X	x
)	)
_	_
>	>
1	1
By	by
theorem	theorem
1	1
simn	simn
+l	+l
(	(
X	x
X	x
)	)
_	_
<	<
1	1
so	so
simn	simn
+l	+l
(	(
X	x
X	x
)	)
=	=
1	1
	
Theorem	theorem
3	3
	
Similarity	similarity
sim	sim
(	(
X	x
3	3
;	;
)	)
is	is
a	a
nondecreasing	nondecreasing
function	function
of	of
the	the
number	number
of	of
iteration	iteration
n	n
	
Proof	proof
	
By	by
induction	induction
on	on
the	the
number	number
of	of
iteration	iteration
Consider	consider
the	the
case	case
of	of
n	n
=	=
1	1
:	:
siml	siml
(	(
X	x
Y	y
)	)
>	>
	
sim0	sim0
(	(
X	x
3	3
;	;
)	)
(	(
if	if
sim0	sim0
(	(
X	x
y	y
)	)
=	=
1	1
then	then
X	x
=	=
3	3
;	;
and	and
siml	siml
(	(
X	x
3	3
;	;
)	)
=	=
1	1
as	as
well	well
;	;
else	else
	
sim0	sim0
(	(
X	x
3	3
;	;
)	)
=	=
0	0
and	and
siml	siml
(	(
X	x
3	3
;	;
)	)
>	>
_	_
0	0
~-	~-
sim0	sim0
(	(
X	x
3	3
;	;
)	)
)	)
Now	now
assume	assume
that	that
the	the
claim	claim
	
holds	holds
for	for
n	n
and	and
prove	prove
for	for
n	n
+	+
1	1
:	:
	
simn	simn
+l	+l
(	(
X	x
Y	y
)	)
-	-
siren	siren
(	(
X	x
Y	y
)	)
=	=
	
=	=
~	~
weight	weight
(	(
K	k
X	x
)	)
maxsim	maxsim
(	(
Xj	xj
Yk	yk
)	)
	
X	x
/	/
~X	~x
YkEY	ykey
	
-	-
E	e
weight	weight
(	(
Xj	xj
A'	a'
)	)
maxsim	maxsim
_l	_l
(	(
Xj	xj
Yt	yt
)	)
	
x	x
/ex	/ex
NkEY	nkey
	
>	>
~	~
weight	weight
(	(
Xj	xj
X	x
)	)
(	(
maxsimn	maxsimn
(	(
Xi	xi
Yk	yk
)	)
-	-
maxsim	maxsim
~_l	~_l
(	(
~	~
Yk	yk
)	)
)	)
	
~cx	~cx
\YkEY	\ykey
YkEY	ykey
	
>	>
0	0
	
The	the
last	last
inequality	inequality
holds	holds
because	because
by	by
the	the
induction	induction
hypothesis	hypothesis
	
V	v
~	~
Yk	yk
simn	simn
(	(
~	~
Yk	yk
)	)
_	_
>	>
simn_l	simn_l
(	(
Xj	xj
Yk	yk
)	)
	
maxsimn	maxsimn
(	(
Xj	xj
Yk	yk
)	)
>	>
_	_
maxsim	maxsim
_l	_l
(	(
Xj	xj
Yk	yk
)	)
	
YkEY	ykey
YkEY	ykey
	
maxsimn	maxsimn
(	(
Xj	xj
Yk	yk
)	)
-	-
maxsimn_l	maxsimn_l
(	(
Xj	xj
Yk	yk
)	)
_	_
>	>
0	0
	
Yk	yk
~Y	~y
Y	y
~EY	~ey
	
Thus	thus
all	all
the	the
items	items
under	under
the	the
sum	sum
are	are
nonnegative	nonnegative
and	and
so	so
must	must
be	be
their	their
weighted	weighted
	
average	average
As	as
a	a
consequence	consequence
we	we
may	may
conclude	conclude
that	that
the	the
iterative	iterative
estimation	estimation
of	of
similarity	similarity
	
converges	converges
	
56	56
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
A	a
3	3
Word	word
weights	weights
	
In	in
our	our
algorithm	algorithm
the	the
weight	weight
of	of
a	a
word	word
estimates	estimates
its	its
expected	expected
contribution	contribution
to	to
the	the
disambiguation	disambiguation
	
task	task
and	and
the	the
extent	extent
to	to
which	which
the	the
word	word
is	is
indicative	indicative
in	in
sentence	sentence
similarity	similarity
	
The	the
weights	weights
do	do
not	not
change	change
with	with
iterations	iterations
They	they
are	are
used	used
to	to
reduce	reduce
the	the
number	number
of	of
	
features	features
to	to
a	a
manageable	manageable
size	size
and	and
to	to
exclude	exclude
words	words
that	that
are	are
expected	expected
to	to
be	be
given	given
	
unreliable	unreliable
similarity	similarity
values	values
The	the
weight	weight
of	of
a	a
word	word
is	is
a	a
product	product
of	of
several	several
factors	factors
:	:
frequency	frequency
	
in	in
the	the
corpus	corpus
the	the
bias	bias
inherent	inherent
in	in
the	the
training	training
set	set
distance	distance
from	from
the	the
target	target
	
word	word
and	and
part-of-speech	part-of-speech
label	label
:	:
	
	
	
Global	global
frequency	frequency
Frequent	frequent
words	words
are	are
less	less
informative	informative
of	of
sense	sense
and	and
of	of
	
sentence	sentence
similarity	similarity
For	for
example	example
the	the
appearance	appearance
of	of
year	year
a	a
frequent	frequent
word	word
	
in	in
two	two
different	different
sentences	sentences
in	in
the	the
corpus	corpus
we	we
employed	employed
would	would
not	not
	
necessarily	necessarily
indicate	indicate
similarity	similarity
between	between
them	them
and	and
would	would
not	not
be	be
effective	effective
	
in	in
disambiguating	disambiguating
the	the
sense	sense
of	of
most	most
target	target
words	words
)	)
The	the
contribution	contribution
of	of
	
freq	freq
(	(
W	w
)	)
frequency	frequency
is	is
max	max
{0	{0
1	1
max5xfreq	max5xfreq
(	(
X	x
)	)
J'	j'
where	where
max5xfreq	max5xfreq
(	(
X	x
)	)
is	is
a	a
function	function
	
of	of
the	the
five	five
highest	highest
frequencies	frequencies
in	in
the	the
global	global
corpus	corpus
and	and
X	x
is	is
any	any
noun	noun
	
or	or
verb	verb
or	or
adjective	adjective
there	there
This	this
factor	factor
excludes	excludes
only	only
the	the
most	most
frequent	frequent
	
words	words
from	from
further	further
consideration	consideration
As	as
long	long
as	as
the	the
frequencies	frequencies
are	are
not	not
	
very	very
high	high
it	it
does	does
not	not
label	label
14	14
;1	;1
whose	whose
frequency	frequency
is	is
twice	twice
that	that
of	of
W2	w2
as	as
	
less	less
informative	informative
	
Log-likelihood	log-likelihood
factor	factor
Words	words
that	that
are	are
indicative	indicative
of	of
the	the
sense	sense
usually	usually
appear	appear
	
in	in
the	the
training	training
set	set
more	more
than	than
what	what
would	would
have	have
been	been
expected	expected
from	from
their	their
	
frequency	frequency
in	in
the	the
general	general
corpus	corpus
The	the
log-likelihood	log-likelihood
factor	factor
captures	captures
this	this
	
tendency	tendency
It	it
is	is
computed	computed
as	as
	
	
	
Pr	pr
(	(
Wi	wi
114	114
;	;
)	)
(	(
10	10
)	)
	
log	log
Pr	pr
(	(
Wi	wi
)	)
	
where	where
Pr	pr
(	(
14	14
;i	;i
)	)
is	is
estimated	estimated
from	from
the	the
frequency	frequency
of	of
14	14
;	;
in	in
the	the
entire	entire
corpus	corpus
	
and	and
Pr	pr
(	(
Wi	wi
I	i
14	14
;	;
)	)
from	from
the	the
frequency	frequency
of	of
Wi	wi
in	in
the	the
training	training
set	set
given	given
the	the
	
examples	examples
of	of
the	the
current	current
ambiguous	ambiguous
word	word
W	w
(	(
cf	cf
Gale	gale
Church	church
and	and
	
Yarowsky	yarowsky
[1992	[1992
]	]
)	)
8	8
To	to
avoid	avoid
poor	poor
estimation	estimation
for	for
words	words
with	with
a	a
low	low
count	count
	
in	in
the	the
training	training
set	set
We	we
multiply	multiply
the	the
log	log
likelihood	likelihood
by	by
min	min
{1	{1
~o	~o
~nt	~nt
(	(
w	w
)	)
10	10
}	}
	
where	where
count	count
(	(
W	w
)	)
is	is
the	the
number	number
of	of
occurrences	occurrences
of	of
14	14
;	;
in	in
the	the
training	training
set	set
	
Part	part
of	of
speech	speech
Each	each
part	part
of	of
speech	speech
is	is
assigned	assigned
a	a
weight	weight
(	(
1	1
0	0
for	for
nouns	nouns
0	0
6	6
	
for	for
verbs	verbs
and	and
1	1
0	0
for	for
the	the
adjectives	adjectives
of	of
the	the
target	target
word	word
)	)
	
Distance	distance
from	from
the	the
target	target
word	word
Context	context
words	words
that	that
are	are
far	far
from	from
the	the
target	target
	
word	word
are	are
less	less
indicative	indicative
than	than
nearby	nearby
ones	ones
The	the
contribution	contribution
of	of
this	this
factor	factor
	
is	is
reciprocally	reciprocally
related	related
to	to
the	the
normalized	normalized
distance	distance
:	:
the	the
weight	weight
of	of
context	context
	
words	words
that	that
appear	appear
in	in
the	the
same	same
sentence	sentence
as	as
the	the
target	target
word	word
is	is
taken	taken
to	to
	
be	be
1	1
0	0
;	;
the	the
weight	weight
of	of
words	words
that	that
appear	appear
in	in
the	the
adjacent	adjacent
sentences	sentences
is	is
0	0
5	5
	
The	the
total	total
weight	weight
of	of
a	a
word	word
is	is
the	the
product	product
of	of
the	the
above	above
factors	factors
each	each
normalized	normalized
by	by
	
factor	factor
(	(
Wi	wi
S	s
)	)
the	the
sum	sum
of	of
factors	factors
of	of
the	the
words	words
in	in
the	the
sentence	sentence
:	:
weight	weight
(	(
Wi	wi
$	$
)	)
=	=
z_	z_
~'wjcs	~'wjcs
factor	factor
(	(
W	w
j	j
S	s
)	)
'	'
	
8	8
Because	because
this	this
estimate	estimate
is	is
unreliable	unreliable
for	for
words	words
with	with
low	low
frequencies	frequencies
in	in
each	each
sense	sense
set	set
Gale	gale
Church	church
and	and
	
Yarowsky	yarowsky
(	(
1992	1992
)	)
suggested	suggested
to	to
interpolate	interpolate
between	between
probabilities	probabilities
computed	computed
within	within
the	the
subcorpus	subcorpus
and	and
	
probabilities	probabilities
computed	computed
over	over
the	the
entire	entire
corpus	corpus
In	in
our	our
case	case
the	the
denominator	denominator
is	is
the	the
frequency	frequency
in	in
the	the
	
general	general
corpus	corpus
instead	instead
of	of
the	the
frequency	frequency
in	in
the	the
sense	sense
examples	examples
so	so
it	it
is	is
more	more
reliable	reliable
	
57	57
	
Computational	computational
Linguistics	linguistics
Volume	volume
24	24
Number	number
1	1
	
where	where
factor	factor
(	(
)	)
is	is
the	the
weight	weight
before	before
normalization	normalization
The	the
use	use
of	of
weights	weights
contributed	contributed
	
about	about
5	5
%	%
to	to
the	the
disambiguation	disambiguation
performance	performance
	
A	a
4	4
Other	other
uses	uses
of	of
context	context
similarity	similarity
	
The	the
similarity	similarity
measure	measure
developed	developed
in	in
the	the
present	present
paper	paper
can	can
be	be
used	used
for	for
tasks	tasks
other	other
than	than
	
word	word
sense	sense
disambiguation	disambiguation
Here	here
we	we
illustrate	illustrate
a	a
possible	possible
application	application
to	to
automatic	automatic
	
construction	construction
of	of
a	a
thesaurus	thesaurus
	
Following	following
the	the
training	training
phase	phase
for	for
a	a
word	word
X	x
we	we
have	have
a	a
word	word
similarity	similarity
matrix	matrix
for	for
	
the	the
words	words
in	in
the	the
contexts	contexts
of	of
:Y	:y
Using	using
this	this
matrix	matrix
we	we
construct	construct
for	for
each	each
sense	sense
si	si
of	of
-Y	-y
	
a	a
set	set
of	of
related	related
words	words
R	r
:	:
	
Â	Â
°	°
	
2	2
	
	
Initialize	initialize
R	r
to	to
the	the
set	set
of	of
words	words
appearing	appearing
in	in
the	the
MRD	mrd
definition	definition
of	of
si	si
;	;
	
Extend	extend
R	r
recursively	recursively
:	:
for	for
each	each
word	word
in	in
R	r
added	added
in	in
the	the
previous	previous
step	step
add	add
	
its	its
k	k
nearest	nearest
neighbors	neighbors
using	using
the	the
similarity	similarity
matrix	matrix
	
Stop	stop
when	when
no	no
new	new
words	words
(	(
or	or
too	too
few	few
new	new
words	words
)	)
are	are
added	added
	
Upon	upon
termination	termination
output	output
for	for
each	each
sense	sense
si	si
the	the
set	set
of	of
its	its
contextually	contextually
similar	similar
words	words
R	r
	
Acknowledgments	acknowledgments
	
We	we
thank	thank
Dan	dan
Roth	roth
for	for
many	many
useful	useful
	
discussions	discussions
and	and
the	the
anonymous	anonymous
reviewers	reviewers
	
for	for
constructive	constructive
comments	comments
on	on
the	the
	
manuscript	manuscript
This	this
work	work
was	was
first	first
presented	presented
	
at	at
the	the
4th	4th
International	international
Workshop	workshop
on	on
Large	large
	
Corpora	corpora
Copenhagen	copenhagen
August	august
1996	1996
	
References	references
	
Brill	brill
Eric	eric
David	david
Magerman	magerman
Mitchell	mitchell
P	p
	
Marcus	marcus
and	and
Beatrice	beatrice
Santorini	santorini
1990	1990
	
Deducing	deducing
linguistic	linguistic
structure	structure
from	from
the	the
	
statistics	statistics
of	of
large	large
corpora	corpora
In	in
DARPA	darpa
	
Speech	speech
and	and
Natural	natural
Language	language
Workshop	workshop
	
pages	pages
275-282	275-282
June	june
	
Brown	brown
Peter	peter
F	f
 Stephen	 stephen
Della	della
Pietra	pietra
	
Vincent	vincent
J	j
Della	della
Pietra	pietra
and	and
Robert	robert
L	l
	
Mercer	mercer
1991	1991
Word	word
sense	sense
disambiguation	disambiguation
	
using	using
statistical	statistical
methods	methods
In	in
Proceedings	proceedings
of	of
	
the	the
29th	29th
Annual	annual
Meeting	meeting
pages	pages
264-270	264-270
	
Association	association
for	for
Computational	computational
	
Linguistics	linguistics
	
Church	church
Kenneth	kenneth
W	w
and	and
Robert	robert
L	l
Mercer	mercer
	
1993	1993
Introduction	introduction
to	to
the	the
special	special
issue	issue
on	on
	
computational	computational
linguistics	linguistics
using	using
large	large
	
corpora	corpora
Computational	computational
Linguistics	linguistics
19	19
:1-24	:1-24
	
Cruse	cruse
D	d
Alan	alan
1986	1986
Lexical	lexical
Semantics	semantics
	
Cambridge	cambridge
University	university
Press	press
Cambridge	cambridge
	
England	england
	
Dagan	dagan
Ido	ido
Alon	alon
Itai	itai
and	and
Ulrike	ulrike
Schwall	schwall
	
1991	1991
Two	two
languages	languages
are	are
more	more
informative	informative
	
than	than
one	one
In	in
Proceedings	proceedings
of	of
the	the
29th	29th
Annual	annual
	
Meeting	meeting
pages	pages
130-137	130-137
Association	association
for	for
	
Computational	computational
Linguistics	linguistics
	
Dagan	dagan
Ido	ido
Shaul	shaul
Marcus	marcus
and	and
Shaul	shaul
	
Markovitch	markovitch
1993	1993
Contextual	contextual
word	word
	
similarity	similarity
and	and
estimation	estimation
from	from
sparse	sparse
	
data	data
In	in
Proceedings	proceedings
of	of
the	the
31st	31st
Annual	annual
	
Meeting	meeting
pages	pages
164-174	164-174
Association	association
for	for
	
Computational	computational
Linguistics	linguistics
	
Gale	gale
William	william
Kenneth	kenneth
W	w
Church	church
and	and
	
David	david
Yarowsky	yarowsky
1992	1992
A	a
method	method
for	for
	
disambiguating	disambiguating
word	word
senses	senses
in	in
a	a
large	large
	
corpus	corpus
Computers	computers
and	and
the	the
Humanities	humanities
	
26	26
:415-439	:415-439
	
Guthrie	guthrie
Joe	joe
A	a
 Louise	 louise
Guthrie	guthrie
Yorick	yorick
	
Wilks	wilks
and	and
Homa	homa
Aidinejad	aidinejad
1991	1991
-	-
	
Subject-dependent	subject-dependent
cooccurrence	cooccurrence
and	and
	
word	word
sense	sense
disambiguation	disambiguation
In	in
Proceedings	proceedings
	
of	of
the	the
29th	29th
Annual	annual
Meeting	meeting
pages	pages
146-152	146-152
	
Association	association
for	for
Computational	computational
	
Linguistics	linguistics
	
Hirschman	hirschman
Lynette	lynette
1986	1986
Discovering	discovering
	
sublanguage	sublanguage
structure	structure
In	in
Ralph	ralph
	
Grishman	grishman
and	and
Richard	richard
Kittredge	kittredge
editors	editors
	
Analyzing	analyzing
Language	language
in	in
Restricted	restricted
Domains	domains
:	:
	
Sublanguage	sublanguage
Description	description
and	and
Processing	processing
	
Lawrence	lawrence
Erlbaum	erlbaum
Hillsdale	hillsdale
NJ	nj
pages	pages
	
211-234	211-234
	
Krovetz	krovetz
Robert	robert
and	and
W	w
Bruce	bruce
Croft	croft
1989	1989
	
Word	word
sense	sense
disambiguation	disambiguation
using	using
	
machine	machine
readable	readable
dictionaries	dictionaries
In	in
	
Proceedings	proceedings
of	of
ACM	acm
SIGIR'89	sigir'89
pages	pages
	
127-136	127-136
	
Lee	lee
Joon	joon
H	h
 Myoung	 myoung
H	h
Kim	kim
and	and
Yoon	yoon
J	j
	
Lee	lee
1993	1993
Information	information
retrieval	retrieval
based	based
on	on
	
conceptual	conceptual
distance	distance
in	in
IS-A	is-a
hierarchies	hierarchies
	
Journal	journal
of	of
Documentation	documentation
49	49
:188-207	:188-207
	
Lesk	lesk
Michael	michael
1986	1986
Automatic	automatic
sense	sense
	
disambiguation	disambiguation
:	:
How	how
to	to
tell	tell
a	a
pine	pine
cone	cone
	
from	from
an	an
ice	ice
cream	cream
cone	cone
In	in
Proceedings	proceedings
of	of
	
the	the
1986	1986
ACM	acm
SIGDOC	sigdoc
Conference	conference
pages	pages
	
24-26	24-26
	
58	58
	
Karov	karov
and	and
Edelman	edelman
Similarity-based	similarity-based
Word	word
Sense	sense
Disambiguation	disambiguation
	
Miller	miller
George	george
A	a
 Richard	 richard
Beckwith	beckwith
	
Christiane	christiane
Fellbaum	fellbaum
Derek	derek
Gross	gross
and	and
	
Katherine	katherine
J	j
Miller	miller
1993	1993
Introduction	introduction
to	to
	
WordNet	wordnet
:	:
An	an
on-line	on-line
lexical	lexical
database	database
	
CSL	csl
43	43
Cognitive	cognitive
Science	science
Laboratory	laboratory
	
Princeton	princeton
University	university
Princeton	princeton
NJ	nj
	
Pereira	pereira
Fernando	fernando
and	and
Naftali	naftali
Tishby	tishby
1992	1992
	
Distibutional	distibutional
similarity	similarity
phase	phase
transitions	transitions
	
and	and
hierarchical	hierarchical
clustering	clustering
In	in
Working	working
	
Notes	notes
of	of
the	the
AAAI	aaai
Fall	fall
Symposium	symposium
on	on
	
Probabilistic	probabilistic
Approaches	approaches
to	to
Natural	natural
Language	language
	
pages	pages
108-112	108-112
	
Pereira	pereira
Fernando	fernando
Naftali	naftali
Tishby	tishby
and	and
	
Lillian	lillian
Lee	lee
1993	1993
Distibutional	distibutional
clustering	clustering
	
of	of
English	english
words	words
In	in
Proceedings	proceedings
of	of
the	the
31st	31st
	
Annual	annual
Meeting	meeting
pages	pages
183-190	183-190
	
Association	association
for	for
Computational	computational
	
Linguistics	linguistics
	
Quine	quine
Willard	willard
V	v
O	o
1960	1960
Word	word
and	and
Object	object
	
MIT	mit
Press	press
Cambridge	cambridge
MA	ma
	
Resnik	resnik
Philip	philip
July	july
1992	1992
WordNet	wordnet
and	and
	
distribuitional	distribuitional
analysis	analysis
:	:
A	a
class-based	class-based
	
approach	approach
to	to
lexical	lexical
discovery	discovery
In	in
AAAI	aaai
	
Workshop	workshop
on	on
Statistically-based	statistically-based
Natural	natural
	
Language	language
Processing	processing
Techniques	techniques
pages	pages
	
56-64	56-64
	
Resnik	resnik
Philip	philip
June	june
1995	1995
Disambiguating	disambiguating
	
noun	noun
groupings	groupings
with	with
respect	respect
to	to
WordNet	wordnet
	
senses	senses
In	in
Third	third
Workshop	workshop
on	on
Very	very
Large	large
	
Corpora	corpora
pages	pages
55-68	55-68
Cambridge	cambridge
MA	ma
	
Schi	schi
~tze	~tze
Hinrich	hinrich
1992	1992
Dimensions	dimensions
of	of
	
meaning	meaning
In	in
Proceedings	proceedings
of	of
Supercomputing	supercomputing
	
Symposium	symposium
pages	pages
787-796	787-796
Minneapolis	minneapolis
	
MN	mn
	
V6ronis	v6ronis
Jean	jean
and	and
Nancy	nancy
Ide	ide
1990	1990
Word	word
	
sense	sense
disambiguation	disambiguation
with	with
very	very
large	large
	
neural	neural
networks	networks
extracted	extracted
from	from
machine	machine
	
readable	readable
dictionaries	dictionaries
In	in
Proceedings	proceedings
of	of
	
COLING-90	coling-90
pages	pages
389-394	389-394
	
Walker	walker
Donald	donald
E	e
and	and
Robert	robert
A	a
Amsler	amsler
	
1986	1986
The	the
use	use
of	of
machine-readable	machine-readable
	
dictionaries	dictionaries
in	in
sublanguage	sublanguage
analysis	analysis
In	in
	
R	r
Grisham	grisham
editor	editor
Analyzing	analyzing
Languages	languages
in	in
	
Restricted	restricted
Domains	domains
:	:
Sublanguage	sublanguage
Description	description
	
and	and
Processing	processing
Lawrence	lawrence
Erlbaum	erlbaum
	
Associates	associates
Hillsdale	hillsdale
NJ	nj
	
Weinreich	weinreich
Uriel	uriel
1980	1980
On	on
Semantics	semantics
	
University	university
of	of
Pennsylvania	pennsylvania
Press	press
	
Philadelphia	philadelphia
PA	pa
	
Yarowsky	yarowsky
David	david
1992	1992
Word	word
sense	sense
	
disambiguation	disambiguation
using	using
statistical	statistical
models	models
of	of
	
Roget's	roget's
categories	categories
trained	trained
on	on
large	large
	
corpora	corpora
In	in
Proceedings	proceedings
of	of
COLING-92	coling-92
	
pages	pages
454-460	454-460
Nantes	nantes
France	france
	
Yarowsky	yarowsky
David	david
1995	1995
Unsupervised	unsupervised
word	word
	
sense	sense
disambiguation	disambiguation
rivaling	rivaling
supervised	supervised
	
methods	methods
In	in
Proceedings	proceedings
of	of
the	the
33rd	33rd
Annual	annual
	
Meeting	meeting
pages	pages
189-196	189-196
Cambridge	cambridge
MA	ma
	
Association	association
for	for
Computational	computational
	
Linguistics	linguistics
	
59	59
	
	
